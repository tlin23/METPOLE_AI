This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app/
  api/
    routes.py
  crawler/
    add_metadata_and_tags.py
    crawl.py
    extract_content.py
  embedder/
    embed_corpus.py
  retriever/
    ask.py
  vector_store/
    __init__.py
    init_chroma.py
  config.py
  logging_config.py
main.py
run_pipeline.py
test_retrieval.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="test_retrieval.py">
# test_retrieval.py

from app.retriever.ask import Retriever
import pprint

retriever = Retriever()

query = "What is metropole?"
results = retriever.query(query, n_results=5)

pprint.pprint(results)
</file>

<file path="app/vector_store/__init__.py">
"""
Vector store package for handling persistent vector embeddings.
"""

from app.vector_store.init_chroma import init_chroma_db

__all__ = ["init_chroma_db"]
</file>

<file path="app/config.py">
"""
Configuration settings for the application.

This module centralizes all environment variables used throughout the application.
"""

import os
from typing import Optional

# Chroma DB settings
CHROMA_DB_PATH: str = os.getenv("CHROMA_DB_PATH", "./data/index")

# OpenAI API settings
OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")

# Feedback logging settings
FEEDBACK_LOG_DIR: str = os.getenv("FEEDBACK_LOG_DIR", "data/logs")
FEEDBACK_LOG_FILE: str = os.getenv("FEEDBACK_LOG_FILE", "feedback.jsonl")
FEEDBACK_LOG_MAX_SIZE_MB: int = int(os.getenv("FEEDBACK_LOG_MAX_SIZE_MB", "10"))
FEEDBACK_LOG_MAX_BACKUPS: int = int(os.getenv("FEEDBACK_LOG_MAX_BACKUPS", "5"))
</file>

<file path="app/logging_config.py">
"""
Centralized logging configuration for the application.

This module provides a standardized way to configure logging across the application.
It sets up a shared logger with consistent formatting and handlers.
"""

import os
import logging
import sys
from pathlib import Path
from logging.handlers import RotatingFileHandler


# Create logs directory if it doesn't exist
LOG_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "logs")
Path(LOG_DIR).mkdir(parents=True, exist_ok=True)


# Configure the root logger
def configure_logging(
    logger_name="metropole_ai",
    log_level=logging.INFO,
    log_format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    log_file=None,
    max_bytes=10 * 1024 * 1024,  # 10 MB
    backup_count=5,
    stream_handler=True,
):
    """
    Configure a logger with the specified parameters.

    Args:
        logger_name (str): Name of the logger.
        log_level (int): Logging level (e.g., logging.INFO, logging.DEBUG).
        log_format (str): Format string for log messages.
        log_file (str, optional): Path to the log file. If None, no file handler is added.
        max_bytes (int): Maximum size of the log file before rotation.
        backup_count (int): Number of backup log files to keep.
        stream_handler (bool): Whether to add a stream handler (console output).

    Returns:
        logging.Logger: The configured logger.
    """
    # Create logger
    logger = logging.getLogger(logger_name)
    logger.setLevel(log_level)

    # Remove existing handlers to avoid duplicates
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Create formatter
    formatter = logging.Formatter(log_format)

    # Add stream handler (console output)
    if stream_handler:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    # Add file handler if log_file is specified
    if log_file:
        # Ensure the directory exists
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)

        file_handler = RotatingFileHandler(
            log_file, maxBytes=max_bytes, backupCount=backup_count
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger


# Create the default application logger
logger = configure_logging(
    logger_name="metropole_ai",
    log_level=logging.INFO,
    log_file=os.path.join(LOG_DIR, "metropole_ai.log"),
)


def get_logger(name=None):
    """
    Get a logger with the specified name.

    Args:
        name (str, optional): Name of the logger. If None, returns the root logger.

    Returns:
        logging.Logger: The logger.
    """
    if name is None:
        return logger

    return logging.getLogger(f"metropole_ai.{name}")
</file>

<file path="app/crawler/add_metadata_and_tags.py">
"""
Script to add metadata and tags to content chunks.

This script:
1. Loads content objects from data/processed/content_objects.py
2. Assigns a page_id, section header, and unique chunk_id to each chunk
3. Uses KeyBERT + MiniLM to extract 3-5 tags per chunk
4. Stores everything in a structured JSON file named metropole_corpus.json
"""

import os
import sys
import json
import uuid
from typing import List, Dict, Any
from keybert import KeyBERT

# Add the project root to the Python path to allow importing from data/processed
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

try:
    from data.processed.content_objects import content_objects

    print(f"Successfully loaded {len(content_objects)} content objects")
except ImportError:
    print(
        "Error: Could not import content_objects. Make sure to run process_html_content.py first."
    )
    sys.exit(1)


def generate_page_ids(content_objects: List[Dict[str, Any]]) -> Dict[str, str]:
    """
    Generate unique page IDs for each unique page name.

    Args:
        content_objects (List[Dict[str, Any]]): List of content objects.

    Returns:
        Dict[str, str]: Dictionary mapping page names to page IDs.
    """
    # Get unique page names
    page_names = set(chunk["page_name"] for chunk in content_objects)

    # Generate a unique ID for each page name
    page_ids = {}
    for page_name in page_names:
        # Use a shortened UUID as the page ID
        page_ids[page_name] = f"page_{str(uuid.uuid4())[:8]}"

    return page_ids


def extract_tags_with_keybert(
    text: str, model: KeyBERT, num_tags: int = 5
) -> List[str]:
    """
    Extract tags from text using KeyBERT with MiniLM.

    Args:
        text (str): Text to extract tags from.
        model (KeyBERT): KeyBERT model instance.
        num_tags (int, optional): Number of tags to extract. Defaults to 5.

    Returns:
        List[str]: List of extracted tags.
    """
    # Skip empty or very short text
    if not text or len(text) < 20:
        return []

    # Extract keywords
    keywords = model.extract_keywords(
        text,
        keyphrase_ngram_range=(1, 2),  # Extract single words and bigrams
        stop_words="english",
        use_mmr=True,  # Use Maximal Marginal Relevance to diversify results
        diversity=0.7,  # Higher diversity means more diverse results
        top_n=num_tags,
    )

    # Return just the keywords (not the scores)
    return [keyword for keyword, _ in keywords]


def process_content_objects() -> List[Dict[str, Any]]:
    """
    Process content objects to add metadata and tags.

    Returns:
        List[Dict[str, Any]]: List of processed content objects with metadata and tags.
    """
    # Initialize KeyBERT with MiniLM
    print("Initializing KeyBERT with MiniLM model...")
    model = KeyBERT(model="all-MiniLM-L6-v2")

    # Generate page IDs
    page_ids = generate_page_ids(content_objects)

    # Process each content object
    processed_objects = []

    print(f"Processing {len(content_objects)} content objects...")
    for i, chunk in enumerate(content_objects):
        # Generate a unique chunk ID
        chunk_id = f"chunk_{str(uuid.uuid4())}"

        # Get the page ID for this chunk
        page_id = page_ids[chunk["page_name"]]

        # Extract tags using KeyBERT
        tags = extract_tags_with_keybert(chunk["content"], model)

        # Create a processed object with metadata and tags
        processed_object = {
            "chunk_id": chunk_id,
            "page_id": page_id,
            "page_title": chunk["page_title"],
            "page_name": chunk["page_name"],
            "section_header": chunk["section_header"],
            "content": chunk["content"],
            "tags": tags,
        }

        processed_objects.append(processed_object)

        # Print progress every 10 chunks
        if (i + 1) % 10 == 0:
            print(f"Processed {i + 1}/{len(content_objects)} chunks")

    return processed_objects


def save_to_json(processed_objects: List[Dict[str, Any]], output_path: str) -> None:
    """
    Save processed objects to a JSON file.

    Args:
        processed_objects (List[Dict[str, Any]]): List of processed content objects.
        output_path (str): Path to save the JSON file.
    """
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # Save to JSON
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(processed_objects, f, indent=2, ensure_ascii=False)

    print(f"Saved {len(processed_objects)} processed objects to {output_path}")


def main():
    """Main function to process content objects and save to JSON."""
    # Process content objects
    processed_objects = process_content_objects()

    # Save to JSON
    output_path = os.path.join("data", "processed", "metropole_corpus.json")
    save_to_json(processed_objects, output_path)

    # Print summary
    page_count = len(set(obj["page_id"] for obj in processed_objects))
    section_count = len(set(obj["section_header"] for obj in processed_objects))
    tag_count = sum(len(obj["tags"]) for obj in processed_objects)

    print("\nSummary:")
    print(f"- Processed {len(processed_objects)} content chunks")
    print(f"- From {page_count} unique pages")
    print(f"- With {section_count} unique sections")
    print(
        f"- Generated {tag_count} tags (avg. {tag_count/len(processed_objects):.1f} per chunk)"
    )


if __name__ == "__main__":
    main()
</file>

<file path="app/crawler/crawl.py">
"""Crawler module for fetching and processing web content."""

import requests
from bs4 import BeautifulSoup
import os
import urllib.parse
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any


def fetch_url(url: str) -> Optional[str]:
    """Fetch content from a URL.

    Args:
        url: The URL to fetch.

    Returns:
        The HTML content of the page if successful, None otherwise.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None


def parse_html(html: Optional[str]) -> Optional[BeautifulSoup]:
    """Parse HTML content using BeautifulSoup.

    Args:
        html: HTML content to parse.

    Returns:
        Parsed HTML as BeautifulSoup object if successful, None otherwise.
    """
    if html:
        return BeautifulSoup(html, "html.parser")
    return None


def extract_text(soup: Optional[BeautifulSoup]) -> Optional[str]:
    """Extract text content from parsed HTML.

    Args:
        soup: Parsed HTML as BeautifulSoup object.

    Returns:
        Extracted text content if successful, None otherwise.
    """
    if soup:
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.extract()

        # Get text
        text = soup.get_text()

        # Break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())

        # Break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

        # Drop blank lines
        text = "\n".join(chunk for chunk in chunks if chunk)

        return text
    return None


def extract_links(soup: Optional[BeautifulSoup], base_url: str) -> List[str]:
    """Extract all internal links from a parsed HTML page.

    Args:
        soup: Parsed HTML as BeautifulSoup object.
        base_url: The base URL of the website.

    Returns:
        List of internal links found in the HTML.
    """
    if not soup:
        return []

    links = []
    base_domain = urllib.parse.urlparse(base_url).netloc
    parsed_base = urllib.parse.urlparse(base_url)

    for a_tag in soup.find_all("a"):
        if not hasattr(a_tag, "get") or not a_tag.get("href"):
            continue
        href = str(a_tag.get("href", ""))

        # Handle relative URLs
        if href.startswith("/"):
            # Convert relative URL to absolute
            parsed_base = urllib.parse.urlparse(base_url)
            absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
            links.append(absolute_url)
        else:
            # Check if it's an internal link (same domain)
            parsed_href = urllib.parse.urlparse(href)
            if parsed_href.netloc == base_domain or not parsed_href.netloc:
                # If it's a fragment or query within the same page, skip
                if href.startswith("#") or href == "":
                    continue

                # If it's a relative path without leading slash
                if not parsed_href.netloc and not href.startswith("http"):
                    # Get the directory of the current URL
                    current_path = os.path.dirname(urllib.parse.urlparse(base_url).path)
                    if not current_path.endswith("/"):
                        current_path += "/"

                    # Join with the relative path
                    if current_path == "/":
                        absolute_url = (
                            f"{parsed_base.scheme}://{parsed_base.netloc}/{href}"
                        )
                    else:
                        absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{current_path}{href}"
                    links.append(absolute_url)
                elif parsed_href.netloc == base_domain:
                    links.append(href)

    return links


def recursive_crawl(
    start_url: str, max_pages: Optional[int] = None, save_to_files: bool = False
) -> Dict[str, str]:
    """Recursively crawl a website starting from a given URL.

    Args:
        start_url: The URL to start crawling from.
        max_pages: Maximum number of pages to crawl. None for unlimited.
        save_to_files: Whether to save HTML content to files.

    Returns:
        Dictionary mapping URLs to their HTML content.
    """
    visited = set()
    to_visit = [start_url]
    page_content = {}
    base_domain = urllib.parse.urlparse(start_url).netloc

    # Create data directory if saving to files
    if save_to_files:
        os.makedirs("data/html", exist_ok=True)

    page_count = 0

    while to_visit and (max_pages is None or page_count < max_pages):
        current_url = to_visit.pop(0)

        # Skip if already visited
        if current_url in visited:
            continue

        print(f"Crawling: {current_url}")

        # Fetch and parse the page
        html_content = fetch_url(current_url)
        if not html_content:
            visited.add(current_url)
            continue

        # Store the content
        page_content[current_url] = html_content
        page_count += 1

        # Save to file if requested
        if save_to_files:
            # Create a filename from the URL
            parsed_url = urllib.parse.urlparse(current_url)
            path = parsed_url.path
            if not path or path == "/":
                path = "/index"

            # Replace special characters
            filename = f"{parsed_url.netloc}{path}".replace("/", "_").replace(":", "_")
            if not filename.endswith(".html"):
                filename += ".html"

            with open(f"data/html/{filename}", "w", encoding="utf-8") as f:
                f.write(html_content)

        # Parse the HTML
        soup = parse_html(html_content)

        # Extract links
        links = extract_links(soup, current_url)

        # Mark as visited
        visited.add(current_url)

        # Add new links to visit
        for link in links:
            if link not in visited and link not in to_visit:
                # Ensure it's from the same domain
                link_domain = urllib.parse.urlparse(link).netloc
                if link_domain == base_domain:
                    to_visit.append(link)

    print(f"Crawling complete. Visited {len(visited)} pages.")
    return page_content


def crawl(url: str) -> Tuple[Optional[str], Dict[str, Any]]:
    """Crawl a URL and extract its text content.

    Args:
        url: The URL to crawl.

    Returns:
        A tuple containing:
            - The extracted text content (or None if extraction failed)
            - A metadata dictionary with information about the crawled page
    """
    html = fetch_url(url)
    soup = parse_html(html)
    text = extract_text(soup)

    # Extract metadata
    title = "No title"
    if soup and hasattr(soup, "title") and soup.title:
        title = str(soup.title.string) if soup.title.string else "No title"

    metadata = {
        "url": url,
        "title": title,
        "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "source": "web_crawl",
    }

    return text, metadata


if __name__ == "__main__":
    # Example usage for recursive crawling
    start_url = "https://www.metropoleballard.com/home"
    content_dict = recursive_crawl(start_url, max_pages=50, save_to_files=True)
    print(f"Collected {len(content_dict)} pages")
</file>

<file path="app/crawler/extract_content.py">
from bs4 import BeautifulSoup
from bs4.element import Tag
from typing import List, Dict, Any, Optional
import os
import re


def extract_structured_content(html_file_path: str) -> Dict[str, Any]:
    with open(html_file_path, "r", encoding="utf-8") as file:
        html_content = file.read()

    soup = BeautifulSoup(html_content, "html.parser")
    title = extract_page_title(soup)
    sections = extract_sections(soup)

    return {"title": title, "sections": sections}


def extract_page_title(soup: BeautifulSoup) -> str:
    meta_title = soup.find("meta", property="og:title")
    if meta_title and meta_title.get("content"):
        return meta_title.get("content")

    title_tag = soup.find("title")
    if title_tag and title_tag.text:
        return title_tag.text

    h1_tag = soup.find("h1")
    if h1_tag and h1_tag.text:
        return h1_tag.text.strip()

    return os.path.basename(soup.title.string) if soup.title else "Untitled Page"


def is_block_level(tag: Tag) -> bool:
    return tag.name in ["p", "li", "h1", "h2", "h3"]


def extract_sections(soup: BeautifulSoup) -> List[Dict[str, Any]]:
    sections = []
    current_section = None

    content_elements = soup.find_all(
        ["h1", "h2", "h3", "p", "li", "div", "span", "br", "hr"]
    )

    for element in content_elements:
        if should_skip_element(element):
            continue

        if element.name in ["h1", "h2", "h3"]:
            if current_section and current_section.get("buffer"):
                combined = " ".join(current_section["buffer"]).strip()
                if combined:
                    current_section["chunks"].append(
                        {
                            "content": combined,
                            "section_header": current_section["header"],
                        }
                    )
                del current_section["buffer"]
                sections.append(current_section)

            current_section = {
                "header": element.get_text(strip=True),
                "header_html": str(element),
                "header_level": int(element.name[1]),
                "chunks": [],
                "buffer": [],
            }
            continue

        if current_section is not None:
            text = element.get_text(separator=" ", strip=True)
            text = re.sub(r"\s+", " ", text)

            if text:
                current_section["buffer"].append(text)

            if element.name in ["p", "br", "hr"] or re.match(r"^[A-Z][a-z]+:", text):
                combined = " ".join(current_section["buffer"]).strip()
                if combined:
                    current_section["chunks"].append(
                        {
                            "content": combined,
                            "section_header": current_section["header"],
                        }
                    )
                current_section["buffer"] = []

    if current_section and current_section.get("buffer"):
        combined = " ".join(current_section["buffer"]).strip()
        if combined:
            current_section["chunks"].append(
                {"content": combined, "section_header": current_section["header"]}
            )
        del current_section["buffer"]
        sections.append(current_section)

    return sections


def find_main_content_area(soup: BeautifulSoup) -> Optional[BeautifulSoup]:
    main_content_candidates = [
        soup.find(attrs={"role": "main"}),
        soup.find("main"),
        soup.find("div", class_="UtePc"),
        soup.find("div", class_="RCETm"),
        soup.find("article"),
        soup.find("section"),
    ]
    for candidate in main_content_candidates:
        if candidate:
            return candidate
    return None


def should_skip_element(element) -> bool:
    if not element.get_text(strip=True):
        return True
    if element.name in ["script", "style", "meta", "link", "noscript"]:
        return True

    skip_classes = [
        "navigation",
        "footer",
        "header",
        "nav",
        "menu",
        "sidebar",
        "BbxBP",
        "JzO0Vc",
        "VLoccc",
        "zDUgLc",
        "TxnWlb",
        "dZA9kd",
        "LqzjUe",
        "hBW7Hb",
        "YkaBSd",
    ]

    if element.has_attr("class"):
        element_classes = " ".join(element.get("class", []))
        for cls in skip_classes:
            if cls in element_classes:
                return True

    skip_ids = ["header", "footer", "navigation", "menu", "sidebar"]
    if element.has_attr("id") and element["id"] in skip_ids:
        return True

    parent = element.parent
    while parent:
        if parent.name in ["nav", "footer", "header"]:
            return True
        if parent.has_attr("class"):
            parent_classes = " ".join(parent.get("class", []))
            for cls in skip_classes:
                if cls in parent_classes:
                    return True
        parent = parent.parent

    return False


def process_all_html_files(directory: str) -> Dict[str, Dict[str, Any]]:
    raw_results = {}

    for filename in os.listdir(directory):
        if filename.endswith(".html"):
            file_path = os.path.join(directory, filename)
            try:
                raw_results[file_path] = extract_structured_content(file_path)
            except Exception as e:
                print(f"Error processing {file_path}: {e}")

    deduped_results = {}
    seen_chunks = set()

    for file_path, page in raw_results.items():
        deduped_sections = []
        for section in page["sections"]:
            deduped_chunks = []
            for chunk in section["chunks"]:
                key = f"{file_path}:{section['header']}:{chunk['content'].strip()}"
                if key not in seen_chunks:
                    seen_chunks.add(key)
                    deduped_chunks.append(chunk)
            if deduped_chunks:
                deduped_sections.append({**section, "chunks": deduped_chunks})
        deduped_results[file_path] = {**page, "sections": deduped_sections}

    return deduped_results


if __name__ == "__main__":
    html_directory = "data/html"
    output_directory = "data/processed"
    results = process_all_html_files(html_directory)

    json_output_path = os.path.join(output_directory, "extracted_content.json")
    with open(json_output_path, "w", encoding="utf-8") as f:
        import json

        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"Processed {len(results)} HTML files")
    for file_path, content in results.items():
        print(f"\nFile: {file_path}")
        print(f"Title: {content['title']}")
        print(f"Sections: {len(content['sections'])}")
        if content["sections"]:
            first_section = content["sections"][0]
            print(f"First section header: {first_section['header']}")
            print(f"Number of chunks: {len(first_section['chunks'])}")
            if first_section["chunks"]:
                print(
                    f"First chunk content: {first_section['chunks'][0]['content'][:100]}..."
                )
</file>

<file path="run_pipeline.py">
#!/usr/bin/env python3
"""
Metropole.AI Pipeline Script

This script orchestrates the full data pipeline:
1. Crawl the website and save HTML files
2. Process HTML files to extract structured content
3. Add metadata and tags to the content
4. Embed the corpus and store in ChromaDB

Usage:
    python run_pipeline.py --start-url https://www.metropoleballard.com/home --max-pages 50
"""

import os
import sys
import argparse
import time
from typing import Dict, Optional

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import app modules
from app.logging_config import get_logger
from app.config import CHROMA_DB_PATH
from app.crawler.crawl import recursive_crawl
from app.crawler.extract_content import process_all_html_files
from app.embedder.embed_corpus import embed_corpus

# Get logger for this module
logger = get_logger("pipeline")


def crawl_website(start_url: str, max_pages: Optional[int] = None) -> Dict[str, str]:
    """
    Crawl the website and save HTML files.

    Args:
        start_url: The URL to start crawling from.
        max_pages: Maximum number of pages to crawl. None for unlimited.

    Returns:
        Dictionary mapping URLs to their HTML content.
    """
    logger.info(f"Starting website crawl from {start_url}")
    logger.info(f"Max pages: {max_pages if max_pages is not None else 'unlimited'}")

    # Create data directory if it doesn't exist
    os.makedirs("data/html", exist_ok=True)

    # Crawl the website
    start_time = time.time()
    content_dict = recursive_crawl(start_url, max_pages=max_pages, save_to_files=True)
    elapsed_time = time.time() - start_time

    logger.info(
        f"Crawling complete. Collected {len(content_dict)} pages in {elapsed_time:.2f} seconds"
    )
    return content_dict


def process_html_content() -> None:
    """
    Process HTML files to extract structured content.
    """
    logger.info("Processing HTML files to extract structured content")

    # Directory containing HTML files
    html_directory = "data/html"

    # Create output directory if it doesn't exist
    output_directory = "data/processed"
    os.makedirs(output_directory, exist_ok=True)

    # Process all HTML files
    start_time = time.time()
    results = process_all_html_files(html_directory)

    # Save results to JSON file
    json_output_path = os.path.join(output_directory, "extracted_content.json")
    with open(json_output_path, "w", encoding="utf-8") as f:
        import json

        json.dump(results, f, indent=2, ensure_ascii=False)

    # Create Python objects for each page
    all_chunks = []

    for file_path, content in results.items():
        page_title = content["title"]
        page_name = os.path.basename(file_path).replace(".html", "")

        # Process each section
        for section in content["sections"]:
            section_header = section["header"]

            # Process each chunk in the section
            for chunk in section["chunks"]:
                # Create a structured object for each chunk
                chunk_object = {
                    "page_title": page_title,
                    "page_name": page_name,
                    "section_header": section_header,
                    "content": chunk["content"],
                }

                all_chunks.append(chunk_object)

    # Save all chunks to a Python file
    py_output_path = os.path.join(output_directory, "content_objects.py")
    with open(py_output_path, "w", encoding="utf-8") as f:
        f.write('"""Generated content objects from HTML files."""\n\n')
        f.write("# This file is auto-generated. Do not edit directly.\n\n")
        f.write("content_objects = [\n")

        # Remove duplicate chunks by tracking content we've seen
        seen_content = set()
        unique_chunks = []

        for chunk in all_chunks:
            # Create a key based on content to detect duplicates
            content_key = f"{chunk['page_name']}:{chunk['section_header']}:{chunk['content'][:100]}"

            if content_key not in seen_content:
                seen_content.add(content_key)
                unique_chunks.append(chunk)

        # Write unique chunks to file
        for chunk in unique_chunks:
            f.write("    {\n")
            f.write(f"        'page_title': {json.dumps(chunk['page_title'])},\n")
            f.write(f"        'page_name': {json.dumps(chunk['page_name'])},\n")
            f.write(
                f"        'section_header': {json.dumps(chunk['section_header'])},\n"
            )
            f.write(f"        'content': {json.dumps(chunk['content'])},\n")
            f.write("    },\n")

        f.write("]\n")

    elapsed_time = time.time() - start_time
    logger.info(f"HTML processing complete in {elapsed_time:.2f} seconds")
    logger.info(
        f"Extracted {len(unique_chunks)} unique content chunks from {len(results)} HTML files"
    )


def add_metadata_and_tags() -> None:
    """
    Add metadata and tags to the content.
    """
    logger.info("Adding metadata and tags to content")

    # Import the add_metadata_and_tags module
    from app.crawler.add_metadata_and_tags import process_content_objects, save_to_json

    # Process content objects
    start_time = time.time()
    processed_objects = process_content_objects()

    # Save to JSON
    output_path = os.path.join("data", "processed", "metropole_corpus.json")
    save_to_json(processed_objects, output_path)

    elapsed_time = time.time() - start_time

    # Print summary
    page_count = len(set(obj["page_id"] for obj in processed_objects))
    tag_count = sum(len(obj["tags"]) for obj in processed_objects)

    logger.info(f"Metadata and tagging complete in {elapsed_time:.2f} seconds")
    logger.info(
        f"Processed {len(processed_objects)} content chunks from {page_count} unique pages"
    )
    logger.info(
        f"Generated {tag_count} tags (avg. {tag_count/len(processed_objects):.1f} per chunk)"
    )


def embed_corpus_data() -> None:
    """
    Embed the corpus and store in ChromaDB.
    """
    logger.info("Embedding corpus data")

    # Path to the corpus file
    corpus_path = os.path.join("data", "processed", "metropole_corpus.json")

    # Embed the corpus
    start_time = time.time()
    embed_corpus(
        corpus_path=corpus_path,
        chroma_path=CHROMA_DB_PATH,
        collection_name="metropole_documents",
        batch_size=100,
    )

    elapsed_time = time.time() - start_time
    logger.info(f"Corpus embedding complete in {elapsed_time:.2f} seconds")


def run_pipeline(start_url: str, max_pages: Optional[int] = None) -> None:
    """
    Run the full pipeline.

    Args:
        start_url: The URL to start crawling from.
        max_pages: Maximum number of pages to crawl. None for unlimited.
    """
    logger.info("Starting Metropole.AI pipeline")
    total_start_time = time.time()

    # Step 1: Crawl the website
    crawl_website(start_url, max_pages)

    # Step 2: Process HTML content
    process_html_content()

    # Step 3: Add metadata and tags
    add_metadata_and_tags()

    # Step 4: Embed the corpus
    embed_corpus_data()

    total_elapsed_time = time.time() - total_start_time
    logger.info(f"Pipeline complete! Total time: {total_elapsed_time:.2f} seconds")


if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run the Metropole.AI pipeline.")
    parser.add_argument(
        "--start-url",
        type=str,
        default="https://www.metropoleballard.com/home",
        help="URL to start crawling from",
    )
    parser.add_argument(
        "--max-pages", type=int, default=50, help="Maximum number of pages to crawl"
    )

    args = parser.parse_args()

    # Run the pipeline
    run_pipeline(args.start_url, args.max_pages)
</file>

<file path="app/embedder/embed_corpus.py">
#!/usr/bin/env python3
"""
Script to load the metropole_corpus.json file, embed each chunk using all-MiniLM-L6-v2,
and store the text and metadata in Chroma.
"""

import sys
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions

from app.config import CHROMA_DB_PATH
from app.logging_config import get_logger

# Get logger for this module
logger = get_logger("embedder.embed_corpus")


def load_corpus(corpus_path: str) -> List[Dict[str, Any]]:
    """
    Load the corpus from a JSON file.

    Args:
        corpus_path (str): Path to the corpus file.

    Returns:
        List[Dict[str, Any]]: The loaded corpus.
    """
    logger.info(f"Loading corpus from {corpus_path}")
    try:
        with open(corpus_path, "r", encoding="utf-8") as f:
            corpus = json.load(f)
        logger.info(f"Successfully loaded corpus with {len(corpus)} chunks")
        return corpus
    except Exception as e:
        logger.error(f"Error loading corpus: {e}")
        raise


def embed_corpus(
    corpus_path: str,
    chroma_path: Optional[str] = None,
    collection_name: str = "metropole_documents",
    batch_size: int = 100,
) -> None:
    """
    Embed the corpus using all-MiniLM-L6-v2 and store in Chroma.

    Args:
        corpus_path (str): Path to the corpus file.
        chroma_path (Optional[str]): Path to the Chroma DB. If None, uses the CHROMA_DB_PATH env var or default.
        collection_name (str): Name of the collection to store embeddings in.
        batch_size (int): Number of documents to embed in each batch.
    """
    start_time = time.time()

    # Get the Chroma DB path
    if chroma_path is None:
        chroma_path = CHROMA_DB_PATH

    # Create the directory if it doesn't exist
    Path(chroma_path).mkdir(parents=True, exist_ok=True)

    logger.info(f"Initializing Chroma DB at: {chroma_path}")

    # Initialize the embedding function
    # Note: DefaultEmbeddingFunction uses the 'all-MiniLM-L6-v2' model internally
    logger.info("Loading default embedding model (all-MiniLM-L6-v2)...")
    embedding_function = embedding_functions.DefaultEmbeddingFunction()

    # Initialize the Chroma client
    client = chromadb.PersistentClient(
        path=chroma_path, settings=Settings(anonymized_telemetry=False)
    )

    # Create or get the collection
    collection = client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function,
        metadata={"description": "Metropole corpus embeddings"},
    )

    # Load the corpus
    corpus = load_corpus(corpus_path)

    # Prepare data for embedding
    total_chunks = len(corpus)
    logger.info(f"Preparing to embed {total_chunks} chunks")

    # Process in batches
    total_batches = (total_chunks + batch_size - 1) // batch_size
    total_embedded = 0

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch = corpus[start_idx:end_idx]

        # Extract data for this batch
        ids = [chunk["chunk_id"] for chunk in batch]
        documents = [chunk["content"] for chunk in batch]
        metadatas = [
            {
                "page_id": chunk["page_id"],
                "page_title": chunk["page_title"],
                "page_name": chunk["page_name"],
                "section_header": chunk["section_header"],
                "tags": (
                    ",".join(chunk["tags"])
                    if isinstance(chunk["tags"], list)
                    else chunk["tags"]
                ),
            }
            for chunk in batch
        ]

        # Add to collection
        logger.info(
            f"Embedding batch {batch_idx + 1}/{total_batches} ({len(batch)} chunks)"
        )
        collection.add(ids=ids, documents=documents, metadatas=metadatas)

        total_embedded += len(batch)
        logger.info(f"Progress: {total_embedded}/{total_chunks} chunks embedded")

    # Log completion
    elapsed_time = time.time() - start_time
    logger.info(
        f"Embedding complete! {total_embedded} chunks embedded in {elapsed_time:.2f} seconds"
    )
    logger.info(
        f"Collection '{collection_name}' now contains {collection.count()} documents"
    )


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(
        description="Embed the Metropole corpus using all-MiniLM-L6-v2."
    )
    parser.add_argument(
        "--corpus-path",
        type=str,
        default="./data/processed/metropole_corpus.json",
        help="Path to the corpus file",
    )
    parser.add_argument(
        "--chroma-path", type=str, default=None, help="Path to the Chroma DB"
    )
    parser.add_argument(
        "--collection-name",
        type=str,
        default="metropole_documents",
        help="Name of the collection to store embeddings in",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=100,
        help="Number of documents to embed in each batch",
    )

    args = parser.parse_args()

    embed_corpus(
        corpus_path=args.corpus_path,
        chroma_path=args.chroma_path,
        collection_name=args.collection_name,
        batch_size=args.batch_size,
    )
</file>

<file path="app/vector_store/init_chroma.py">
"""
Script to initialize a Chroma persistent vector store.
"""

import sys
from pathlib import Path
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.config import CHROMA_DB_PATH
from app.logging_config import get_logger

# Get logger for this module
logger = get_logger("vector_store.init_chroma")

# Load environment variables
load_dotenv()


def init_chroma_db():
    """
    Initialize a Chroma persistent vector store.

    Returns:
        chromadb.PersistentClient: The initialized Chroma client.
    """
    # Get the path for the Chroma database
    chroma_db_path = CHROMA_DB_PATH

    # Create the directory if it doesn't exist
    Path(chroma_db_path).mkdir(parents=True, exist_ok=True)

    logger.info(f"Initializing Chroma DB at: {chroma_db_path}")

    # Create a persistent client
    client = chromadb.PersistentClient(
        path=chroma_db_path, settings=Settings(anonymized_telemetry=False)
    )

    # Create a default collection if it doesn't exist
    collection = client.get_or_create_collection(
        name="documents",
        metadata={"description": "Main document collection for MetPol AI"},
    )

    logger.info(
        f"Collection '{collection.name}' initialized with {collection.count()} documents"
    )

    return client


if __name__ == "__main__":
    # Initialize the Chroma DB
    client = init_chroma_db()

    # List all collections
    collections = client.list_collections()
    logger.info(f"Available collections: {[c.name for c in collections]}")
</file>

<file path="main.py">
from dotenv import load_dotenv
import os
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
from pathlib import Path
from fastapi.middleware.cors import CORSMiddleware
from app.api.routes import router as api_router
from app.vector_store.init_chroma import init_chroma_db

# Load environment variables
load_dotenv()

# Set default Chroma DB path if not in environment
if not os.getenv("CHROMA_DB_PATH"):
    os.environ["CHROMA_DB_PATH"] = "./data/index"


# Define lifespan context manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan context manager for FastAPI application.
    Handles startup and shutdown events.
    """
    # Startup: Initialize services
    chroma_db_path = os.getenv("CHROMA_DB_PATH")
    print(f"Initializing Chroma DB at: {chroma_db_path}")

    # Ensure the directory exists
    Path(chroma_db_path).mkdir(parents=True, exist_ok=True)

    # Initialize the Chroma DB
    init_chroma_db()

    yield

    # Shutdown: Clean up resources if needed
    print("Shutting down application...")


# Create FastAPI app with lifespan
app = FastAPI(
    title="MetPol AI",
    description="A FastAPI application for crawling, embedding, and retrieving information",
    version="0.1.0",
    lifespan=lifespan,
)

# Configure CORS
origins = [
    "http://localhost:5173",  # Local development frontend
    "https://metpol-ai-frontend.onrender.com",  # Render frontend
    "https://metpol-ai-frontend.vercel.app",  # Optional Vercel frontend
    "*",  # Allow all origins during development (remove in production)
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API routes
app.include_router(api_router, prefix="/api")


@app.get("/")
def read_root():
    return {"message": "Hello World"}


# The startup event is now handled by the lifespan context manager above


if __name__ == "__main__":
    # Print some environment info
    print(f"CHROMA_DB_PATH: {os.getenv('CHROMA_DB_PATH')}")

    # Run the FastAPI server
    print(
        "Run the FastAPI server at http://127.0.0.1:8000 with: uvicorn main:app --reload"
    )
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)
</file>

<file path="app/api/routes.py">
"""
API routes for the application.
"""

from fastapi import APIRouter
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

from app.retriever.ask import Retriever

# Create router
router = APIRouter()

# Initialize components
retriever = Retriever()


# Define models
class AskRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5


class ChunkResult(BaseModel):
    text: str
    metadata: Optional[Dict[str, Any]] = None
    distance: Optional[float] = None


class AskResponse(BaseModel):
    question: str
    chunks: List[ChunkResult]
    answer: Optional[str] = None
    is_general_knowledge: Optional[bool] = False
    contains_diy_advice: Optional[bool] = False
    source_info: Optional[str] = None
    success: bool
    message: str


@router.post("/ask", response_model=AskResponse)
async def ask_question(request: AskRequest):
    """
    Ask a question and get an answer generated by OpenAI's GPT model based on the most relevant chunks.

    Returns the model's answer along with the top-k most relevant chunks used to generate the answer.
    The response includes:
    - The answer text with appropriate disclaimers if applicable
    - Flags indicating if the answer is based on general knowledge or contains DIY advice
    - Source information tracing the chunks used to generate the answer
    - The chunks themselves with their metadata
    """
    try:
        # Query the vector store using cosine similarity
        results = retriever.query(request.question, request.top_k)

        # Format the results
        chunks = []
        for i in range(len(results["documents"][0])):
            chunk = ChunkResult(
                text=results["documents"][0][i],
                metadata=(
                    results["metadatas"][0][i]
                    if "metadatas" in results and results["metadatas"][0]
                    else None
                ),
                distance=(
                    results["distances"][0][i]
                    if "distances" in results and results["distances"][0]
                    else None
                ),
            )
            chunks.append(chunk)

        # Generate an answer using OpenAI's GPT model
        answer_result = retriever.generate_answer(request.question, chunks)

        return AskResponse(
            question=request.question,
            chunks=chunks,
            answer=answer_result["answer"],
            is_general_knowledge=answer_result["is_general_knowledge"],
            contains_diy_advice=answer_result["contains_diy_advice"],
            source_info=answer_result["source_info"],
            success=True,
            message="Question answered successfully with AI-generated response",
        )

    except Exception as e:
        return AskResponse(
            question=request.question,
            chunks=[],
            success=False,
            message=f"Error: {str(e)}",
        )
</file>

<file path="app/retriever/ask.py">
"""Retriever module for querying and retrieving information from embeddings."""

from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings
from pathlib import Path
from openai import OpenAI
from typing import Dict, List, Any

from app.config import OPENAI_API_KEY, CHROMA_DB_PATH
from app.logging_config import get_logger

# Get logger for this module
logger = get_logger("retriever.ask")

# Load environment variables
load_dotenv()

# Initialize OpenAI client
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is not set")

# Initialize OpenAI client with proper error handling for proxy settings
client = OpenAI(api_key=OPENAI_API_KEY)


class Retriever:
    """Class for retrieving information from embeddings and generating answers using OpenAI.

    This class provides methods to query a vector database for relevant documents
    and generate answers to user questions using OpenAI's language models.
    """

    def __init__(self) -> None:
        """Initialize the retriever with ChromaDB connection.

        Sets up connection to the ChromaDB vector database and initializes
        the documents collection.
        """
        # Get the path for the Chroma database
        self.chroma_db_path = CHROMA_DB_PATH

        # Create the directory if it doesn't exist
        Path(self.chroma_db_path).mkdir(parents=True, exist_ok=True)

        # Initialize the persistent client
        self.chroma_client = chromadb.PersistentClient(
            path=self.chroma_db_path, settings=Settings(anonymized_telemetry=False)
        )

        # Get the collection
        self.collection = self.chroma_client.get_or_create_collection(
            "metropole_documents"
        )

    def query(self, query_text: str, n_results: int = 5) -> Dict[str, Any]:
        """Query the embeddings database for relevant documents.

        Args:
            query_text: The query text to search for.
            n_results: Number of results to return. Defaults to 5.

        Returns:
            Dictionary containing query results with documents, metadatas, and distances.
        """
        # Query using cosine similarity
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results,
            include=["documents", "metadatas", "distances"],
        )

        # Convert to a standard dictionary
        return dict(results)

    def generate_answer(
        self, question: str, chunks: List[Any], model: str = "gpt-3.5-turbo"
    ) -> Dict[str, Any]:
        """Generate an answer to a question using OpenAI's GPT model and retrieved chunks.

        Uses the provided text chunks to generate a contextually relevant answer
        to the user's question. Adds appropriate disclaimers based on the nature
        of the answer.

        Args:
            question: The user's question.
            chunks: List of text chunks retrieved from the vector store.
            model: The OpenAI model to use. Defaults to "gpt-3.5-turbo".

        Returns:
            A dictionary containing:
                - answer: The generated answer text
                - is_general_knowledge: Flag indicating if answer is based on general knowledge
                - contains_diy_advice: Flag indicating if answer contains DIY advice
                - source_info: Information about the sources used
        """
        # Construct the prompt with the retrieved chunks
        prompt = self._construct_prompt(question, chunks)

        # Call the OpenAI API
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant that answers questions based on the provided building content.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,  # Lower temperature for more factual responses
            max_tokens=1000,  # Limit response length
        )

        answer_text = response.choices[0].message.content

        # Process the answer to detect if it's based on building data or general knowledge
        # and if it contains DIY advice
        answer_lower = answer_text.lower() if answer_text else ""
        is_general_knowledge = (
            "general knowledge" in answer_lower
            or "i don't have specific information" in answer_lower
        )
        contains_diy_advice = any(
            phrase in answer_lower
            for phrase in [
                "diy",
                "do it yourself",
                "you can try",
                "you could try",
                "steps to",
                "how to",
            ]
        )

        # Prepare source information
        source_info = self._prepare_source_info(chunks)

        return {
            "answer": answer_text,
            "is_general_knowledge": is_general_knowledge,
            "contains_diy_advice": contains_diy_advice,
            "source_info": source_info,
        }

    def _prepare_source_info(self, chunks: List[Any]) -> str:
        """Prepare formatted source information from chunks.

        Extracts metadata from chunks and formats it into a readable source citation.

        Args:
            chunks: List of text chunks retrieved from the vector store.

        Returns:
            Formatted string containing source information.
        """
        sources = []
        for i, chunk in enumerate(chunks):
            # Extract metadata
            metadata = (
                chunk.metadata if hasattr(chunk, "metadata") and chunk.metadata else {}
            )
            chunk_id = str(metadata.get("chunk_id", f"unknown-{i}"))
            page_title = str(metadata.get("page_title", "Unknown Page"))
            section = metadata.get("section_header", "")

            # Format source info using f-strings to avoid None concatenation
            if section and isinstance(section, str):
                source_info = f"Chunk {chunk_id} ({section}) from {page_title}"
            else:
                source_info = f"Chunk {chunk_id} from {page_title}"

            sources.append(source_info)

        return "; ".join(sources)

    def _construct_prompt(self, question: str, chunks: List[Any]) -> str:
        """Construct a prompt for the OpenAI model that includes the question and retrieved chunks.

        Creates a detailed prompt that includes the user's question, relevant content
        chunks with their metadata, and instructions for the model on how to formulate
        the answer.

        Args:
            question: The user's question.
            chunks: List of text chunks retrieved from the vector store.

        Returns:
            The constructed prompt string ready to be sent to the OpenAI API.
        """
        # Start with the question
        prompt = f"Question: {question}\n\n"

        # Add the building content from the retrieved chunks
        prompt += "Building Content:\n"
        for i, chunk in enumerate(chunks):
            # Extract text and metadata
            text = str(chunk.text if hasattr(chunk, "text") else chunk)
            metadata = (
                chunk.metadata if hasattr(chunk, "metadata") and chunk.metadata else {}
            )

            # Add source information if available
            chunk_id = str(metadata.get("chunk_id", f"unknown-{i}"))
            page_title = str(metadata.get("page_title", "Unknown Page"))
            section = metadata.get("section_header", "")
            url = metadata.get("url", "")

            # Build source info parts
            source_parts = [f"ID: {chunk_id}"]
            if section and isinstance(section, str):
                source_parts.append(f"Section: {section}")
            if page_title:
                source_parts.append(f"Page: {page_title}")
            if url and isinstance(url, str):
                source_parts.append(f"URL: {url}")

            # Join parts with commas
            source_info = f" ({', '.join(source_parts)})"

            prompt += f"Chunk {i+1}{source_info}:\n{text}\n\n"

        # Add instructions for the model
        prompt += """Based on the building content provided above, please answer the question. Do not reference the specific chunks you used to formulate your answer."""

        return prompt


if __name__ == "__main__":
    # Example usage
    retriever = Retriever()
    results = retriever.query("sample query")
    logger.info(f"Query results: {results}")
</file>

</files>
