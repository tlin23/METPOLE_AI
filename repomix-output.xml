This file is a merged representation of a subset of the codebase, containing specifically included files, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app/
  api/
    models.py
    routes.py
  crawler/
    crawl.py
    extract_content.py
    utils.py
  embedder/
    embed_corpus.py
  retriever/
    ask.py
  vector_store/
    __init__.py
    init_chroma.py
  config.py
  logging_config.py
tests/
  test_crawler/
    test_crawl.py
    test_extract_content.py
    test_utils.py
  test_embedder/
    test_embed_corpus.py
  test_retriever/
    test_ask.py
  test_pipeline.py
main.py
manage_git_index.py
run_pipeline.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="tests/test_crawler/test_utils.py">
import sys
from pathlib import Path
from unittest.mock import MagicMock

# Add project root to Python path (only needed if running outside pytest)
sys.path.append(str(Path(__file__).resolve().parents[2]))

from app.crawler import utils


def test_clean_text_removes_special_chars():
    raw = "Some\u200btext\nwith\u202apunctuations… and “quotes”."
    cleaned = utils.clean_text(raw)
    assert "\u200b" not in cleaned
    assert "\u202a" not in cleaned
    assert "\n" not in cleaned
    assert "..." in cleaned
    assert '"' in cleaned


def test_hash_id_consistency():
    text = "Same text"
    assert utils.hash_id(text) == utils.hash_id(text)


def test_extract_tags_with_short_text():
    mock_model = MagicMock()
    result = utils.extract_tags_with_keybert("Too short", mock_model)
    assert result == []


def test_extract_tags_with_keybert_calls_model():
    text = "This is a test sentence with enough content to extract meaningful tags."
    mock_model = MagicMock()
    mock_model.extract_keywords.return_value = [("tag1", 0.9), ("tag2", 0.8)]
    tags = utils.extract_tags_with_keybert(text, mock_model)
    mock_model.extract_keywords.assert_called_once()
    assert tags == ["tag1", "tag2"]
</file>

<file path="tests/test_pipeline.py">
import pytest
from unittest.mock import patch

from run_pipeline import run_pipeline


@patch("run_pipeline.recursive_crawl")
@patch("run_pipeline.process_all_html_files")
@patch("run_pipeline.embed_corpus_data")
def test_run_pipeline_all_steps_called(mock_embed, mock_process, mock_crawl):
    # Mock crawl output
    mock_crawl.return_value = {"http://example.com": "<html>Test</html>"}

    # Mock processed output
    mock_process.return_value = [
        {
            "chunk_id": "chunk_1",
            "page_id": "page_1",
            "content": "Example content",
            "tags": ["tag1"],
        }
    ]

    # Run pipeline
    run_pipeline(start_url="http://example.com", max_pages=1)

    # Assert all steps were called
    mock_crawl.assert_called_once_with(
        "http://example.com", max_pages=1, save_to_files=True
    )
    mock_process.assert_called_once()
    mock_embed.assert_called_once()


@patch("run_pipeline.recursive_crawl")
@patch("run_pipeline.process_all_html_files")
@patch("run_pipeline.embed_corpus_data")
def test_run_pipeline_error_handling(mock_embed, mock_process, mock_crawl):
    # Simulate crawl error
    mock_crawl.side_effect = Exception("Crawl failed")

    with pytest.raises(Exception, match="Crawl failed"):
        run_pipeline(start_url="http://example.com", max_pages=1)

    # Ensure remaining steps were not executed
    mock_process.assert_not_called()
    mock_embed.assert_not_called()
</file>

<file path="app/crawler/utils.py">
# app/crawler/utils.py

import re
import hashlib
from ftfy import fix_text
from keybert import KeyBERT


def clean_text(text: str) -> str:
    """Clean and normalize text by fixing encoding, stripping special characters, and collapsing whitespace."""
    text = fix_text(text)
    text = re.sub(r"[\u200b\u200e\u200f\u202a-\u202e]", "", text)  # invisible chars
    text = (
        text.replace("“", '"')
        .replace("”", '"')
        .replace("‘", "'")
        .replace("’", "'")
        .replace("–", "-")
        .replace("—", "-")
        .replace("…", "...")
    )
    text = re.sub(r"\s*\n\s*", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def normalize(text: str) -> str:
    text = text.lower()
    text = re.sub(r"\W+", " ", text)
    return " ".join(text.split())


def hash_id(text: str) -> str:
    normalized = normalize(text)
    return "chunk_" + hashlib.md5(normalized.encode("utf-8")).hexdigest()


def extract_tags_with_keybert(text: str, model: KeyBERT, num_tags: int = 5) -> list:
    text = text.strip()
    if not text or len(text.split()) < 10:
        return []
    keywords = model.extract_keywords(
        text,
        keyphrase_ngram_range=(1, 2),
        stop_words="english",
        use_mmr=True,
        diversity=0.7,
        top_n=num_tags,
    )
    return [kw for kw, _ in keywords]
</file>

<file path="app/vector_store/__init__.py">
"""
Vector store package for handling persistent vector embeddings.
"""

from app.vector_store.init_chroma import init_chroma_db

__all__ = ["init_chroma_db"]
</file>

<file path="app/config.py">
"""
Configuration settings for the application.

This module centralizes all environment variables used throughout the application.
"""

import os
from typing import Optional

# Chroma DB settings
CHROMA_DB_PATH: str = os.getenv("CHROMA_DB_PATH", "./data/index")

# OpenAI API settings
OPENAI_API_KEY: Optional[str] = os.getenv("OPENAI_API_KEY")

# Feedback logging settings
FEEDBACK_LOG_DIR: str = os.getenv("FEEDBACK_LOG_DIR", "data/logs")
FEEDBACK_LOG_FILE: str = os.getenv("FEEDBACK_LOG_FILE", "feedback.jsonl")
FEEDBACK_LOG_MAX_SIZE_MB: int = int(os.getenv("FEEDBACK_LOG_MAX_SIZE_MB", "10"))
FEEDBACK_LOG_MAX_BACKUPS: int = int(os.getenv("FEEDBACK_LOG_MAX_BACKUPS", "5"))
</file>

<file path="manage_git_index.py">
#!/usr/bin/env python3
"""
Script to manage Git index and .gitignore for Chroma index directories.
"""

import re
import subprocess
import sys
from pathlib import Path

INDEX_DIR = Path("./data/index")
GITIGNORE_PATH = Path("./.gitignore")


def get_latest_index_directory():
    if not INDEX_DIR.exists() or not INDEX_DIR.is_dir():
        print(f"Error: {INDEX_DIR} does not exist or is not a directory.")
        return None

    uuid_dirs = [
        d
        for d in INDEX_DIR.iterdir()
        if d.is_dir()
        and re.match(
            r"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$",
            d.name,
        )
    ]
    if not uuid_dirs:
        print("No index directories found.")
        return None

    latest_dir = max(
        uuid_dirs,
        key=lambda d: max(
            (f.stat().st_mtime for f in d.glob("**/*") if f.is_file()),
            default=d.stat().st_mtime,
        ),
    )
    return latest_dir


def update_gitignore(latest_dir):
    start_marker = "# BEGIN MANAGED CHROMA INDEX SECTION"
    end_marker = "# END MANAGED CHROMA INDEX SECTION"

    if not latest_dir:
        return False

    latest_dir_relative = latest_dir.relative_to(Path("."))

    new_section = f"""
{start_marker}
# Only include the latest index directory
data/index/*
!data/index/chroma.sqlite3
!{latest_dir_relative}/
{end_marker}
"""

    if GITIGNORE_PATH.exists():
        gitignore_content = GITIGNORE_PATH.read_text()
    else:
        gitignore_content = ""

    # Check if the gitignore would change
    if start_marker in gitignore_content and end_marker in gitignore_content:
        start = gitignore_content.find(start_marker)
        end = gitignore_content.find(end_marker) + len(end_marker)
        old_section = gitignore_content[start:end]
        if old_section.strip() == new_section.strip():
            return False  # No change needed
        else:
            gitignore_content = (
                gitignore_content[:start] + new_section + gitignore_content[end:]
            )
    else:
        gitignore_content = gitignore_content.strip() + "\n\n" + new_section

    # Ensure final newline
    if not gitignore_content.endswith("\n"):
        gitignore_content += "\n"

    GITIGNORE_PATH.write_text(gitignore_content)
    print(f"Updated .gitignore to allow latest index directory: {latest_dir.name}")
    return True


def remove_old_indexes_from_git(latest_dir_name):
    result = subprocess.run(
        ["git", "ls-files", "data/index/"], capture_output=True, text=True, check=True
    )
    tracked_files = result.stdout.strip().split("\n")
    tracked_dirs = set()

    uuid_pattern = (
        r"data/index/([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})/"
    )

    for file_path in tracked_files:
        match = re.match(uuid_pattern, file_path)
        if match:
            tracked_dirs.add(match.group(1))

    dirs_to_remove = [d for d in tracked_dirs if d != latest_dir_name]

    if not dirs_to_remove:
        return False

    for dir_name in dirs_to_remove:
        dir_path = f"data/index/{dir_name}"
        print(f"Removing {dir_path} from Git tracking...")
        subprocess.run(["git", "rm", "--cached", "-r", dir_path], check=True)

    return True


def main():
    latest_dir = get_latest_index_directory()
    if not latest_dir:
        print("No valid index directories found. Skipping.")
        sys.exit(0)

    modified = False
    modified |= update_gitignore(latest_dir)
    modified |= remove_old_indexes_from_git(latest_dir.name)

    if modified:
        # If we modified files, tell pre-commit to re-run
        sys.exit(1)
    else:
        # No changes needed
        sys.exit(0)


if __name__ == "__main__":
    main()
</file>

<file path="app/api/models.py">
"""
API models for the application.
"""

from pydantic import BaseModel
from typing import List, Optional, Dict, Any


class ChunkResult(BaseModel):
    text: str
    metadata: Optional[Dict[str, Any]] = None
    distance: Optional[float] = None


class AskRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5


class AskResponse(BaseModel):
    question: str
    chunks: List[ChunkResult]
    answer: Optional[str] = None
    is_general_knowledge: Optional[bool] = False
    contains_diy_advice: Optional[bool] = False
    source_info: Optional[str] = None
    success: bool
    message: str
    prompt: Optional[str] = None
</file>

<file path="app/logging_config.py">
"""
Centralized logging configuration for the application.

This module provides a standardized way to configure logging across the application.
It sets up a shared logger with consistent formatting and handlers.
"""

import os
import logging
import sys
from pathlib import Path
from logging.handlers import RotatingFileHandler


# Create logs directory if it doesn't exist
LOG_DIR = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "logs")
Path(LOG_DIR).mkdir(parents=True, exist_ok=True)


# Configure the root logger
def configure_logging(
    logger_name="metropole_ai",
    log_level=logging.INFO,
    log_format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    log_file=None,
    max_bytes=10 * 1024 * 1024,  # 10 MB
    backup_count=5,
    stream_handler=True,
):
    """
    Configure a logger with the specified parameters.

    Args:
        logger_name (str): Name of the logger.
        log_level (int): Logging level (e.g., logging.INFO, logging.DEBUG).
        log_format (str): Format string for log messages.
        log_file (str, optional): Path to the log file. If None, no file handler is added.
        max_bytes (int): Maximum size of the log file before rotation.
        backup_count (int): Number of backup log files to keep.
        stream_handler (bool): Whether to add a stream handler (console output).

    Returns:
        logging.Logger: The configured logger.
    """
    # Create logger
    logger = logging.getLogger(logger_name)
    logger.setLevel(log_level)

    # Remove existing handlers to avoid duplicates
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    # Create formatter
    formatter = logging.Formatter(log_format)

    # Add stream handler (console output)
    if stream_handler:
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    # Add file handler if log_file is specified
    if log_file:
        # Ensure the directory exists
        log_path = Path(log_file)
        log_path.parent.mkdir(parents=True, exist_ok=True)

        file_handler = RotatingFileHandler(
            log_file, maxBytes=max_bytes, backupCount=backup_count
        )
        file_handler.setFormatter(formatter)
        logger.addHandler(file_handler)

    return logger


# Create the default application logger
logger = configure_logging(
    logger_name="metropole_ai",
    log_level=logging.INFO,
    log_file=os.path.join(LOG_DIR, "metropole_ai.log"),
)


def get_logger(name=None):
    """
    Get a logger with the specified name.

    Args:
        name (str, optional): Name of the logger. If None, returns the root logger.

    Returns:
        logging.Logger: The logger.
    """
    if name is None:
        return logger

    return logging.getLogger(f"metropole_ai.{name}")
</file>

<file path="app/crawler/crawl.py">
"""Crawler module for fetching and processing web content."""

import requests
from bs4 import BeautifulSoup
import os
import urllib.parse
from datetime import datetime
from typing import Dict, List, Optional, Tuple, Any


def fetch_url(url: str) -> Optional[str]:
    """Fetch content from a URL.

    Args:
        url: The URL to fetch.

    Returns:
        The HTML content of the page if successful, None otherwise.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None


def parse_html(html: Optional[str]) -> Optional[BeautifulSoup]:
    """Parse HTML content using BeautifulSoup.

    Args:
        html: HTML content to parse.

    Returns:
        Parsed HTML as BeautifulSoup object if successful, None otherwise.
    """
    if html:
        return BeautifulSoup(html, "html.parser")
    return None


def extract_text(soup: Optional[BeautifulSoup]) -> Optional[str]:
    """Extract text content from parsed HTML.

    Args:
        soup: Parsed HTML as BeautifulSoup object.

    Returns:
        Extracted text content if successful, None otherwise.
    """
    if soup:
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.extract()

        # Get text
        text = soup.get_text()

        # Break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())

        # Break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

        # Drop blank lines
        text = "\n".join(chunk for chunk in chunks if chunk)

        return text
    return None


def extract_links(soup: Optional[BeautifulSoup], base_url: str) -> List[str]:
    """Extract all internal links from a parsed HTML page.

    Args:
        soup: Parsed HTML as BeautifulSoup object.
        base_url: The base URL of the website.

    Returns:
        List of internal links found in the HTML.
    """
    if not soup:
        return []

    links = []
    base_domain = urllib.parse.urlparse(base_url).netloc
    parsed_base = urllib.parse.urlparse(base_url)

    for a_tag in soup.find_all("a"):
        if not hasattr(a_tag, "get") or not a_tag.get("href"):
            continue
        href = str(a_tag.get("href", ""))

        # Handle relative URLs
        if href.startswith("/"):
            # Convert relative URL to absolute
            parsed_base = urllib.parse.urlparse(base_url)
            absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
            links.append(absolute_url)
        else:
            # Check if it's an internal link (same domain)
            parsed_href = urllib.parse.urlparse(href)
            if parsed_href.netloc == base_domain or not parsed_href.netloc:
                # If it's a fragment or query within the same page, skip
                if href.startswith("#") or href == "":
                    continue

                # If it's a relative path without leading slash
                if not parsed_href.netloc and not href.startswith("http"):
                    # Get the directory of the current URL
                    current_path = os.path.dirname(urllib.parse.urlparse(base_url).path)
                    if not current_path.endswith("/"):
                        current_path += "/"

                    # Join with the relative path
                    if current_path == "/":
                        absolute_url = (
                            f"{parsed_base.scheme}://{parsed_base.netloc}/{href}"
                        )
                    else:
                        absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{current_path}{href}"
                    links.append(absolute_url)
                elif parsed_href.netloc == base_domain:
                    links.append(href)

    return links


def recursive_crawl(
    start_url: str, max_pages: Optional[int] = None, save_to_files: bool = False
) -> Dict[str, str]:
    """Recursively crawl a website starting from a given URL.

    Args:
        start_url: The URL to start crawling from.
        max_pages: Maximum number of pages to crawl. None for unlimited.
        save_to_files: Whether to save HTML content to files.

    Returns:
        Dictionary mapping URLs to their HTML content.
    """
    visited = set()
    to_visit = [start_url]
    page_content = {}
    base_domain = urllib.parse.urlparse(start_url).netloc

    # Create data directory if saving to files
    if save_to_files:
        os.makedirs("data/html", exist_ok=True)

    page_count = 0

    while to_visit and (max_pages is None or page_count < max_pages):
        current_url = to_visit.pop(0)

        # Skip if already visited
        if current_url in visited:
            continue

        print(f"Crawling: {current_url}")

        # Fetch and parse the page
        html_content = fetch_url(current_url)
        if not html_content:
            visited.add(current_url)
            continue

        # Store the content
        page_content[current_url] = html_content
        page_count += 1

        # Save to file if requested
        if save_to_files:
            # Create a filename from the URL
            parsed_url = urllib.parse.urlparse(current_url)
            path = parsed_url.path
            if not path or path == "/":
                path = "/index"

            # Replace special characters
            filename = f"{parsed_url.netloc}{path}".replace("/", "_").replace(":", "_")
            if not filename.endswith(".html"):
                filename += ".html"

            with open(f"data/html/{filename}", "w", encoding="utf-8") as f:
                f.write(html_content)

        # Parse the HTML
        soup = parse_html(html_content)

        # Extract links
        links = extract_links(soup, current_url)

        # Mark as visited
        visited.add(current_url)

        # Add new links to visit
        for link in links:
            if link not in visited and link not in to_visit:
                # Ensure it's from the same domain
                link_domain = urllib.parse.urlparse(link).netloc
                if link_domain == base_domain:
                    to_visit.append(link)

    print(f"Crawling complete. Visited {len(visited)} pages.")
    return page_content


def crawl(url: str) -> Tuple[Optional[str], Dict[str, Any]]:
    """Crawl a URL and extract its text content.

    Args:
        url: The URL to crawl.

    Returns:
        A tuple containing:
            - The extracted text content (or None if extraction failed)
            - A metadata dictionary with information about the crawled page
    """
    html = fetch_url(url)
    soup = parse_html(html)
    text = extract_text(soup)

    # Extract metadata
    title = "No title"
    if soup and hasattr(soup, "title") and soup.title:
        title = str(soup.title.string) if soup.title.string else "No title"

    metadata = {
        "url": url,
        "title": title,
        "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "source": "web_crawl",
    }

    return text, metadata


if __name__ == "__main__":
    # Example usage for recursive crawling
    start_url = "https://www.metropoleballard.com/home"
    content_dict = recursive_crawl(start_url, max_pages=50, save_to_files=True)
    print(f"Collected {len(content_dict)} pages")
</file>

<file path="tests/test_crawler/test_crawl.py">
import sys
from unittest.mock import patch
from pathlib import Path

# Add project root to Python path when running locally
sys.path.append(str(Path(__file__).resolve().parents[2]))

from app.crawler import crawl


@patch("app.crawler.crawl.requests.get")
def test_fetch_url_success(mock_get):
    mock_get.return_value.status_code = 200
    mock_get.return_value.text = "<html>Hello</html>"
    result = crawl.fetch_url("http://example.com")
    assert result == "<html>Hello</html>"


@patch("app.crawler.crawl.requests.get", side_effect=Exception("Connection error"))
def test_fetch_url_failure(mock_get):
    result = crawl.fetch_url("http://example.com")
    assert result is None


def test_parse_html_returns_soup():
    html = "<html><body><p>Hello</p></body></html>"
    soup = crawl.parse_html(html)
    assert soup.find("p").text == "Hello"


def test_extract_text_removes_scripts():
    html = (
        "<html><head><script>alert(1);</script></head><body>Visible text</body></html>"
    )
    soup = crawl.parse_html(html)
    text = crawl.extract_text(soup)
    assert "alert" not in text
    assert "Visible text" in text


def test_extract_links_internal_and_relative():
    html = """
    <html><body>
    <a href="/about">About</a>
    <a href="http://example.com/contact">Contact</a>
    <a href="https://other.com">External</a>
    </body></html>
    """
    soup = crawl.parse_html(html)
    links = crawl.extract_links(soup, "http://example.com")
    assert "http://example.com/about" in links
    assert "http://example.com/contact" in links
    assert not any("other.com" in link for link in links)


@patch("app.crawler.crawl.fetch_url", return_value="<html><body>Test</body></html>")
def test_recursive_crawl_single_page(mock_fetch):
    output = crawl.recursive_crawl("http://example.com", max_pages=1)
    assert "http://example.com" in output
    assert "Test" in output["http://example.com"]


@patch(
    "app.crawler.crawl.fetch_url",
    return_value="<html><body><a href='/about'>More</a></body></html>",
)
def test_recursive_crawl_follows_internal_links(mock_fetch):
    output = crawl.recursive_crawl("http://example.com", max_pages=2)
    assert "http://example.com" in output
    assert any("about" in url for url in output)
</file>

<file path="tests/test_crawler/test_extract_content.py">
import sys
import json
from unittest.mock import patch, MagicMock
from pathlib import Path

# Only needed if running manually; ignored in real pytest discovery
if __name__ == "__main__":
    sys.path.append(str(Path(__file__).resolve().parents[2]))

from app.crawler import extract_content


@patch("app.crawler.extract_content.partition_html")
@patch("app.crawler.extract_content.RecursiveCharacterTextSplitter")
@patch("app.crawler.extract_content.extract_tags_with_keybert")
def test_process_all_html_files(
    mock_extract_tags, mock_splitter_class, mock_partition, tmp_path
):
    # Create temp HTML input
    html_dir = tmp_path / "html"
    html_dir.mkdir()
    html_file = html_dir / "test_file.html"
    html_file.write_text(
        "<html><body><p>This is a long enough paragraph to be tagged properly.</p></body></html>"
    )

    # Mock tag extraction directly
    mock_extract_tags.return_value = ["tag1", "tag2"]

    # Mock partition_html
    mock_element = MagicMock()
    mock_element.text = "This is a long enough paragraph to be tagged properly."
    mock_partition.return_value = [mock_element]

    # Mock text splitter
    mock_splitter = MagicMock()
    mock_splitter.split_text.return_value = [mock_element.text]
    mock_splitter_class.return_value = mock_splitter

    # Run extraction
    output_path = tmp_path / "corpus.json"
    result = extract_content.process_all_html_files(str(html_dir), str(output_path))

    # Assertions
    assert isinstance(result, list)
    assert len(result) == 1
    assert result[0]["chunk_id"].startswith("chunk_")
    assert result[0]["content"].startswith("This is a long enough paragraph")
    assert result[0]["tags"] == ["tag1", "tag2"]

    # Verify JSON output is written
    with output_path.open("r", encoding="utf-8") as f:
        saved = json.load(f)
        assert len(saved) == 1
        assert saved[0]["tags"] == ["tag1", "tag2"]
</file>

<file path="tests/test_embedder/test_embed_corpus.py">
import sys
import json
import pytest
from unittest.mock import patch, MagicMock
from pathlib import Path

# Add project root to Python path
sys.path.append(str(Path(__file__).resolve().parents[2]))

from app.embedder.embed_corpus import load_corpus, assert_unique_chunk_ids, embed_corpus


@pytest.fixture
def sample_corpus():
    return [
        {
            "chunk_id": "chunk_1",
            "page_id": "page_1",
            "page_title": "Title 1",
            "page_name": "page_name_1",
            "section_header": "Header 1",
            "content": "Sample content",
            "tags": ["tag1", "tag2"],
        },
        {
            "chunk_id": "chunk_2",
            "page_id": "page_2",
            "page_title": "Title 2",
            "page_name": "page_name_2",
            "section_header": "Header 2",
            "content": "More sample content",
            "tags": ["tag3"],
        },
    ]


def test_load_corpus(tmp_path, sample_corpus):
    corpus_path = tmp_path / "corpus.json"
    with open(corpus_path, "w", encoding="utf-8") as f:
        json.dump(sample_corpus, f)
    loaded = load_corpus(str(corpus_path))
    assert len(loaded) == 2
    assert loaded[0]["chunk_id"] == "chunk_1"


def test_assert_unique_chunk_ids_pass(sample_corpus):
    assert_unique_chunk_ids(sample_corpus)


def test_assert_unique_chunk_ids_fail():
    dupe_corpus = [{"chunk_id": "chunk_1"}, {"chunk_id": "chunk_1"}]
    with pytest.raises(ValueError, match="Duplicate chunk_id"):
        assert_unique_chunk_ids(dupe_corpus)


@patch("app.embedder.embed_corpus.chromadb.PersistentClient")
@patch("app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction")
def test_embed_corpus(mock_embed_fn_class, mock_chroma_class, tmp_path, sample_corpus):
    corpus_path = tmp_path / "corpus.json"
    with open(corpus_path, "w", encoding="utf-8") as f:
        json.dump(sample_corpus, f)

    mock_embed_fn = MagicMock()
    mock_embed_fn.embed_documents.return_value = [[0.1, 0.2, 0.3]]
    mock_embed_fn_class.return_value = mock_embed_fn

    mock_collection = MagicMock()
    mock_collection.count.return_value = 0
    mock_collection.add = MagicMock()
    mock_client = MagicMock()
    mock_client.list_collections.return_value = []
    mock_client.create_collection.return_value = mock_collection
    mock_chroma_class.return_value = mock_client

    embed_corpus(
        corpus_path=str(corpus_path),
        chroma_path=str(tmp_path),
        collection_name="test_collection",
        batch_size=1,
    )

    mock_collection.add.assert_called()
</file>

<file path="tests/test_retriever/test_ask.py">
import sys
import pytest
from unittest.mock import MagicMock, patch
from pathlib import Path

# Add project root to Python path for local test running
sys.path.append(str(Path(__file__).resolve().parents[2]))

from app.api.models import ChunkResult
from app.retriever.ask import Retriever


@pytest.fixture
def sample_chunks():
    return [
        ChunkResult(
            text="This is a test chunk",
            metadata={
                "chunk_id": "chunk_123",
                "page_title": "Test Page",
                "section_header": "Intro",
            },
            distance=0.1,
        ),
        ChunkResult(
            text="Another chunk of content",
            metadata={
                "chunk_id": "chunk_456",
                "page_title": "Another Page",
                "section_header": "FAQ",
            },
            distance=0.2,
        ),
    ]


@patch("app.retriever.ask.chromadb.PersistentClient")
@patch("app.retriever.ask.OpenAI")
def test_query_returns_results(mock_openai, mock_chroma):
    mock_collection = MagicMock()
    mock_collection.count.return_value = 2
    mock_collection.query.return_value = {
        "documents": [["doc1", "doc2"]],
        "metadatas": [[{"chunk_id": "chunk_1"}, {"chunk_id": "chunk_2"}]],
        "distances": [[0.1, 0.2]],
    }

    mock_chroma.return_value.get_or_create_collection.return_value = mock_collection

    retriever = Retriever()
    results = retriever.query("What is Metropole?")
    assert "documents" in results
    assert len(results["documents"][0]) == 2


def test_generate_answer_with_flags(sample_chunks):
    with patch.object(
        Retriever, "_construct_prompt", return_value="mock prompt"
    ), patch.object(
        Retriever, "_prepare_source_info", return_value="Chunk sources"
    ), patch(
        "app.retriever.ask.client.chat.completions.create"
    ) as mock_create:

        mock_create.return_value.choices = [
            MagicMock(
                message=MagicMock(
                    content="You can try these DIY tips to fix the issue."
                )
            )
        ]

        retriever = Retriever()
        result = retriever.generate_answer("How do I fix a leak?", sample_chunks)

        assert result["contains_diy_advice"] is True
        assert "DIY" in result["answer"]
        assert "Chunk sources" in result["source_info"]


def test_prepare_source_info_formatting(sample_chunks):
    retriever = Retriever()
    info = retriever._prepare_source_info(sample_chunks)
    assert "chunk_123" in info
    assert "Test Page" in info
    assert "chunk_456" in info


def test_construct_prompt_structure(sample_chunks):
    retriever = Retriever()
    prompt = retriever._construct_prompt("What are the amenities?", sample_chunks)
    assert "Question:" in prompt
    assert "Building Content:" in prompt
    assert "Chunk 1" in prompt
    assert "Section: Intro" in prompt
    assert "Page: Test Page" in prompt
</file>

<file path="main.py">
from dotenv import load_dotenv
import os
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
from pathlib import Path
from fastapi.middleware.cors import CORSMiddleware
from app.api.routes import router as api_router
from app.vector_store.init_chroma import init_chroma_db

# Load environment variables
load_dotenv()

# Set default Chroma DB path if not in environment
if not os.getenv("CHROMA_DB_PATH"):
    os.environ["CHROMA_DB_PATH"] = "./data/index"


# Define lifespan context manager
@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Lifespan context manager for FastAPI application.
    Handles startup and shutdown events.
    """
    # Startup: Initialize services
    chroma_db_path = os.getenv("CHROMA_DB_PATH")
    print(f"Initializing Chroma DB at: {chroma_db_path}")

    # Ensure the directory exists
    Path(chroma_db_path).mkdir(parents=True, exist_ok=True)

    # Initialize the Chroma DB
    init_chroma_db()

    yield

    # Shutdown: Clean up resources if needed
    print("Shutting down application...")


# Create FastAPI app with lifespan
app = FastAPI(
    title="MetPol AI",
    description="A FastAPI application for crawling, embedding, and retrieving information",
    version="0.1.0",
    lifespan=lifespan,
)

# Configure CORS
origins = [
    "http://localhost:5173",  # Local development frontend
    "https://metpol-ai-frontend.onrender.com",  # Render frontend
    "https://metpol-ai-frontend.vercel.app",  # Optional Vercel frontend
    "*",  # Allow all origins during development (remove in production)
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API routes
app.include_router(api_router, prefix="/api")


@app.get("/")
def read_root():
    return {"message": "Hello World"}


# The startup event is now handled by the lifespan context manager above


if __name__ == "__main__":
    # Print some environment info
    print(f"CHROMA_DB_PATH: {os.getenv('CHROMA_DB_PATH')}")

    # Run the FastAPI server
    print(
        "Run the FastAPI server at http://127.0.0.1:8000 with: uvicorn main:app --reload"
    )
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)
</file>

<file path="app/vector_store/init_chroma.py">
"""
Script to initialize a Chroma persistent vector store.
"""

import sys
from pathlib import Path
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.config import CHROMA_DB_PATH
from app.logging_config import get_logger

# Get logger for this module
logger = get_logger("vector_store.init_chroma")

# Load environment variables
load_dotenv()


def init_chroma_db():
    """
    Initialize a Chroma persistent vector store.

    Returns:
        chromadb.PersistentClient: The initialized Chroma client.
    """
    # Get the path for the Chroma database
    chroma_db_path = CHROMA_DB_PATH

    # Create the directory if it doesn't exist
    Path(chroma_db_path).mkdir(parents=True, exist_ok=True)

    logger.info(f"Initializing Chroma DB at: {chroma_db_path}")

    # Create a persistent client
    client = chromadb.PersistentClient(
        path=chroma_db_path, settings=Settings(anonymized_telemetry=False)
    )

    # Create a default collection if it doesn't exist
    collection = client.get_or_create_collection(
        name="metropole_documents",
        metadata={"description": "Main document collection for MetPol AI"},
    )

    logger.info(
        f"Collection '{collection.name}' initialized with {collection.count()} documents"
    )

    return client


if __name__ == "__main__":
    # Initialize the Chroma DB
    client = init_chroma_db()

    # List all collections
    collections = client.list_collections()
    logger.info(f"Available collections: {[c.name for c in collections]}")
</file>

<file path="run_pipeline.py">
#!/usr/bin/env python3
"""
Metropole.AI Pipeline Script

This script orchestrates the full data pipeline:
1. Crawl the website and save HTML files
2. Process HTML files to extract structured content
3. Add metadata and tags to the content
4. Embed the corpus and store in ChromaDB

Usage:
    python run_pipeline.py --start-url https://www.metropoleballard.com/home --max-pages 50
"""

import os
import sys
import argparse
import time
from typing import Dict, Optional

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import app modules
from app.logging_config import get_logger
from app.config import CHROMA_DB_PATH
from app.crawler.crawl import recursive_crawl
from app.crawler.extract_content import process_all_html_files
from app.embedder.embed_corpus import embed_corpus

# Get logger for this module
logger = get_logger("pipeline")


def crawl_website(start_url: str, max_pages: Optional[int] = None) -> Dict[str, str]:
    """
    Crawl the website and save HTML files.

    Args:
        start_url: The URL to start crawling from.
        max_pages: Maximum number of pages to crawl. None for unlimited.

    Returns:
        Dictionary mapping URLs to their HTML content.
    """
    logger.info(f"Starting website crawl from {start_url}")
    logger.info(f"Max pages: {max_pages if max_pages is not None else 'unlimited'}")

    # Create data directory if it doesn't exist
    os.makedirs("data/html", exist_ok=True)

    # Crawl the website
    start_time = time.time()
    content_dict = recursive_crawl(start_url, max_pages=max_pages, save_to_files=True)
    elapsed_time = time.time() - start_time

    logger.info(
        f"Crawling complete. Collected {len(content_dict)} pages in {elapsed_time:.2f} seconds"
    )
    return content_dict


def embed_corpus_data() -> None:
    """
    Embed the corpus and store in ChromaDB.
    """
    logger.info("Embedding corpus data")

    # Path to the corpus file
    corpus_path = os.path.join("data", "processed", "metropole_corpus.json")

    # Embed the corpus
    start_time = time.time()
    embed_corpus(
        corpus_path=corpus_path,
        chroma_path=CHROMA_DB_PATH,
        collection_name="metropole_documents",
        batch_size=100,
    )

    elapsed_time = time.time() - start_time
    logger.info(f"Corpus embedding complete in {elapsed_time:.2f} seconds")


def run_pipeline(start_url: str, max_pages: Optional[int] = None) -> None:
    """
    Run the full pipeline.

    Args:
        start_url: The URL to start crawling from.
        max_pages: Maximum number of pages to crawl. None for unlimited.
    """
    logger.info("Starting Metropole.AI pipeline")
    total_start_time = time.time()

    try:
        # Step 1: Crawl the website
        crawl_website(start_url, max_pages)

        # Step 2: Process HTML content
        process_all_html_files()

        # Step 3: Embed the corpus
        embed_corpus_data()

        total_elapsed_time = time.time() - total_start_time
        logger.info(f"Pipeline complete! Total time: {total_elapsed_time:.2f} seconds")

    except Exception:
        logger.exception("Pipeline failed due to an unexpected error.")
        raise


if __name__ == "__main__":
    # Parse command line arguments
    parser = argparse.ArgumentParser(description="Run the Metropole.AI pipeline.")
    parser.add_argument(
        "--start-url",
        type=str,
        default="https://www.metropoleballard.com/home",
        help="URL to start crawling from",
    )
    parser.add_argument(
        "--max-pages", type=int, default=50, help="Maximum number of pages to crawl"
    )

    args = parser.parse_args()

    # Run the pipeline
    run_pipeline(args.start_url, args.max_pages)
</file>

<file path="app/crawler/extract_content.py">
# app/crawler/extract_content.py

import os
import json
import uuid
from unstructured.partition.html import partition_html
from langchain.text_splitter import RecursiveCharacterTextSplitter
from keybert import KeyBERT
from app.crawler.utils import (
    clean_text,
    hash_id,
    extract_tags_with_keybert,
)  # Move helpers to a utils module


def process_all_html_files(
    input_dir="data/html", output_path="data/processed/metropole_corpus.json"
):
    print("Initializing KeyBERT with MiniLM model...")
    model = KeyBERT(model="all-MiniLM-L6-v2")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)

    all_chunks = []
    seen_hashes = set()
    page_ids = {}

    for root, _, files in os.walk(input_dir):
        for file in files:
            if not file.endswith(".html"):
                continue

            file_path = os.path.join(root, file)
            with open(file_path, "r", encoding="utf-8") as f:
                html = f.read()

            elements = partition_html(text=html)
            full_text = "\n".join([el.text for el in elements if hasattr(el, "text")])
            chunks = text_splitter.split_text(full_text)

            page_name = file.replace(".html", "")
            page_id = page_ids.setdefault(page_name, f"page_{str(uuid.uuid4())[:8]}")
            page_title = (
                f"metropoleballard.com - {page_name.split('_')[-1].capitalize()}"
            )

            for chunk_text in chunks:
                chunk_text = clean_text(chunk_text)
                chunk_id = hash_id(chunk_text)
                if chunk_id in seen_hashes or len(chunk_text) < 20:
                    continue

                tags = extract_tags_with_keybert(chunk_text, model)
                all_chunks.append(
                    {
                        "chunk_id": chunk_id,
                        "page_id": page_id,
                        "page_name": page_name,
                        "page_title": page_title,
                        "section_header": "Auto",
                        "content": chunk_text,
                        "tags": tags,
                    }
                )
                seen_hashes.add(chunk_id)

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(all_chunks, f, indent=2, ensure_ascii=False)

    print(f"[extract_content.py] Extracted {len(all_chunks)} chunks to {output_path}")
    return all_chunks


if __name__ == "__main__":
    html_directory = "data/html"
    output_directory = "data/processed"
    os.makedirs(output_directory, exist_ok=True)
    output_file = os.path.join(output_directory, "metropole_corpus.json")
    process_all_html_files(html_directory, output_file)
    print(f"[extract_content.py] Processed HTML files and saved to {output_file}")
</file>

<file path="app/embedder/embed_corpus.py">
#!/usr/bin/env python3
"""
Script to load the metropole_corpus.json file, embed each chunk using all-MiniLM-L6-v2,
and store the text and metadata in Chroma.
"""

import sys
import json
import time
from pathlib import Path
from collections import Counter
from typing import Dict, List, Any

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions

from app.config import CHROMA_DB_PATH
from app.logging_config import get_logger

# Get logger for this module
logger = get_logger("embedder.embed_corpus")


def load_corpus(corpus_path: str) -> List[Dict[str, Any]]:
    """
    Load the corpus from a JSON file.

    Args:
        corpus_path (str): Path to the corpus file.

    Returns:
        List[Dict[str, Any]]: The loaded corpus.
    """
    logger.info(f"Loading corpus from {corpus_path}")
    try:
        with open(corpus_path, "r", encoding="utf-8") as f:
            corpus = json.load(f)
        logger.info(f"Successfully loaded corpus with {len(corpus)} chunks")
        return corpus
    except Exception as e:
        logger.error(f"Error loading corpus: {e}")
        raise


def assert_unique_chunk_ids(corpus: List[Dict[str, Any]]):
    ids = [chunk["chunk_id"] for chunk in corpus]
    dupes = [id for id, count in Counter(ids).items() if count > 1]
    if dupes:
        raise ValueError(f"Duplicate chunk_id(s) found: {dupes}")


def embed_corpus(
    corpus_path: str = "./data/processed/metropole_corpus.json",
    chroma_path: str = CHROMA_DB_PATH,
    collection_name: str = "metropole_documents",
    batch_size: int = 100,
) -> None:
    """
    Embed the corpus using all-MiniLM-L6-v2 and store in Chroma.

    Args:
        corpus_path (str): Path to the corpus file.
        chroma_path (Optional[str]): Path to the Chroma DB. If None, uses the CHROMA_DB_PATH env var or default.
        collection_name (str): Name of the collection to store embeddings in.
        batch_size (int): Number of documents to embed in each batch.
    """
    start_time = time.time()

    # Create the directory if it doesn't exist
    Path(chroma_path).mkdir(parents=True, exist_ok=True)

    logger.info(f"Initializing Chroma DB at: {chroma_path}")

    # Initialize the embedding function
    # Note: DefaultEmbeddingFunction uses the 'all-MiniLM-L6-v2' model internally
    logger.info("Loading default embedding model (all-MiniLM-L6-v2)...")
    embedding_function = embedding_functions.DefaultEmbeddingFunction()

    # Initialize the Chroma client
    client = chromadb.PersistentClient(
        path=chroma_path, settings=Settings(anonymized_telemetry=False)
    )

    # Check if the collection already exists and has documents
    existing_collections = {col.name: col for col in client.list_collections()}
    if collection_name in existing_collections:
        collection = client.get_collection(collection_name)
        count = collection.count()
        if count > 0:
            logger.info(
                f"Deleting and recreating collection '{collection_name}' with {count} documents"
            )
            client.delete_collection(collection_name)
            collection = client.create_collection(
                name=collection_name,
                embedding_function=embedding_function,
                metadata={"description": "Metropole corpus embeddings"},
            )
        else:
            collection._embedding_function = embedding_function
    else:
        collection = client.create_collection(
            name=collection_name,
            embedding_function=embedding_function,
            metadata={"description": "Metropole corpus embeddings"},
        )

    # Load the corpus
    corpus = load_corpus(corpus_path)
    assert_unique_chunk_ids(corpus)

    # Prepare data for embedding
    total_chunks = len(corpus)
    logger.info(f"Preparing to embed {total_chunks} chunks")

    # Process in batches
    total_batches = (total_chunks + batch_size - 1) // batch_size
    total_embedded = 0

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch = corpus[start_idx:end_idx]

        # Extract data for this batch
        ids = [chunk["chunk_id"] for chunk in batch]
        documents = [
            f"[Tags: {', '.join(chunk['tags'])}]\n{chunk['content']}" for chunk in batch
        ]
        metadatas = [
            {
                "page_id": chunk["page_id"],
                "page_title": chunk["page_title"],
                "page_name": chunk["page_name"],
                "section_header": chunk["section_header"],
                "tags": (
                    ",".join(chunk["tags"])
                    if isinstance(chunk["tags"], list)
                    else chunk["tags"]
                ),
            }
            for chunk in batch
        ]

        # Add to collection
        logger.info(
            f"Embedding batch {batch_idx + 1}/{total_batches} ({len(batch)} chunks)"
        )
        collection.add(ids=ids, documents=documents, metadatas=metadatas)

        total_embedded += len(batch)
        logger.info(f"Progress: {total_embedded}/{total_chunks} chunks embedded")

    # Log completion
    elapsed_time = time.time() - start_time
    logger.info(
        f"Embedding complete! {total_embedded} chunks embedded in {elapsed_time:.2f} seconds"
    )
    logger.info(
        f"Collection '{collection_name}' now contains {collection.count()} documents"
    )


if __name__ == "__main__":
    embed_corpus()
</file>

<file path="app/api/routes.py">
"""
API routes for the application.
"""

from fastapi import APIRouter

from app.api.models import AskRequest, ChunkResult, AskResponse
from app.retriever.ask import Retriever

# Create router
router = APIRouter()

# Initialize components
retriever = Retriever()


@router.post("/ask", response_model=AskResponse)
async def ask_question(request: AskRequest):
    """
    Ask a question and get an answer generated by OpenAI's GPT model based on the most relevant chunks.

    Returns the model's answer along with the top-k most relevant chunks used to generate the answer.
    The response includes:
    - The answer text with appropriate disclaimers if applicable
    - Flags indicating if the answer is based on general knowledge or contains DIY advice
    - Source information tracing the chunks used to generate the answer
    - The chunks themselves with their metadata
    """
    try:
        # Query the vector store using cosine similarity
        results = retriever.query(request.question, request.top_k)

        # Format the results
        chunks = []
        for i in range(len(results["documents"][0])):
            chunk = ChunkResult(
                text=results["documents"][0][i],
                metadata=(
                    results["metadatas"][0][i]
                    if "metadatas" in results and results["metadatas"][0]
                    else None
                ),
                distance=(
                    results["distances"][0][i]
                    if "distances" in results and results["distances"][0]
                    else None
                ),
            )
            chunks.append(chunk)

        # Generate an answer using OpenAI's GPT model
        answer_result = retriever.generate_answer(request.question, chunks)

        return AskResponse(
            question=request.question,
            chunks=chunks,
            answer=answer_result["answer"],
            is_general_knowledge=answer_result["is_general_knowledge"],
            contains_diy_advice=answer_result["contains_diy_advice"],
            source_info=answer_result["source_info"],
            success=True,
            message="Question answered successfully with AI-generated response",
            prompt=answer_result["prompt"],
        )

    except Exception as e:
        return AskResponse(
            question=request.question,
            chunks=[],
            success=False,
            message=f"Error: {str(e)}",
        )
</file>

<file path="app/retriever/ask.py">
"""Retriever module for querying and retrieving information from embeddings."""

from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings
from pathlib import Path
from openai import OpenAI
from typing import Dict, List, Any

from app.api.models import ChunkResult
from app.config import OPENAI_API_KEY, CHROMA_DB_PATH
from app.logging_config import get_logger

# Get logger for this module
logger = get_logger("retriever.ask")

# Load environment variables
load_dotenv()

# Initialize OpenAI client
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY environment variable is not set")

# Initialize OpenAI client with proper error handling for proxy settings
client = OpenAI(api_key=OPENAI_API_KEY)


class Retriever:
    """Class for retrieving information from embeddings and generating answers using OpenAI.

    This class provides methods to query a vector database for relevant documents
    and generate answers to user questions using OpenAI's language models.
    """

    def __init__(self) -> None:
        """Initialize the retriever with ChromaDB connection.

        Sets up connection to the ChromaDB vector database and initializes
        the documents collection.
        """
        # Get the path for the Chroma database
        self.chroma_db_path = CHROMA_DB_PATH

        # Create the directory if it doesn't exist
        Path(self.chroma_db_path).mkdir(parents=True, exist_ok=True)

        # Initialize the persistent client
        self.chroma_client = chromadb.PersistentClient(
            path=self.chroma_db_path, settings=Settings(anonymized_telemetry=False)
        )

        # Get the collection
        self.collection = self.chroma_client.get_or_create_collection(
            "metropole_documents"
        )

        logger.info(f"Collection loaded: {self.collection.count()} documents found")

    def query(self, query_text: str, n_results: int = 5) -> chromadb.QueryResult:
        """Query the embeddings database for relevant documents.

        Args:
            query_text: The query text to search for.
            n_results: Number of results to return. Defaults to 5.

        Returns:
            Dictionary containing query results with documents, metadatas, and distances.
        """
        # Query using cosine similarity
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results,
            include=["documents", "metadatas", "distances"],
        )

        # Convert to a standard dictionary
        return results

    def generate_answer(
        self, question: str, chunks: List[ChunkResult], model: str = "gpt-3.5-turbo"
    ) -> Dict[str, Any]:
        """Generate an answer to a question using OpenAI's GPT model and retrieved chunks.

        Uses the provided text chunks to generate a contextually relevant answer
        to the user's question. Adds appropriate disclaimers based on the nature
        of the answer.

        Args:
            question: The user's question.
            chunks: List of text chunks retrieved from the vector store.
            model: The OpenAI model to use. Defaults to "gpt-3.5-turbo".

        Returns:
            A dictionary containing:
                - answer: The generated answer text
                - is_general_knowledge: Flag indicating if answer is based on general knowledge
                - contains_diy_advice: Flag indicating if answer contains DIY advice
                - source_info: Information about the sources used
        """
        # Construct the prompt with the retrieved chunks
        prompt = self._construct_prompt(question, chunks)

        # Call the OpenAI API
        response = client.chat.completions.create(
            model=model,
            messages=[
                {
                    "role": "system",
                    "content": "You are a helpful assistant that answers questions based on the provided building content.",
                },
                {"role": "user", "content": prompt},
            ],
            temperature=0.3,  # Lower temperature for more factual responses
            max_tokens=1000,  # Limit response length
        )

        answer_text = response.choices[0].message.content

        # Process the answer to detect if it's based on building data or general knowledge
        # and if it contains DIY advice
        answer_lower = answer_text.lower() if answer_text else ""
        is_general_knowledge = (
            "general knowledge" in answer_lower
            or "i don't have specific information" in answer_lower
        )
        contains_diy_advice = any(
            phrase in answer_lower
            for phrase in [
                "diy",
                "do it yourself",
                "you can try",
                "you could try",
                "steps to",
                "how to",
            ]
        )

        # Prepare source information
        source_info = self._prepare_source_info(chunks)

        return {
            "answer": answer_text,
            "is_general_knowledge": is_general_knowledge,
            "contains_diy_advice": contains_diy_advice,
            "source_info": source_info,
            "prompt": prompt,
        }

    def _prepare_source_info(self, chunks: List[ChunkResult]) -> str:
        """Prepare formatted source information from chunks.

        Extracts metadata from chunks and formats it into a readable source citation.

        Args:
            chunks: List of text chunks retrieved from the vector store.

        Returns:
            Formatted string containing source information.
        """
        sources = []
        for i, chunk in enumerate(chunks):
            # Extract metadata
            metadata = (
                chunk.metadata if hasattr(chunk, "metadata") and chunk.metadata else {}
            )
            chunk_id = str(metadata.get("chunk_id", f"unknown-{i}"))
            page_title = str(metadata.get("page_title", "Unknown Page"))
            section = metadata.get("section_header", "")

            # Format source info using f-strings to avoid None concatenation
            if section and isinstance(section, str):
                source_info = f"Chunk {chunk_id} ({section}) from {page_title}"
            else:
                source_info = f"Chunk {chunk_id} from {page_title}"

            sources.append(source_info)

        return "; ".join(sources)

    def _construct_prompt(self, question: str, chunks: List[Any]) -> str:
        """Construct a prompt for the OpenAI model that includes the question and retrieved chunks.

        Creates a detailed prompt that includes the user's question, relevant content
        chunks with their metadata, and instructions for the model on how to formulate
        the answer.

        Args:
            question: The user's question.
            chunks: List of text chunks retrieved from the vector store.

        Returns:
            The constructed prompt string ready to be sent to the OpenAI API.
        """
        # Start with the question
        prompt = f"Question: {question}\n\n"

        prompt += """Based on the context provided below, please answer the question. Use only information that you deem relevant. Do not reference the specific chunks you used to formulate your answer. \n\n"""

        # Add the building content from the retrieved chunks
        prompt += "Building Content:\n"
        for i, chunk in enumerate(chunks):
            # Extract text and metadata
            text = str(chunk.text if hasattr(chunk, "text") else chunk)
            metadata = (
                chunk.metadata if hasattr(chunk, "metadata") and chunk.metadata else {}
            )

            # Add source information if available
            page_title = str(metadata.get("page_title", "Unknown Page"))
            section = metadata.get("section_header", "")

            # Build source info parts
            source_parts = []
            if section and isinstance(section, str):
                source_parts.append(f"Section: {section}")
            if page_title:
                source_parts.append(f"Page: {page_title}")

            # Join parts with commas
            source_info = f" ({', '.join(source_parts)})"

            prompt += f"Chunk {i+1}{source_info}:\n{text}\n\n"

        return prompt


if __name__ == "__main__":
    # Example usage
    retriever = Retriever()
    results = retriever.query("sample query")
    logger.info(f"Query results: {results}")
</file>

</files>
