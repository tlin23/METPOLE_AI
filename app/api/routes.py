"""
API routes for the application.
"""

import os
import sys
from fastapi import APIRouter
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

from app.crawler.crawl import crawl
from app.embedder.embed import Embedder
from app.retriever.ask import Retriever
from app.utils.helpers import save_feedback_log
from app.config import (
    FEEDBACK_LOG_DIR,
    FEEDBACK_LOG_FILE,
    FEEDBACK_LOG_MAX_SIZE_MB,
    FEEDBACK_LOG_MAX_BACKUPS,
)


# Add the project root to the Python path to allow importing from data/processed
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

# Create router
router = APIRouter()

# Initialize components
embedder = Embedder()
retriever = Retriever()


# Define models
class CrawlRequest(BaseModel):
    url: str


class CrawlResponse(BaseModel):
    url: str
    content: Optional[str] = None
    doc_id: Optional[str] = None
    success: bool
    message: str


class QueryRequest(BaseModel):
    query: str
    n_results: Optional[int] = 5


class QueryResponse(BaseModel):
    query: str
    results: List
    success: bool
    message: str


class AskRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5


class ChunkResult(BaseModel):
    text: str
    metadata: Optional[Dict[str, Any]] = None
    distance: Optional[float] = None


class AskResponse(BaseModel):
    question: str
    chunks: List[ChunkResult]
    answer: Optional[str] = None
    is_general_knowledge: Optional[bool] = False
    contains_diy_advice: Optional[bool] = False
    source_info: Optional[str] = None
    success: bool
    message: str


class FeedbackRequest(BaseModel):
    question: str
    response: str
    chunk_ids: Optional[List[str]] = None
    rating: Optional[int] = None
    comments: Optional[str] = None
    user_id: Optional[str] = None
    session_id: Optional[str] = None


class FeedbackResponse(BaseModel):
    success: bool
    message: str


@router.post("/crawl", response_model=CrawlResponse)
async def crawl_url(request: CrawlRequest):
    """
    Crawl a URL and store its content.
    """
    try:
        # Crawl the URL
        content, metadata = crawl(request.url)

        if not content:
            return CrawlResponse(
                url=request.url,
                success=False,
                message="Failed to extract content from URL",
            )

        # Embed the content with metadata
        doc_id = embedder.embed_text(content, metadata=metadata)

        return CrawlResponse(
            url=request.url,
            content=content[:100] + "..." if len(content) > 100 else content,
            doc_id=doc_id,
            success=True,
            message="URL crawled and embedded successfully",
        )

    except Exception as e:
        return CrawlResponse(url=request.url, success=False, message=f"Error: {str(e)}")


@router.post("/query", response_model=QueryResponse)
async def query_data(request: QueryRequest):
    """
    Query the embedded data.
    """
    try:
        results = retriever.query(request.query, request.n_results)

        return QueryResponse(
            query=request.query,
            results=results,
            success=True,
            message="Query executed successfully",
        )

    except Exception as e:
        return QueryResponse(
            query=request.query, results=[], success=False, message=f"Error: {str(e)}"
        )


@router.post("/ask", response_model=AskResponse)
async def ask_question(request: AskRequest):
    """
    Ask a question and get an answer generated by OpenAI's GPT model based on the most relevant chunks.

    Returns the model's answer along with the top-k most relevant chunks used to generate the answer.
    The response includes:
    - The answer text with appropriate disclaimers if applicable
    - Flags indicating if the answer is based on general knowledge or contains DIY advice
    - Source information tracing the chunks used to generate the answer
    - The chunks themselves with their metadata
    """
    try:
        # Query the vector store using cosine similarity
        results = retriever.query(request.question, request.top_k)

        # Format the results
        chunks = []
        for i in range(len(results["documents"][0])):
            chunk = ChunkResult(
                text=results["documents"][0][i],
                metadata=(
                    results["metadatas"][0][i]
                    if "metadatas" in results and results["metadatas"][0]
                    else None
                ),
                distance=(
                    results["distances"][0][i]
                    if "distances" in results and results["distances"][0]
                    else None
                ),
            )
            chunks.append(chunk)

        # Generate an answer using OpenAI's GPT model
        answer_result = retriever.generate_answer(request.question, chunks)

        return AskResponse(
            question=request.question,
            chunks=chunks,
            answer=answer_result["answer"],
            is_general_knowledge=answer_result["is_general_knowledge"],
            contains_diy_advice=answer_result["contains_diy_advice"],
            source_info=answer_result["source_info"],
            success=True,
            message="Question answered successfully with AI-generated response",
        )

    except Exception as e:
        return AskResponse(
            question=request.question,
            chunks=[],
            success=False,
            message=f"Error: {str(e)}",
        )


@router.post("/feedback", response_model=FeedbackResponse)
async def submit_feedback(request: FeedbackRequest):
    """
    Submit feedback about a question and response.

    Logs the question, response, timestamp, and chunk IDs to a local JSONL file.
    Additional metadata like rating, comments, user_id, and session_id can also be included.
    """
    try:
        # Prepare feedback data for logging
        feedback_data = {
            "question": request.question,
            "response": request.response,
            "chunk_ids": request.chunk_ids or [],
            "timestamp": None,  # Will be added by save_feedback_log
            "rating": request.rating,
            "comments": request.comments,
            "user_id": request.user_id,
            "session_id": request.session_id,
        }

        # Save to log file with rotation support
        success = save_feedback_log(
            feedback_data=feedback_data,
            log_dir=FEEDBACK_LOG_DIR,
            filename=FEEDBACK_LOG_FILE,
            max_size_mb=FEEDBACK_LOG_MAX_SIZE_MB,
            max_backups=FEEDBACK_LOG_MAX_BACKUPS,
        )

        if not success:
            return FeedbackResponse(
                success=False, message="Failed to save feedback to log file"
            )

        return FeedbackResponse(success=True, message="Feedback logged successfully")

    except Exception as e:
        return FeedbackResponse(success=False, message=f"Error: {str(e)}")
