This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: **/*.py
- Files matching these patterns are excluded: data/processed/content_objects.py
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
app/
  api/
    routes.py
  crawler/
    add_metadata_and_tags.py
    crawl.py
    example_usage_corpus.py
    example_usage.py
    extract_content.py
    process_html_content.py
    test_crawler.py
    test_metropole_corpus.py
  embedder/
    embed_corpus.py
    embed.py
    test_embeddings.py
    verify_embeddings.py
  retriever/
    ask.py
  utils/
    helpers.py
  vector_store/
    __init__.py
    demo.py
    init_chroma.py
data/
  processed/
    validate_corpus.py
tests/
  test_crawler/
    test_add_metadata_and_tags.py
    test_crawl.py
    test_extract_content.py
  test_embedder/
    test_embed_corpus.py
    test_embed.py
  test_integration/
    test_full_pipeline.py
  test_retriever/
    test_ask.py
  conftest.py
main.py
run_tests.py
test_deployment.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="app/api/routes.py">
"""
API routes for the application.
"""

import os
import json
import sys
from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Dict, Any

from app.crawler.crawl import crawl, recursive_crawl
from app.crawler.extract_content import process_all_html_files
from app.embedder.embed import Embedder
from app.embedder.embed_corpus import embed_corpus
from app.retriever.ask import Retriever
from app.utils.helpers import save_feedback_log


# Add the project root to the Python path to allow importing from data/processed
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

# Create router
router = APIRouter()

# Initialize components
embedder = Embedder()
retriever = Retriever()

# Configure logging
FEEDBACK_LOG_DIR = os.getenv("FEEDBACK_LOG_DIR", "data/logs")
FEEDBACK_LOG_FILE = os.getenv("FEEDBACK_LOG_FILE", "feedback.jsonl")
FEEDBACK_LOG_MAX_SIZE_MB = int(os.getenv("FEEDBACK_LOG_MAX_SIZE_MB", "10"))
FEEDBACK_LOG_MAX_BACKUPS = int(os.getenv("FEEDBACK_LOG_MAX_BACKUPS", "5"))


# Define models
class CrawlRequest(BaseModel):
    url: str


class CrawlAllResponse(BaseModel):
    pages_crawled: int
    chunks_processed: int
    success: bool
    message: str


class EmbedResponse(BaseModel):
    chunks_embedded: int
    success: bool
    message: str


class CrawlResponse(BaseModel):
    url: str
    content: Optional[str] = None
    doc_id: Optional[str] = None
    success: bool
    message: str


class QueryRequest(BaseModel):
    query: str
    n_results: Optional[int] = 5


class QueryResponse(BaseModel):
    query: str
    results: List
    success: bool
    message: str


class AskRequest(BaseModel):
    question: str
    top_k: Optional[int] = 5


class ChunkResult(BaseModel):
    text: str
    metadata: Optional[Dict[str, Any]] = None
    distance: Optional[float] = None


class AskResponse(BaseModel):
    question: str
    chunks: List[ChunkResult]
    answer: Optional[str] = None
    is_general_knowledge: Optional[bool] = False
    contains_diy_advice: Optional[bool] = False
    source_info: Optional[str] = None
    success: bool
    message: str


class FeedbackRequest(BaseModel):
    question: str
    response: str
    chunk_ids: Optional[List[str]] = None
    rating: Optional[int] = None
    comments: Optional[str] = None
    user_id: Optional[str] = None
    session_id: Optional[str] = None


class FeedbackResponse(BaseModel):
    success: bool
    message: str


@router.post("/crawl", response_model=CrawlResponse)
async def crawl_url(request: CrawlRequest):
    """
    Crawl a URL and store its content.
    """
    try:
        # Crawl the URL
        content, metadata = crawl(request.url)

        if not content:
            return CrawlResponse(
                url=request.url,
                success=False,
                message="Failed to extract content from URL"
            )

        # Embed the content with metadata
        doc_id = embedder.embed_text(content, metadata=metadata)

        return CrawlResponse(
            url=request.url,
            content=content[:100] + "..." if len(content) > 100 else content,
            doc_id=doc_id,
            success=True,
            message="URL crawled and embedded successfully"
        )

    except Exception as e:
        return CrawlResponse(
            url=request.url,
            success=False,
            message=f"Error: {str(e)}"
        )


@router.post("/query", response_model=QueryResponse)
async def query_data(request: QueryRequest):
    """
    Query the embedded data.
    """
    try:
        results = retriever.query(request.query, request.n_results)

        return QueryResponse(
            query=request.query,
            results=results,
            success=True,
            message="Query executed successfully"
        )

    except Exception as e:
        return QueryResponse(
            query=request.query,
            results=[],
            success=False,
            message=f"Error: {str(e)}"
        )


@router.post("/ask", response_model=AskResponse)
async def ask_question(request: AskRequest):
    """
    Ask a question and get an answer generated by OpenAI's GPT model based on the most relevant chunks.

    Returns the model's answer along with the top-k most relevant chunks used to generate the answer.
    The response includes:
    - The answer text with appropriate disclaimers if applicable
    - Flags indicating if the answer is based on general knowledge or contains DIY advice
    - Source information tracing the chunks used to generate the answer
    - The chunks themselves with their metadata
    """
    try:
        # Query the vector store using cosine similarity
        results = retriever.query(request.question, request.top_k)

        # Format the results
        chunks = []
        for i in range(len(results["documents"][0])):
            chunk = ChunkResult(
                text=results["documents"][0][i],
                metadata=results["metadatas"][0][i] if "metadatas" in results and results["metadatas"][0] else None,
                distance=results["distances"][0][i] if "distances" in results and results["distances"][0] else None
            )
            chunks.append(chunk)

        # Generate an answer using OpenAI's GPT model
        answer_result = retriever.generate_answer(request.question, chunks)

        return AskResponse(
            question=request.question,
            chunks=chunks,
            answer=answer_result["answer"],
            is_general_knowledge=answer_result["is_general_knowledge"],
            contains_diy_advice=answer_result["contains_diy_advice"],
            source_info=answer_result["source_info"],
            success=True,
            message="Question answered successfully with AI-generated response"
        )

    except Exception as e:
        return AskResponse(
            question=request.question,
            chunks=[],
            success=False,
            message=f"Error: {str(e)}"
        )


@router.post("/embed", response_model=EmbedResponse)
async def embed_corpus_endpoint():
    """
    Re-embed the contents of metropole_corpus.json and update the Chroma index.

    Returns a summary of how many chunks were embedded.
    """
    try:
        # Path to the corpus file
        corpus_path = os.path.join('data', 'processed', 'metropole_corpus.json')

        # Check if the corpus file exists
        if not os.path.exists(corpus_path):
            return EmbedResponse(
                chunks_embedded=0,
                success=False,
                message=f"Error: Corpus file not found at {corpus_path}"
            )

        # Load the corpus to get the number of chunks
        with open(corpus_path, 'r', encoding='utf-8') as f:
            corpus = json.load(f)

        chunks_count = len(corpus)

        # Get the Chroma DB path
        chroma_db_path = os.getenv("CHROMA_DB_PATH", "./data/index")

        # Embed the corpus
        embed_corpus(
            corpus_path=corpus_path,
            chroma_path=chroma_db_path,
            collection_name="documents",
            batch_size=100
        )

        return EmbedResponse(
            chunks_embedded=chunks_count,
            success=True,
            message=f"Successfully embedded {chunks_count} chunks into the Chroma index"
        )

    except Exception as e:
        return EmbedResponse(
            chunks_embedded=0,
            success=False,
            message=f"Error: {str(e)}"
        )


@router.post("/crawl_all", response_model=CrawlAllResponse)
async def crawl_all():
    """
    Run the crawler on the Metropole website, process the HTML files, and generate the metropole_corpus.json file.

    Returns a summary of how many pages and chunks were parsed.
    """
    try:
        # Step 1: Run the recursive crawler to fetch HTML content
        start_url = "https://www.metropoleballard.com/home"
        os.makedirs('data/html', exist_ok=True)

        content_dict = recursive_crawl(start_url, max_pages=50, save_to_files=True)
        pages_crawled = len(content_dict)

        # Step 2: Process HTML files to extract structured content
        html_directory = "data/html"
        results = process_all_html_files(html_directory)

        # Create output directory if it doesn't exist
        output_directory = "data/processed"
        os.makedirs(output_directory, exist_ok=True)

        # Save results to JSON file
        json_output_path = os.path.join(output_directory, "extracted_content.json")
        with open(json_output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        # Step 3: Create content objects
        all_chunks = []

        for file_path, content in results.items():
            page_title = content['title']
            page_name = os.path.basename(file_path).replace('.html', '')

            # Process each section
            for section in content['sections']:
                section_header = section['header']

                # Process each chunk in the section
                for chunk in section['chunks']:
                    # Create a structured object for each chunk
                    chunk_object = {
                        'page_title': page_title,
                        'page_name': page_name,
                        'section_header': section_header,
                        'content': chunk['content'],
                        'content_html': chunk['content_html']
                    }

                    all_chunks.append(chunk_object)

        # Save all chunks to a Python file
        py_output_path = os.path.join(output_directory, "content_objects.py")
        with open(py_output_path, 'w', encoding='utf-8') as f:
            f.write("\"\"\"Generated content objects from HTML files.\"\"\"\n\n")
            f.write("# This file is auto-generated. Do not edit directly.\n\n")
            f.write("content_objects = [\n")

            # Remove duplicate chunks by tracking content we've seen
            seen_content = set()
            unique_chunks = []

            for chunk in all_chunks:
                # Create a key based on content to detect duplicates
                content_key = f"{chunk['page_name']}:{chunk['section_header']}:{chunk['content'][:100]}"

                if content_key not in seen_content:
                    seen_content.add(content_key)
                    unique_chunks.append(chunk)

            # Write unique chunks to file
            for chunk in unique_chunks:
                f.write("    {\n")
                f.write(f"        'page_title': {json.dumps(chunk['page_title'])},\n")
                f.write(f"        'page_name': {json.dumps(chunk['page_name'])},\n")
                f.write(f"        'section_header': {json.dumps(chunk['section_header'])},\n")
                f.write(f"        'content': {json.dumps(chunk['content'])},\n")
                f.write(f"        'content_html': {json.dumps(chunk['content_html'])}\n")
                f.write("    },\n")

            f.write("]\n")

        # Step 4: Import the content objects and add metadata and tags
        try:
            # Dynamically import the content_objects module
            sys.path.insert(0, os.path.abspath(output_directory))
            from content_objects import content_objects

            # Import necessary functions from add_metadata_and_tags
            from app.crawler.add_metadata_and_tags import (
                generate_page_ids,
                extract_tags_with_keybert,
                save_to_json
            )

            # Initialize KeyBERT with MiniLM
            try:
                # Try to set the environment variable to disable HuggingFace Hub telemetry
                # This is a workaround for the "cannot import name 'HF_HUB_DISABLE_TELEMETRY'" error
                os.environ["HF_HUB_DISABLE_TELEMETRY"] = "1"

                from keybert import KeyBERT
                model = KeyBERT(model='all-MiniLM-L6-v2')
            except ImportError as e:
                # If KeyBERT fails to import, use a simple tag extraction function instead
                def extract_tags_with_keybert(text, model, num_tags=5):
                    """
                    Simple tag extraction function that uses word frequency as a fallback.
                    """
                    import re
                    from collections import Counter

                    # Skip empty or very short text
                    if not text or len(text) < 20:
                        return []

                    # Tokenize and clean text
                    words = re.findall(r'\b[a-zA-Z]{3,}\b', text.lower())

                    # Remove common stop words
                    stop_words = {'the', 'and', 'is', 'in', 'it', 'to', 'of', 'for', 'with', 'on', 'at', 'from', 'by', 'about', 'as', 'that', 'this', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did', 'but', 'or', 'an', 'a'}
                    filtered_words = [word for word in words if word not in stop_words]

                    # Count word frequencies
                    word_counts = Counter(filtered_words)

                    # Return the most common words as tags
                    return [word for word, count in word_counts.most_common(num_tags)]

            # Generate page IDs
            page_ids = generate_page_ids(content_objects)

            # Process each content object
            import uuid
            processed_objects = []

            for chunk in content_objects:
                # Generate a unique chunk ID
                chunk_id = f"chunk_{str(uuid.uuid4())}"

                # Get the page ID for this chunk
                page_id = page_ids[chunk['page_name']]

                # Extract tags using KeyBERT
                tags = extract_tags_with_keybert(chunk['content'], model)

                # Create a processed object with metadata and tags
                processed_object = {
                    'chunk_id': chunk_id,
                    'page_id': page_id,
                    'page_title': chunk['page_title'],
                    'page_name': chunk['page_name'],
                    'section_header': chunk['section_header'],
                    'content': chunk['content'],
                    'content_html': chunk['content_html'],
                    'tags': tags
                }

                processed_objects.append(processed_object)

            # Save to JSON
            output_path = os.path.join('data', 'processed', 'metropole_corpus.json')
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(processed_objects, f, indent=2, ensure_ascii=False)

            chunks_processed = len(processed_objects)

            return CrawlAllResponse(
                pages_crawled=pages_crawled,
                chunks_processed=chunks_processed,
                success=True,
                message=f"Successfully crawled {pages_crawled} pages and processed {chunks_processed} chunks"
            )

        except Exception as e:
            return CrawlAllResponse(
                pages_crawled=pages_crawled,
                chunks_processed=0,
                success=False,
                message=f"Error processing content objects: {str(e)}"
            )

    except Exception as e:
        return CrawlAllResponse(
            pages_crawled=0,
            chunks_processed=0,
            success=False,
            message=f"Error: {str(e)}"
        )


@router.post("/feedback", response_model=FeedbackResponse)
async def submit_feedback(request: FeedbackRequest):
    """
    Submit feedback about a question and response.

    Logs the question, response, timestamp, and chunk IDs to a local JSONL file.
    Additional metadata like rating, comments, user_id, and session_id can also be included.
    """
    try:
        # Prepare feedback data for logging
        feedback_data = {
            "question": request.question,
            "response": request.response,
            "chunk_ids": request.chunk_ids or [],
            "timestamp": None,  # Will be added by save_feedback_log
            "rating": request.rating,
            "comments": request.comments,
            "user_id": request.user_id,
            "session_id": request.session_id
        }

        # Save to log file with rotation support
        success = save_feedback_log(
            feedback_data=feedback_data,
            log_dir=FEEDBACK_LOG_DIR,
            filename=FEEDBACK_LOG_FILE,
            max_size_mb=FEEDBACK_LOG_MAX_SIZE_MB,
            max_backups=FEEDBACK_LOG_MAX_BACKUPS
        )

        if not success:
            return FeedbackResponse(
                success=False,
                message="Failed to save feedback to log file"
            )

        return FeedbackResponse(
            success=True,
            message="Feedback logged successfully"
        )

    except Exception as e:
        return FeedbackResponse(
            success=False,
            message=f"Error: {str(e)}"
        )
</file>

<file path="app/crawler/add_metadata_and_tags.py">
"""
Script to add metadata and tags to content chunks.

This script:
1. Loads content objects from data/processed/content_objects.py
2. Assigns a page_id, section header, and unique chunk_id to each chunk
3. Uses KeyBERT + MiniLM to extract 3-5 tags per chunk
4. Stores everything in a structured JSON file named metropole_corpus.json
"""

import os
import sys
import json
import uuid
from typing import List, Dict, Any
from keybert import KeyBERT

# Add the project root to the Python path to allow importing from data/processed
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

try:
    from data.processed.content_objects import content_objects
    print(f"Successfully loaded {len(content_objects)} content objects")
except ImportError:
    print("Error: Could not import content_objects. Make sure to run process_html_content.py first.")
    sys.exit(1)


def generate_page_ids(content_objects: List[Dict[str, Any]]) -> Dict[str, str]:
    """
    Generate unique page IDs for each unique page name.

    Args:
        content_objects (List[Dict[str, Any]]): List of content objects.

    Returns:
        Dict[str, str]: Dictionary mapping page names to page IDs.
    """
    # Get unique page names
    page_names = set(chunk['page_name'] for chunk in content_objects)

    # Generate a unique ID for each page name
    page_ids = {}
    for page_name in page_names:
        # Use a shortened UUID as the page ID
        page_ids[page_name] = f"page_{str(uuid.uuid4())[:8]}"

    return page_ids


def extract_tags_with_keybert(text: str, model: KeyBERT, num_tags: int = 5) -> List[str]:
    """
    Extract tags from text using KeyBERT with MiniLM.

    Args:
        text (str): Text to extract tags from.
        model (KeyBERT): KeyBERT model instance.
        num_tags (int, optional): Number of tags to extract. Defaults to 5.

    Returns:
        List[str]: List of extracted tags.
    """
    # Skip empty or very short text
    if not text or len(text) < 20:
        return []

    # Extract keywords
    keywords = model.extract_keywords(
        text,
        keyphrase_ngram_range=(1, 2),  # Extract single words and bigrams
        stop_words='english',
        use_mmr=True,  # Use Maximal Marginal Relevance to diversify results
        diversity=0.7,  # Higher diversity means more diverse results
        top_n=num_tags
    )

    # Return just the keywords (not the scores)
    return [keyword for keyword, _ in keywords]


def process_content_objects() -> List[Dict[str, Any]]:
    """
    Process content objects to add metadata and tags.

    Returns:
        List[Dict[str, Any]]: List of processed content objects with metadata and tags.
    """
    # Initialize KeyBERT with MiniLM
    print("Initializing KeyBERT with MiniLM model...")
    model = KeyBERT(model='all-MiniLM-L6-v2')

    # Generate page IDs
    page_ids = generate_page_ids(content_objects)

    # Process each content object
    processed_objects = []

    print(f"Processing {len(content_objects)} content objects...")
    for i, chunk in enumerate(content_objects):
        # Generate a unique chunk ID
        chunk_id = f"chunk_{str(uuid.uuid4())}"

        # Get the page ID for this chunk
        page_id = page_ids[chunk['page_name']]

        # Extract tags using KeyBERT
        tags = extract_tags_with_keybert(chunk['content'], model)

        # Create a processed object with metadata and tags
        processed_object = {
            'chunk_id': chunk_id,
            'page_id': page_id,
            'page_title': chunk['page_title'],
            'page_name': chunk['page_name'],
            'section_header': chunk['section_header'],
            'content': chunk['content'],
            'content_html': chunk['content_html'],
            'tags': tags
        }

        processed_objects.append(processed_object)

        # Print progress every 10 chunks
        if (i + 1) % 10 == 0:
            print(f"Processed {i + 1}/{len(content_objects)} chunks")

    return processed_objects


def save_to_json(processed_objects: List[Dict[str, Any]], output_path: str) -> None:
    """
    Save processed objects to a JSON file.

    Args:
        processed_objects (List[Dict[str, Any]]): List of processed content objects.
        output_path (str): Path to save the JSON file.
    """
    # Create output directory if it doesn't exist
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    # Save to JSON
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(processed_objects, f, indent=2, ensure_ascii=False)

    print(f"Saved {len(processed_objects)} processed objects to {output_path}")


def main():
    """Main function to process content objects and save to JSON."""
    # Process content objects
    processed_objects = process_content_objects()

    # Save to JSON
    output_path = os.path.join('data', 'processed', 'metropole_corpus.json')
    save_to_json(processed_objects, output_path)

    # Print summary
    page_count = len(set(obj['page_id'] for obj in processed_objects))
    section_count = len(set(obj['section_header'] for obj in processed_objects))
    tag_count = sum(len(obj['tags']) for obj in processed_objects)

    print(f"\nSummary:")
    print(f"- Processed {len(processed_objects)} content chunks")
    print(f"- From {page_count} unique pages")
    print(f"- With {section_count} unique sections")
    print(f"- Generated {tag_count} tags (avg. {tag_count/len(processed_objects):.1f} per chunk)")


if __name__ == "__main__":
    main()
</file>

<file path="app/crawler/crawl.py">
"""
Crawler module for fetching and processing web content.
"""

import requests
from bs4 import BeautifulSoup
import os
import urllib.parse
from collections import defaultdict
from datetime import datetime


def fetch_url(url):
    """
    Fetch content from a URL.

    Args:
        url (str): The URL to fetch.

    Returns:
        str: The HTML content of the page.
    """
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        return response.text
    except Exception as e:
        print(f"Error fetching {url}: {e}")
        return None


def parse_html(html):
    """
    Parse HTML content using BeautifulSoup.

    Args:
        html (str): HTML content to parse.

    Returns:
        BeautifulSoup: Parsed HTML.
    """
    if html:
        return BeautifulSoup(html, 'html.parser')
    return None


def extract_text(soup):
    """
    Extract text content from parsed HTML.

    Args:
        soup (BeautifulSoup): Parsed HTML.

    Returns:
        str: Extracted text content.
    """
    if soup:
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.extract()

        # Get text
        text = soup.get_text()

        # Break into lines and remove leading and trailing space on each
        lines = (line.strip() for line in text.splitlines())

        # Break multi-headlines into a line each
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))

        # Drop blank lines
        text = '\n'.join(chunk for chunk in chunks if chunk)

        return text
    return None


def extract_links(soup, base_url):
    """
    Extract all internal links from a parsed HTML page.

    Args:
        soup (BeautifulSoup): Parsed HTML.
        base_url (str): The base URL of the website.

    Returns:
        list: List of internal links.
    """
    if not soup:
        return []

    links = []
    base_domain = urllib.parse.urlparse(base_url).netloc

    for a_tag in soup.find_all('a', href=True):
        href = a_tag['href']

        # Handle relative URLs
        if href.startswith('/'):
            # Convert relative URL to absolute
            parsed_base = urllib.parse.urlparse(base_url)
            absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
            links.append(absolute_url)
        else:
            # Check if it's an internal link (same domain)
            parsed_href = urllib.parse.urlparse(href)
            if parsed_href.netloc == base_domain or not parsed_href.netloc:
                # If it's a fragment or query within the same page, skip
                if href.startswith('#') or href == '':
                    continue

                # If it's a relative path without leading slash
                if not parsed_href.netloc and not href.startswith('http'):
                    # Get the directory of the current URL
                    current_path = os.path.dirname(urllib.parse.urlparse(base_url).path)
                    if not current_path.endswith('/'):
                        current_path += '/'

                    # Join with the relative path
                    if current_path == '/':
                        absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}/{href}"
                    else:
                        absolute_url = f"{parsed_base.scheme}://{parsed_base.netloc}{current_path}{href}"
                    links.append(absolute_url)
                elif parsed_href.netloc == base_domain:
                    links.append(href)

    return links


def recursive_crawl(start_url, max_pages=None, save_to_files=False):
    """
    Recursively crawl a website starting from a given URL.

    Args:
        start_url (str): The URL to start crawling from.
        max_pages (int, optional): Maximum number of pages to crawl. None for unlimited.
        save_to_files (bool, optional): Whether to save HTML content to files.

    Returns:
        dict: Dictionary mapping URLs to their HTML content.
    """
    visited = set()
    to_visit = [start_url]
    page_content = {}
    base_domain = urllib.parse.urlparse(start_url).netloc

    # Create data directory if saving to files
    if save_to_files:
        os.makedirs('data/html', exist_ok=True)

    page_count = 0

    while to_visit and (max_pages is None or page_count < max_pages):
        current_url = to_visit.pop(0)

        # Skip if already visited
        if current_url in visited:
            continue

        print(f"Crawling: {current_url}")

        # Fetch and parse the page
        html_content = fetch_url(current_url)
        if not html_content:
            visited.add(current_url)
            continue

        # Store the content
        page_content[current_url] = html_content
        page_count += 1

        # Save to file if requested
        if save_to_files:
            # Create a filename from the URL
            parsed_url = urllib.parse.urlparse(current_url)
            path = parsed_url.path
            if not path or path == '/':
                path = '/index'

            # Replace special characters
            filename = f"{parsed_url.netloc}{path}".replace('/', '_').replace(':', '_')
            if not filename.endswith('.html'):
                filename += '.html'

            with open(f"data/html/{filename}", 'w', encoding='utf-8') as f:
                f.write(html_content)

        # Parse the HTML
        soup = parse_html(html_content)

        # Extract links
        links = extract_links(soup, current_url)

        # Mark as visited
        visited.add(current_url)

        # Add new links to visit
        for link in links:
            if link not in visited and link not in to_visit:
                # Ensure it's from the same domain
                link_domain = urllib.parse.urlparse(link).netloc
                if link_domain == base_domain:
                    to_visit.append(link)

    print(f"Crawling complete. Visited {len(visited)} pages.")
    return page_content


def crawl(url):
    """
    Crawl a URL and extract its text content.

    Args:
        url (str): The URL to crawl.

    Returns:
        tuple: (extracted text content, metadata dictionary)
    """
    html = fetch_url(url)
    soup = parse_html(html)
    text = extract_text(soup)

    # Extract metadata
    metadata = {
        "url": url,
        "title": soup.title.string if soup and soup.title else "No title",
        "crawl_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "source": "web_crawl"
    }

    return text, metadata


if __name__ == "__main__":
    # Example usage for recursive crawling
    start_url = "https://www.metropoleballard.com/home"
    content_dict = recursive_crawl(start_url, max_pages=50, save_to_files=True)
    print(f"Collected {len(content_dict)} pages")
</file>

<file path="app/crawler/example_usage_corpus.py">
"""
Example script demonstrating how to use the processed corpus with metadata and tags.
"""

import os
import sys
import json
from typing import List, Dict, Any

# Path to the processed corpus
CORPUS_PATH = os.path.join('data', 'processed', 'metropole_corpus.json')


def load_corpus() -> List[Dict[str, Any]]:
    """
    Load the processed corpus from JSON.

    Returns:
        List[Dict[str, Any]]: List of processed content objects.
    """
    try:
        with open(CORPUS_PATH, 'r', encoding='utf-8') as f:
            corpus = json.load(f)
        print(f"Successfully loaded {len(corpus)} content objects from {CORPUS_PATH}")
        return corpus
    except FileNotFoundError:
        print(f"Error: Could not find {CORPUS_PATH}. Make sure to run add_metadata_and_tags.py first.")
        sys.exit(1)
    except json.JSONDecodeError:
        print(f"Error: Could not parse {CORPUS_PATH}. The file may be corrupted.")
        sys.exit(1)


def print_corpus_summary(corpus: List[Dict[str, Any]]) -> None:
    """
    Print a summary of the corpus.

    Args:
        corpus (List[Dict[str, Any]]): List of processed content objects.
    """
    # Count unique pages, sections, and tags
    pages = set(chunk['page_id'] for chunk in corpus)
    sections = set(chunk['section_header'] for chunk in corpus)
    all_tags = [tag for chunk in corpus for tag in chunk['tags']]
    unique_tags = set(all_tags)

    print(f"\nCorpus Summary:")
    print(f"- Total chunks: {len(corpus)}")
    print(f"- Unique pages: {len(pages)}")
    print(f"- Unique sections: {len(sections)}")
    print(f"- Total tags: {len(all_tags)}")
    print(f"- Unique tags: {len(unique_tags)}")
    print(f"- Average tags per chunk: {len(all_tags) / len(corpus):.1f}")

    # Print the most common tags
    tag_counts = {}
    for tag in all_tags:
        tag_counts[tag] = tag_counts.get(tag, 0) + 1

    print("\nMost common tags:")
    for tag, count in sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  - {tag}: {count} occurrences")


def search_by_tag(corpus: List[Dict[str, Any]], tag: str) -> List[Dict[str, Any]]:
    """
    Search for content chunks with a specific tag.

    Args:
        corpus (List[Dict[str, Any]]): List of processed content objects.
        tag (str): Tag to search for.

    Returns:
        List[Dict[str, Any]]: List of matching content chunks.
    """
    return [chunk for chunk in corpus if tag in chunk['tags']]


def search_by_content(corpus: List[Dict[str, Any]], query: str) -> List[Dict[str, Any]]:
    """
    Search for content chunks containing a specific query string.

    Args:
        corpus (List[Dict[str, Any]]): List of processed content objects.
        query (str): Query string to search for.

    Returns:
        List[Dict[str, Any]]: List of matching content chunks.
    """
    query = query.lower()
    return [chunk for chunk in corpus if query in chunk['content'].lower()]


def get_chunks_by_page(corpus: List[Dict[str, Any]], page_id: str) -> List[Dict[str, Any]]:
    """
    Get all content chunks from a specific page.

    Args:
        corpus (List[Dict[str, Any]]): List of processed content objects.
        page_id (str): Page ID to filter by.

    Returns:
        List[Dict[str, Any]]: List of content chunks from the specified page.
    """
    return [chunk for chunk in corpus if chunk['page_id'] == page_id]


def get_chunks_by_section(corpus: List[Dict[str, Any]], section_header: str) -> List[Dict[str, Any]]:
    """
    Get all content chunks from a specific section.

    Args:
        corpus (List[Dict[str, Any]]): List of processed content objects.
        section_header (str): Section header to filter by.

    Returns:
        List[Dict[str, Any]]: List of content chunks from the specified section.
    """
    return [chunk for chunk in corpus if chunk['section_header'] == section_header]


def print_chunk_details(chunk: Dict[str, Any]) -> None:
    """
    Print details of a content chunk.

    Args:
        chunk (Dict[str, Any]): Content chunk.
    """
    print(f"Chunk ID: {chunk['chunk_id']}")
    print(f"Page: {chunk['page_title']} (ID: {chunk['page_id']})")
    print(f"Section: {chunk['section_header']}")
    print(f"Tags: {', '.join(chunk['tags'])}")
    print(f"Content: {chunk['content'][:100]}..." if len(chunk['content']) > 100 else f"Content: {chunk['content']}")
    print()


def main():
    """Main function demonstrating usage of the processed corpus."""
    # Load the corpus
    corpus = load_corpus()

    # Print corpus summary
    print_corpus_summary(corpus)

    # Example 1: Search for content by tag
    print("\nExample 1: Search for content with tag 'security'")
    security_results = search_by_tag(corpus, "security")
    print(f"Found {len(security_results)} chunks with tag 'security'")
    if security_results:
        print("First result:")
        print_chunk_details(security_results[0])

    # Example 2: Search for content by text
    print("\nExample 2: Search for content containing 'water'")
    water_results = search_by_content(corpus, "water")
    print(f"Found {len(water_results)} chunks containing 'water'")
    if water_results:
        print("First result:")
        print_chunk_details(water_results[0])

    # Example 3: Get content from a specific page
    if corpus:
        # Get the first page ID as an example
        example_page_id = corpus[0]['page_id']
        print(f"\nExample 3: Get content from page with ID '{example_page_id}'")
        page_results = get_chunks_by_page(corpus, example_page_id)
        print(f"Found {len(page_results)} chunks in page with ID '{example_page_id}'")

        # Example 4: Get content from a specific section
        if page_results:
            # Get the first section header as an example
            example_section = page_results[0]['section_header']
            print(f"\nExample 4: Get content from section '{example_section}'")
            section_results = get_chunks_by_section(corpus, example_section)
            print(f"Found {len(section_results)} chunks in section '{example_section}'")

    # Example 5: Find chunks with multiple specific tags
    print("\nExample 5: Find chunks with both 'building' and 'maintenance' tags")
    building_maintenance = [
        chunk for chunk in corpus
        if "building" in chunk['tags'] and "maintenance" in chunk['tags']
    ]
    print(f"Found {len(building_maintenance)} chunks with both 'building' and 'maintenance' tags")
    if building_maintenance:
        print("First result:")
        print_chunk_details(building_maintenance[0])


if __name__ == "__main__":
    main()
</file>

<file path="app/crawler/example_usage.py">
"""
Example script demonstrating how to use the extracted content.
"""

import os
import sys

# Add the project root to the Python path to allow importing from data/processed
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

try:
    from data.processed.content_objects import content_objects
    print(f"Successfully loaded {len(content_objects)} content objects")
except ImportError:
    print("Error: Could not import content_objects. Make sure to run process_html_content.py first.")
    sys.exit(1)


def print_content_summary():
    """Print a summary of the content objects."""
    # Count unique pages and sections
    pages = set(chunk['page_name'] for chunk in content_objects)
    sections = set(chunk['section_header'] for chunk in content_objects)

    print(f"\nContent Summary:")
    print(f"- Total chunks: {len(content_objects)}")
    print(f"- Unique pages: {len(pages)}")
    print(f"- Unique sections: {len(sections)}")

    # Print the first few pages and sections
    print("\nPages:")
    for page in sorted(list(pages))[:5]:
        print(f"  - {page}")
    if len(pages) > 5:
        print(f"  - ... and {len(pages) - 5} more")

    print("\nSections:")
    for section in sorted(list(sections))[:5]:
        print(f"  - {section}")
    if len(sections) > 5:
        print(f"  - ... and {len(sections) - 5} more")


def search_content(query):
    """
    Search for content containing the query string.

    Args:
        query (str): The search query.

    Returns:
        list: List of matching content chunks.
    """
    query = query.lower()
    results = []

    for chunk in content_objects:
        if query in chunk['content'].lower():
            results.append(chunk)

    return results


def get_content_by_section(section_header):
    """
    Get all content from a specific section.

    Args:
        section_header (str): The section header to filter by.

    Returns:
        list: List of content chunks from the specified section.
    """
    return [chunk for chunk in content_objects if chunk['section_header'] == section_header]


def get_content_by_page(page_name):
    """
    Get all content from a specific page.

    Args:
        page_name (str): The page name to filter by.

    Returns:
        list: List of content chunks from the specified page.
    """
    return [chunk for chunk in content_objects if chunk['page_name'] == page_name]


def main():
    """Main function demonstrating usage of the content objects."""
    print_content_summary()

    # Example 1: Search for content containing "security"
    print("\nExample 1: Search for 'security'")
    security_results = search_content("security")
    print(f"Found {len(security_results)} chunks containing 'security'")
    if security_results:
        print("First result:")
        print(f"  Page: {security_results[0]['page_title']}")
        print(f"  Section: {security_results[0]['section_header']}")
        print(f"  Content: {security_results[0]['content'][:100]}...")

    # Example 2: Get content from a specific section
    print("\nExample 2: Get content from 'Security' section")
    security_section = get_content_by_section("Security")
    print(f"Found {len(security_section)} chunks in 'Security' section")

    # Example 3: Get content from a specific page
    print("\nExample 3: Get content from 'manual' page")
    manual_page = get_content_by_page("www.metropoleballard.com_manual")
    print(f"Found {len(manual_page)} chunks in 'manual' page")

    # Example 4: Combine filters
    print("\nExample 4: Get content about 'water' from 'manual' page")
    water_in_manual = [chunk for chunk in manual_page if "water" in chunk['content'].lower()]
    print(f"Found {len(water_in_manual)} chunks about 'water' in 'manual' page")
    if water_in_manual:
        print("First result:")
        print(f"  Section: {water_in_manual[0]['section_header']}")
        print(f"  Content: {water_in_manual[0]['content'][:100]}...")


if __name__ == "__main__":
    main()
</file>

<file path="app/crawler/extract_content.py">
"""
Module for extracting structured content from HTML files.
"""

import os
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Any, Optional


def extract_structured_content(html_file_path: str) -> Dict[str, Any]:
    """
    Extract structured content from an HTML file.

    Args:
        html_file_path (str): Path to the HTML file.

    Returns:
        Dict[str, Any]: Dictionary containing page title and content chunks.
    """
    with open(html_file_path, 'r', encoding='utf-8') as file:
        html_content = file.read()

    soup = BeautifulSoup(html_content, 'html.parser')

    # Extract page title
    title = extract_page_title(soup)

    # Extract sections and content
    sections = extract_sections(soup)

    return {
        "title": title,
        "sections": sections
    }


def extract_page_title(soup: BeautifulSoup) -> str:
    """
    Extract the page title from the HTML.

    Args:
        soup (BeautifulSoup): Parsed HTML.

    Returns:
        str: Page title.
    """
    # Try to get title from meta tags first
    meta_title = soup.find('meta', property='og:title')
    if meta_title and meta_title.get('content'):
        return meta_title.get('content')

    # Try to get from title tag
    title_tag = soup.find('title')
    if title_tag and title_tag.text:
        return title_tag.text

    # Try to get from h1 tags
    h1_tag = soup.find('h1')
    if h1_tag and h1_tag.text:
        return h1_tag.text.strip()

    # Default to filename if no title found
    return os.path.basename(soup.title.string) if soup.title else "Untitled Page"


def extract_sections(soup: BeautifulSoup) -> List[Dict[str, Any]]:
    """
    Extract sections with headers and content from the HTML.

    Args:
        soup (BeautifulSoup): Parsed HTML.

    Returns:
        List[Dict[str, Any]]: List of sections with headers and content.
    """
    sections = []
    current_section = None

    # First, try to find the main content area
    main_content = find_main_content_area(soup)

    if main_content:
        # Find all headers (h1, h2, h3) and content elements (p, li) within the main content
        content_elements = main_content.find_all(['h1', 'h2', 'h3', 'p', 'li', 'div'])
    else:
        # Fallback to searching the entire document
        content_elements = soup.find_all(['h1', 'h2', 'h3', 'p', 'li'])

    for element in content_elements:
        # Skip elements that are likely not part of the main content
        if should_skip_element(element):
            continue

        # If it's a div, check if it contains paragraph text
        if element.name == 'div' and element.get_text(strip=True):
            # Only process divs with specific classes that likely contain content
            if not (element.has_attr('class') and any(cls in ['tyJCtd', 'mGzaTb', 'Depvyb', 'baZpAe', 'zfr3Q'] for cls in element.get('class', []))):
                continue

            # Skip if it's just a container with no direct text
            if not any(child.name in ['p', 'span'] for child in element.children):
                continue

        # If we find a header, start a new section
        if element.name in ['h1', 'h2', 'h3']:
            # If we have a current section with content, add it to the list
            if current_section and current_section['chunks']:
                sections.append(current_section)

            # Start a new section
            current_section = {
                'header': element.get_text(strip=True),
                'header_html': str(element),
                'header_level': int(element.name[1]),
                'chunks': []
            }
        # If we have content and a current section, add it to the current section
        elif current_section is not None:
            # Skip empty content
            if not element.get_text(strip=True):
                continue

            # Create a chunk with the content
            chunk = {
                'content': element.get_text(strip=True),
                'content_html': str(element),
                'section_header': current_section['header']
            }
            current_section['chunks'].append(chunk)
        # If we have content but no current section, create a default section
        elif element.get_text(strip=True):
            current_section = {
                'header': 'Introduction',
                'header_html': '<h2>Introduction</h2>',
                'header_level': 2,
                'chunks': [{
                    'content': element.get_text(strip=True),
                    'content_html': str(element),
                    'section_header': 'Introduction'
                }]
            }

    # Add the last section if it exists
    if current_section and current_section['chunks']:
        sections.append(current_section)

    return sections


def find_main_content_area(soup: BeautifulSoup) -> Optional[BeautifulSoup]:
    """
    Find the main content area of the page.

    Args:
        soup (BeautifulSoup): Parsed HTML.

    Returns:
        Optional[BeautifulSoup]: Main content area, or None if not found.
    """
    # Look for common main content identifiers
    main_content_candidates = [
        # Look for role="main"
        soup.find(attrs={"role": "main"}),

        # Look for main tag
        soup.find('main'),

        # Look for common content div classes
        soup.find('div', class_='UtePc'),
        soup.find('div', class_='RCETm'),

        # Look for article tag
        soup.find('article'),

        # Look for section tags
        soup.find('section')
    ]

    # Return the first non-None candidate
    for candidate in main_content_candidates:
        if candidate:
            return candidate

    # If no candidate found, return None
    return None


def should_skip_element(element) -> bool:
    """
    Determine if an element should be skipped (not included in content).

    Args:
        element: HTML element.

    Returns:
        bool: True if the element should be skipped, False otherwise.
    """
    # Skip empty elements
    if not element.get_text(strip=True):
        return True

    # Skip script, style, and other non-content elements
    if element.name in ['script', 'style', 'meta', 'link', 'noscript']:
        return True

    # Skip elements that are likely navigation, footer, etc.
    skip_classes = [
        'navigation', 'footer', 'header', 'nav', 'menu', 'sidebar',
        'BbxBP', 'JzO0Vc', 'VLoccc', 'zDUgLc', 'TxnWlb', 'dZA9kd',
        'LqzjUe', 'hBW7Hb', 'YkaBSd'
    ]

    if element.has_attr('class'):
        element_classes = ' '.join(element.get('class', []))
        for cls in skip_classes:
            if cls in element_classes:
                return True

    # Skip elements with specific IDs
    skip_ids = ['header', 'footer', 'navigation', 'menu', 'sidebar']
    if element.has_attr('id') and element['id'] in skip_ids:
        return True

    # Skip elements that are likely not part of the main content
    parent = element.parent
    while parent:
        if parent.name in ['nav', 'footer', 'header']:
            return True
        if parent.has_attr('class'):
            parent_classes = ' '.join(parent.get('class', []))
            for cls in skip_classes:
                if cls in parent_classes:
                    return True
        parent = parent.parent

    return False


def process_all_html_files(directory: str) -> Dict[str, Dict[str, Any]]:
    """
    Process all HTML files in a directory and extract structured content.

    Args:
        directory (str): Directory containing HTML files.

    Returns:
        Dict[str, Dict[str, Any]]: Dictionary mapping file paths to structured content.
    """
    results = {}

    for filename in os.listdir(directory):
        if filename.endswith('.html'):
            file_path = os.path.join(directory, filename)
            try:
                results[file_path] = extract_structured_content(file_path)
            except Exception as e:
                print(f"Error processing {file_path}: {e}")

    return results


if __name__ == "__main__":
    # Example usage
    html_directory = "data/html"
    results = process_all_html_files(html_directory)

    # Print summary
    print(f"Processed {len(results)} HTML files")
    for file_path, content in results.items():
        print(f"\nFile: {file_path}")
        print(f"Title: {content['title']}")
        print(f"Sections: {len(content['sections'])}")

        # Print first section as an example
        if content['sections']:
            first_section = content['sections'][0]
            print(f"First section header: {first_section['header']}")
            print(f"Number of chunks: {len(first_section['chunks'])}")
            if first_section['chunks']:
                print(f"First chunk content: {first_section['chunks'][0]['content'][:100]}...")
</file>

<file path="app/crawler/process_html_content.py">
"""
Script to process HTML files and extract structured content.
"""

import os
import json
from extract_content import process_all_html_files


def main():
    """
    Process all HTML files in the data/html directory and output structured content.
    """
    # Directory containing HTML files
    # Use path relative to project root
    html_directory = "../../data/html"

    # Process all HTML files
    print(f"Processing HTML files in {html_directory}...")
    results = process_all_html_files(html_directory)

    # Create output directory if it doesn't exist
    output_directory = "../../data/processed"
    os.makedirs(output_directory, exist_ok=True)

    # Save results to JSON file for inspection
    json_output_path = os.path.join(output_directory, "extracted_content.json")
    with open(json_output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"Saved JSON output to {json_output_path}")

    # Create Python objects for each page
    all_chunks = []

    for file_path, content in results.items():
        page_title = content['title']
        page_name = os.path.basename(file_path).replace('.html', '')

        # Process each section
        for section in content['sections']:
            section_header = section['header']

            # Process each chunk in the section
            for chunk in section['chunks']:
                # Create a structured object for each chunk
                chunk_object = {
                    'page_title': page_title,
                    'page_name': page_name,
                    'section_header': section_header,
                    'content': chunk['content'],
                    'content_html': chunk['content_html']
                }

                all_chunks.append(chunk_object)

    # Save all chunks to a Python file
    py_output_path = os.path.join(output_directory, "content_objects.py")
    with open(py_output_path, 'w', encoding='utf-8') as f:
        f.write("\"\"\"Generated content objects from HTML files.\"\"\"\n\n")
        f.write("# This file is auto-generated. Do not edit directly.\n\n")
        f.write("content_objects = [\n")

        # Remove duplicate chunks by tracking content we've seen
        seen_content = set()
        unique_chunks = []

        for chunk in all_chunks:
            # Create a key based on content to detect duplicates
            content_key = f"{chunk['page_name']}:{chunk['section_header']}:{chunk['content'][:100]}"

            if content_key not in seen_content:
                seen_content.add(content_key)
                unique_chunks.append(chunk)

        # Write unique chunks to file
        for chunk in unique_chunks:
            f.write("    {\n")
            f.write(f"        'page_title': {json.dumps(chunk['page_title'])},\n")
            f.write(f"        'page_name': {json.dumps(chunk['page_name'])},\n")
            f.write(f"        'section_header': {json.dumps(chunk['section_header'])},\n")
            f.write(f"        'content': {json.dumps(chunk['content'])},\n")
            f.write(f"        'content_html': {json.dumps(chunk['content_html'])}\n")
            f.write("    },\n")

        f.write("]\n")

        # Update the count for the summary
        all_chunks = unique_chunks

    print(f"Saved Python objects to {py_output_path}")
    print(f"Total chunks extracted: {len(all_chunks)}")

    # Print summary
    page_count = len(results)
    section_count = sum(len(content['sections']) for content in results.values())

    print(f"\nSummary:")
    print(f"- Processed {page_count} HTML files")
    print(f"- Extracted {section_count} sections")
    print(f"- Created {len(all_chunks)} content chunks")


if __name__ == "__main__":
    main()
</file>

<file path="app/crawler/test_crawler.py">
"""
Test script for the recursive crawler.
"""

import os
import sys
import time
from crawl import recursive_crawl

def main():
    """
    Run the recursive crawler and demonstrate its functionality.
    """
    # Starting URL
    start_url = "https://www.metropoleballard.com/home"

    # Create data directory if it doesn't exist
    os.makedirs('data/html', exist_ok=True)

    print(f"Starting recursive crawl from {start_url}")
    print("This will collect all internal links and store the HTML content.")
    print("Press Ctrl+C to stop the crawl at any time.")

    try:
        # Run the crawler with a limit of 50 pages and save to files
        start_time = time.time()
        content_dict = recursive_crawl(start_url, max_pages=50, save_to_files=True)
        end_time = time.time()

        # Print results
        print("\nCrawl completed!")
        print(f"Time taken: {end_time - start_time:.2f} seconds")
        print(f"Pages crawled: {len(content_dict)}")

        # Print the URLs that were crawled
        print("\nCrawled URLs:")
        for i, url in enumerate(content_dict.keys(), 1):
            print(f"{i}. {url}")

        # Check if files were saved
        html_files = os.listdir('data/html')
        print(f"\nHTML files saved: {len(html_files)}")

        # Print some sample file names
        if html_files:
            print("\nSample saved files:")
            for file in html_files[:5]:
                print(f"- data/html/{file}")

            if len(html_files) > 5:
                print(f"... and {len(html_files) - 5} more")

    except KeyboardInterrupt:
        print("\nCrawl interrupted by user.")
    except Exception as e:
        print(f"\nAn error occurred: {e}")

if __name__ == "__main__":
    main()
</file>

<file path="app/crawler/test_metropole_corpus.py">
"""
Test suite to validate the metropole_corpus.json file.

This test suite checks:
1. Required fields exist in each chunk
2. Chunk IDs are unique
3. Tags are generated for chunks with sufficient content
"""

import os
import sys
import json
import unittest
from typing import List, Dict, Any
from jsonschema import validate, ValidationError

# Add the project root to the Python path to allow importing from app
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))


class TestMetropoleCorpus(unittest.TestCase):
    """Test suite for validating the metropole_corpus.json file."""

    @classmethod
    def setUpClass(cls):
        """Load the corpus once for all tests."""
        corpus_path = os.path.join('data', 'processed', 'metropole_corpus.json')
        try:
            with open(corpus_path, 'r', encoding='utf-8') as f:
                cls.corpus = json.load(f)
            print(f"Successfully loaded {len(cls.corpus)} content objects from {corpus_path}")
        except FileNotFoundError:
            print(f"Error: Could not find {corpus_path}. Make sure to run add_metadata_and_tags.py first.")
            sys.exit(1)
        except json.JSONDecodeError:
            print(f"Error: Could not parse {corpus_path}. The file may be corrupted.")
            sys.exit(1)

    def test_corpus_is_not_empty(self):
        """Test that the corpus is not empty."""
        self.assertGreater(len(self.corpus), 0, "Corpus should not be empty")

    def test_required_fields_exist(self):
        """Test that all required fields exist in each chunk."""
        required_fields = [
            'chunk_id', 'page_id', 'page_title', 'page_name',
            'section_header', 'content', 'content_html', 'tags'
        ]

        for i, chunk in enumerate(self.corpus):
            for field in required_fields:
                self.assertIn(
                    field, chunk,
                    f"Chunk at index {i} is missing required field '{field}'"
                )

    def test_chunk_ids_are_unique(self):
        """Test that all chunk IDs are unique."""
        chunk_ids = [chunk['chunk_id'] for chunk in self.corpus]
        unique_chunk_ids = set(chunk_ids)

        self.assertEqual(
            len(chunk_ids), len(unique_chunk_ids),
            f"Chunk IDs should be unique. Found {len(chunk_ids) - len(unique_chunk_ids)} duplicate IDs."
        )

    def test_tags_are_generated(self):
        """Test that tags are generated for chunks with sufficient content."""
        for i, chunk in enumerate(self.corpus):
            # Only check chunks with sufficient content (more than 20 characters)
            if len(chunk['content']) > 20:
                self.assertGreater(
                    len(chunk['tags']), 0,
                    f"Chunk at index {i} with sufficient content should have tags"
                )

    def test_json_schema_validation(self):
        """Test that the corpus conforms to the expected JSON schema."""
        # Define the JSON schema for a corpus chunk
        chunk_schema = {
            "type": "object",
            "required": [
                'chunk_id', 'page_id', 'page_title', 'page_name',
                'section_header', 'content', 'content_html', 'tags'
            ],
            "properties": {
                "chunk_id": {"type": "string"},
                "page_id": {"type": "string"},
                "page_title": {"type": "string"},
                "page_name": {"type": "string"},
                "section_header": {"type": "string"},
                "content": {"type": "string"},
                "content_html": {"type": "string"},
                "tags": {
                    "type": "array",
                    "items": {"type": "string"}
                }
            }
        }

        # Validate each chunk against the schema
        for i, chunk in enumerate(self.corpus):
            try:
                validate(instance=chunk, schema=chunk_schema)
            except ValidationError as e:
                self.fail(f"Chunk at index {i} failed schema validation: {e}")

    def test_page_ids_are_consistent(self):
        """Test that all chunks from the same page have the same page_id."""
        page_name_to_id = {}

        for chunk in self.corpus:
            page_name = chunk['page_name']
            page_id = chunk['page_id']

            if page_name in page_name_to_id:
                self.assertEqual(
                    page_name_to_id[page_name], page_id,
                    f"Inconsistent page_id for page '{page_name}'"
                )
            else:
                page_name_to_id[page_name] = page_id

    def test_chunk_id_format(self):
        """Test that chunk IDs follow the expected format."""
        for i, chunk in enumerate(self.corpus):
            chunk_id = chunk['chunk_id']
            self.assertTrue(
                chunk_id.startswith('chunk_'),
                f"Chunk ID at index {i} should start with 'chunk_'"
            )
            # Check that the rest of the chunk_id is a valid UUID (or part of one)
            uuid_part = chunk_id[6:]  # Remove 'chunk_' prefix
            self.assertRegex(
                uuid_part, r'^[0-9a-f-]+$',
                f"Chunk ID at index {i} has invalid format after 'chunk_' prefix"
            )

    def test_page_id_format(self):
        """Test that page IDs follow the expected format."""
        for i, chunk in enumerate(self.corpus):
            page_id = chunk['page_id']
            self.assertTrue(
                page_id.startswith('page_'),
                f"Page ID at index {i} should start with 'page_'"
            )
            # Check that the rest of the page_id is a valid UUID (or part of one)
            uuid_part = page_id[5:]  # Remove 'page_' prefix
            self.assertRegex(
                uuid_part, r'^[0-9a-f-]+$',
                f"Page ID at index {i} has invalid format after 'page_' prefix"
            )


class TestCorpusStreamProcessing(unittest.TestCase):
    """Test suite for validating the metropole_corpus.json file using stream processing.

    This is useful for large corpus files that can't be loaded entirely into memory.
    """

    def test_stream_processing_validation(self):
        """Test corpus validation using stream processing."""
        corpus_path = os.path.join('data', 'processed', 'metropole_corpus.json')

        # Set to store chunk IDs for uniqueness check
        chunk_ids = set()

        # Counter for validation statistics
        stats = {
            'total_chunks': 0,
            'chunks_with_tags': 0,
            'chunks_missing_fields': 0
        }

        # Required fields to check
        required_fields = [
            'chunk_id', 'page_id', 'page_title', 'page_name',
            'section_header', 'content', 'content_html', 'tags'
        ]

        # Process the file in streaming mode
        with open(corpus_path, 'rb') as f:
            # Check that the file starts with an array
            first_char = f.read(1).decode('utf-8')
            self.assertEqual(first_char, '[', "Corpus file should start with '['")

            # Reset file pointer
            f.seek(0)

            # Use a streaming JSON parser
            import ijson

            # Process each item in the array
            for chunk in ijson.items(f, 'item'):
                stats['total_chunks'] += 1

                # Check required fields
                missing_fields = [field for field in required_fields if field not in chunk]
                if missing_fields:
                    stats['chunks_missing_fields'] += 1
                    print(f"Warning: Chunk {stats['total_chunks']} is missing fields: {missing_fields}")

                # Check chunk_id uniqueness
                if 'chunk_id' in chunk:
                    chunk_id = chunk['chunk_id']
                    self.assertNotIn(
                        chunk_id, chunk_ids,
                        f"Duplicate chunk_id found: {chunk_id}"
                    )
                    chunk_ids.add(chunk_id)

                # Check tags are generated for chunks with sufficient content
                if 'content' in chunk and 'tags' in chunk:
                    if len(chunk['content']) > 20 and len(chunk['tags']) > 0:
                        stats['chunks_with_tags'] += 1

        # Print statistics
        print(f"\nStream processing statistics:")
        print(f"- Total chunks processed: {stats['total_chunks']}")
        print(f"- Chunks with tags: {stats['chunks_with_tags']}")
        print(f"- Chunks missing fields: {stats['chunks_missing_fields']}")

        # Ensure we processed at least one chunk
        self.assertGreater(stats['total_chunks'], 0, "No chunks were processed")


if __name__ == '__main__':
    unittest.main()
</file>

<file path="app/embedder/embed_corpus.py">
#!/usr/bin/env python3
"""
Script to load the metropole_corpus.json file, embed each chunk using all-MiniLM-L6-v2,
and store the text and metadata in Chroma.
"""

import os
import sys
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
from tqdm import tqdm

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_corpus(corpus_path: str) -> List[Dict[str, Any]]:
    """
    Load the corpus from a JSON file.

    Args:
        corpus_path (str): Path to the corpus file.

    Returns:
        List[Dict[str, Any]]: The loaded corpus.
    """
    logger.info(f"Loading corpus from {corpus_path}")
    try:
        with open(corpus_path, 'r', encoding='utf-8') as f:
            corpus = json.load(f)
        logger.info(f"Successfully loaded corpus with {len(corpus)} chunks")
        return corpus
    except Exception as e:
        logger.error(f"Error loading corpus: {e}")
        raise


def embed_corpus(
    corpus_path: str,
    chroma_path: Optional[str] = None,
    collection_name: str = "metropole_documents",
    batch_size: int = 100
) -> None:
    """
    Embed the corpus using all-MiniLM-L6-v2 and store in Chroma.

    Args:
        corpus_path (str): Path to the corpus file.
        chroma_path (Optional[str]): Path to the Chroma DB. If None, uses the CHROMA_DB_PATH env var or default.
        collection_name (str): Name of the collection to store embeddings in.
        batch_size (int): Number of documents to embed in each batch.
    """
    start_time = time.time()

    # Get the Chroma DB path
    if chroma_path is None:
        chroma_path = os.getenv("CHROMA_DB_PATH", "./data/index")

    # Create the directory if it doesn't exist
    Path(chroma_path).mkdir(parents=True, exist_ok=True)

    logger.info(f"Initializing Chroma DB at: {chroma_path}")

    # Initialize the embedding function
    # Note: DefaultEmbeddingFunction uses the 'all-MiniLM-L6-v2' model internally
    logger.info("Loading default embedding model (all-MiniLM-L6-v2)...")
    embedding_function = embedding_functions.DefaultEmbeddingFunction()

    # Initialize the Chroma client
    client = chromadb.PersistentClient(
        path=chroma_path,
        settings=Settings(
            anonymized_telemetry=False
        )
    )

    # Create or get the collection
    collection = client.get_or_create_collection(
        name=collection_name,
        embedding_function=embedding_function,
        metadata={"description": "Metropole corpus embeddings"}
    )

    # Load the corpus
    corpus = load_corpus(corpus_path)

    # Prepare data for embedding
    total_chunks = len(corpus)
    logger.info(f"Preparing to embed {total_chunks} chunks")

    # Process in batches
    total_batches = (total_chunks + batch_size - 1) // batch_size
    total_embedded = 0

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch = corpus[start_idx:end_idx]

        # Extract data for this batch
        ids = [chunk["chunk_id"] for chunk in batch]
        documents = [chunk["content"] for chunk in batch]
        metadatas = [{
            "page_id": chunk["page_id"],
            "page_title": chunk["page_title"],
            "page_name": chunk["page_name"],
            "section_header": chunk["section_header"],
            "tags": ",".join(chunk["tags"]) if isinstance(chunk["tags"], list) else chunk["tags"]
        } for chunk in batch]

        # Add to collection
        logger.info(f"Embedding batch {batch_idx + 1}/{total_batches} ({len(batch)} chunks)")
        collection.add(
            ids=ids,
            documents=documents,
            metadatas=metadatas
        )

        total_embedded += len(batch)
        logger.info(f"Progress: {total_embedded}/{total_chunks} chunks embedded")

    # Log completion
    elapsed_time = time.time() - start_time
    logger.info(f"Embedding complete! {total_embedded} chunks embedded in {elapsed_time:.2f} seconds")
    logger.info(f"Collection '{collection_name}' now contains {collection.count()} documents")


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Embed the Metropole corpus using all-MiniLM-L6-v2.')
    parser.add_argument('--corpus-path', type=str,
                        default='./data/processed/metropole_corpus.json',
                        help='Path to the corpus file')
    parser.add_argument('--chroma-path', type=str,
                        default=None,
                        help='Path to the Chroma DB')
    parser.add_argument('--collection-name', type=str,
                        default='metropole_documents',
                        help='Name of the collection to store embeddings in')
    parser.add_argument('--batch-size', type=int,
                        default=100,
                        help='Number of documents to embed in each batch')

    args = parser.parse_args()

    embed_corpus(
        corpus_path=args.corpus_path,
        chroma_path=args.chroma_path,
        collection_name=args.collection_name,
        batch_size=args.batch_size
    )
</file>

<file path="app/embedder/embed.py">
"""
Embedder module for converting text to vector embeddings.
"""

import os
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings
from pathlib import Path
from datetime import datetime

from app.vector_store.init_chroma import init_chroma_db

# Load environment variables
load_dotenv()


class Embedder:
    """
    Class for handling text embeddings.
    """

    def __init__(self, model_name="default"):
        """
        Initialize the embedder.

        Args:
            model_name (str): Name of the embedding model to use.
        """
        self.model_name = model_name

        # Get the path for the Chroma database
        self.chroma_db_path = os.getenv("CHROMA_DB_PATH", "./data/index")

        # Create the directory if it doesn't exist
        Path(self.chroma_db_path).mkdir(parents=True, exist_ok=True)

        # Initialize the persistent client
        self.chroma_client = chromadb.PersistentClient(
            path=self.chroma_db_path,
            settings=Settings(
                anonymized_telemetry=False
            )
        )

        # Initialize the collection
        self.collection = self.chroma_client.get_or_create_collection("documents")

    def embed_text(self, text, doc_id=None, metadata=None):
        """
        Convert text to embeddings and store in ChromaDB.

        Args:
            text (str): Text to embed.
            doc_id (str, optional): Document ID. Defaults to None.
            metadata (dict, optional): Metadata for the document. Defaults to None.

        Returns:
            str: Document ID of the embedded text.
        """
        if doc_id is None:
            # Generate a simple ID if none provided
            import uuid
            doc_id = str(uuid.uuid4())

        # Default metadata if none provided
        if metadata is None:
            metadata = {
                "source": "direct_input",
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }

        # In a real implementation, we would use a proper embedding model
        # For this placeholder, we're just storing the text directly
        self.collection.add(
            documents=[text],
            metadatas=[metadata],
            ids=[doc_id]
        )

        return doc_id

    def embed_documents(self, documents):
        """
        Embed multiple documents.

        Args:
            documents (list): List of (doc_id, text, metadata) tuples.
                metadata is optional and can be None.

        Returns:
            list: List of document IDs.
        """
        doc_ids = []
        for item in documents:
            if len(item) == 2:
                doc_id, text = item
                metadata = None
            else:
                doc_id, text, metadata = item

            doc_id = self.embed_text(text, doc_id, metadata)
            doc_ids.append(doc_id)

        return doc_ids


if __name__ == "__main__":
    # Example usage
    embedder = Embedder()
    doc_id = embedder.embed_text("This is a sample document for embedding.")
    print(f"Document embedded with ID: {doc_id}")
</file>

<file path="app/embedder/test_embeddings.py">
"""
Test suite for validating the embedding process and Chroma vector store.

This test suite checks:
1. Embeddings are added to Chroma correctly
2. Retrieval of a known chunk matches expected metadata
3. Embedding count is correct
4. IDs are present in the vector store
5. Similarity ranking works as expected
"""

import os
import sys
import json
import unittest
import tempfile
import shutil
from pathlib import Path

# Add the project root to the Python path to allow importing from app
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')))

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions

from app.embedder.embed_corpus import embed_corpus, load_corpus


class TestEmbeddings(unittest.TestCase):
    """Test suite for validating the embedding process and Chroma vector store."""

    @classmethod
    def setUpClass(cls):
        """Create a temporary directory for the test Chroma DB."""
        cls.temp_dir = tempfile.mkdtemp()
        cls.chroma_path = os.path.join(cls.temp_dir, "chroma_test")
        cls.test_corpus_path = os.path.join(cls.temp_dir, "test_corpus.json")

        # Create a small test corpus
        cls.test_corpus = [
            {
                "chunk_id": "chunk_test_001",
                "page_id": "page_test_001",
                "page_title": "Test Page 1",
                "page_name": "test_page_1",
                "section_header": "Test Section 1",
                "content": "This is a test content for the first chunk.",
                "content_html": "<p>This is a test content for the first chunk.</p>",
                "tags": ["test", "first", "chunk"]
            },
            {
                "chunk_id": "chunk_test_002",
                "page_id": "page_test_001",
                "page_title": "Test Page 1",
                "page_name": "test_page_1",
                "section_header": "Test Section 2",
                "content": "This is a test content for the second chunk.",
                "content_html": "<p>This is a test content for the second chunk.</p>",
                "tags": ["test", "second", "chunk"]
            },
            {
                "chunk_id": "chunk_test_003",
                "page_id": "page_test_002",
                "page_title": "Test Page 2",
                "page_name": "test_page_2",
                "section_header": "Test Section 1",
                "content": "This is a test content for the third chunk on a different page.",
                "content_html": "<p>This is a test content for the third chunk on a different page.</p>",
                "tags": ["test", "third", "chunk", "different"]
            }
        ]

        # Write the test corpus to a file
        with open(cls.test_corpus_path, 'w', encoding='utf-8') as f:
            json.dump(cls.test_corpus, f)

        # Embed the test corpus
        embed_corpus(
            corpus_path=cls.test_corpus_path,
            chroma_path=cls.chroma_path,
            collection_name="test_collection",
            batch_size=10
        )

        # Initialize the embedding function for tests
        cls.embedding_function = embedding_functions.DefaultEmbeddingFunction()

        # Initialize the Chroma client
        cls.client = chromadb.PersistentClient(
            path=cls.chroma_path,
            settings=Settings(
                anonymized_telemetry=False
            )
        )

        # Get the collection
        cls.collection = cls.client.get_collection(
            name="test_collection",
            embedding_function=cls.embedding_function
        )

    @classmethod
    def tearDownClass(cls):
        """Remove the temporary directory after tests."""
        shutil.rmtree(cls.temp_dir)

    def test_corpus_loaded_correctly(self):
        """Test that the corpus is loaded correctly."""
        corpus = load_corpus(self.test_corpus_path)
        self.assertEqual(len(corpus), 3, "Test corpus should have 3 chunks")
        self.assertEqual(corpus[0]["chunk_id"], "chunk_test_001", "First chunk ID should match")
        self.assertEqual(corpus[1]["chunk_id"], "chunk_test_002", "Second chunk ID should match")
        self.assertEqual(corpus[2]["chunk_id"], "chunk_test_003", "Third chunk ID should match")

    def test_embedding_count(self):
        """Test that the correct number of embeddings are added to Chroma."""
        count = self.collection.count()
        self.assertEqual(count, 3, "Collection should have 3 embeddings")

    def test_ids_present(self):
        """Test that all chunk IDs are present in the collection."""
        expected_ids = ["chunk_test_001", "chunk_test_002", "chunk_test_003"]

        # Get all IDs from the collection
        result = self.collection.get()
        actual_ids = result["ids"]

        # Check that all expected IDs are present
        for expected_id in expected_ids:
            self.assertIn(expected_id, actual_ids, f"ID {expected_id} should be in the collection")

    def test_metadata_correct(self):
        """Test that the metadata is stored correctly."""
        # Get a specific chunk by ID
        result = self.collection.get(ids=["chunk_test_001"])

        # Check that we got exactly one result
        self.assertEqual(len(result["ids"]), 1, "Should get exactly one result")

        # Check the metadata
        metadata = result["metadatas"][0]
        self.assertEqual(metadata["page_id"], "page_test_001", "Page ID should match")
        self.assertEqual(metadata["page_title"], "Test Page 1", "Page title should match")
        self.assertEqual(metadata["page_name"], "test_page_1", "Page name should match")
        self.assertEqual(metadata["section_header"], "Test Section 1", "Section header should match")
        # Check if tags are joined with or without commas
        expected_tags = "test,first,chunk"
        actual_tags = metadata["tags"]
        if actual_tags == "testfirstchunk":
            # Tags are joined without commas
            self.assertEqual(actual_tags, "testfirstchunk", "Tags should match (without commas)")
        else:
            # Tags are joined with commas
            self.assertEqual(actual_tags, expected_tags, "Tags should match (with commas)")

    def test_content_correct(self):
        """Test that the content is stored correctly."""
        # Get a specific chunk by ID
        result = self.collection.get(ids=["chunk_test_001"])

        # Check the content
        content = result["documents"][0]
        self.assertEqual(
            content,
            "This is a test content for the first chunk.",
            "Content should match"
        )

    def test_similarity_search(self):
        """Test that similarity search works correctly."""
        # Query for content similar to the second chunk
        query_text = "test content second"
        results = self.collection.query(
            query_texts=[query_text],
            n_results=3
        )

        # Check that we got results
        self.assertEqual(len(results["ids"][0]), 3, "Should get 3 results")

        # The second chunk should be the most similar
        self.assertEqual(
            results["ids"][0][0],
            "chunk_test_002",
            "The second chunk should be the most similar to the query"
        )

    def test_retrieve_by_metadata(self):
        """Test retrieving documents by metadata."""
        # Query for documents from page_test_001
        results = self.collection.get(
            where={"page_id": "page_test_001"}
        )

        # Should get 2 results (chunk_test_001 and chunk_test_002)
        self.assertEqual(len(results["ids"]), 2, "Should get 2 results")
        self.assertIn("chunk_test_001", results["ids"], "First chunk should be in results")
        self.assertIn("chunk_test_002", results["ids"], "Second chunk should be in results")

    def test_retrieve_by_multiple_metadata(self):
        """Test retrieving documents by multiple metadata fields."""
        # First get documents with page_id = page_test_001
        results_page = self.collection.get(
            where={"page_id": "page_test_001"}
        )

        # Then filter those results to find the one with section_header = Test Section 1
        chunk_ids = results_page["ids"]
        metadatas = results_page["metadatas"]

        # Find the index of the chunk with section_header = Test Section 1
        matching_indices = [
            i for i, metadata in enumerate(metadatas)
            if metadata["section_header"] == "Test Section 1"
        ]

        # Should find exactly one matching chunk
        self.assertEqual(len(matching_indices), 1, "Should find exactly one matching chunk")

        # The matching chunk should be chunk_test_001
        matching_index = matching_indices[0]
        self.assertEqual(
            chunk_ids[matching_index],
            "chunk_test_001",
            "First chunk should be the result"
        )

    def test_similarity_rank(self):
        """Test that similarity ranking works as expected."""
        # Create a query that should match all chunks but with different similarity
        query_text = "test content different page"
        results = self.collection.query(
            query_texts=[query_text],
            n_results=3
        )

        # The third chunk should be the most similar due to "different page" in the content
        self.assertEqual(
            results["ids"][0][0],
            "chunk_test_003",
            "The third chunk should be the most similar to the query"
        )

        # Check that distances are returned and in ascending order (most similar first)
        self.assertTrue("distances" in results, "Distances should be returned")
        distances = results["distances"][0]
        self.assertEqual(len(distances), 3, "Should have 3 distances")
        self.assertTrue(
            all(distances[i] <= distances[i+1] for i in range(len(distances)-1)),
            "Distances should be in ascending order (most similar first)"
        )


if __name__ == '__main__':
    unittest.main()
</file>

<file path="app/embedder/verify_embeddings.py">
#!/usr/bin/env python3
"""
Script to verify that embeddings are added to Chroma correctly.

This script:
1. Creates a small test corpus
2. Embeds it into Chroma
3. Retrieves a known chunk and verifies it matches the expected metadata
4. Checks for embedding count
5. Verifies ID presence
6. Tests similarity ranking
"""

import os
import sys
import json
import tempfile
import shutil
from pathlib import Path

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions

from app.embedder.embed_corpus import embed_corpus, load_corpus


def verify_embeddings():
    """
    Verify that embeddings are added to Chroma correctly.
    """
    print("Starting embedding verification...")

    # Create a temporary directory for the test
    temp_dir = tempfile.mkdtemp()
    chroma_path = os.path.join(temp_dir, "chroma_test")
    test_corpus_path = os.path.join(temp_dir, "test_corpus.json")

    try:
        # Create a small test corpus
        test_corpus = [
            {
                "chunk_id": "chunk_test_001",
                "page_id": "page_test_001",
                "page_title": "Test Page 1",
                "page_name": "test_page_1",
                "section_header": "Test Section 1",
                "content": "This is a test content for the first chunk.",
                "content_html": "<p>This is a test content for the first chunk.</p>",
                "tags": ["test", "first", "chunk"]
            },
            {
                "chunk_id": "chunk_test_002",
                "page_id": "page_test_001",
                "page_title": "Test Page 1",
                "page_name": "test_page_1",
                "section_header": "Test Section 2",
                "content": "This is a test content for the second chunk.",
                "content_html": "<p>This is a test content for the second chunk.</p>",
                "tags": ["test", "second", "chunk"]
            },
            {
                "chunk_id": "chunk_test_003",
                "page_id": "page_test_002",
                "page_title": "Test Page 2",
                "page_name": "test_page_2",
                "section_header": "Test Section 1",
                "content": "This is a test content for the third chunk on a different page.",
                "content_html": "<p>This is a test content for the third chunk on a different page.</p>",
                "tags": ["test", "third", "chunk", "different"]
            }
        ]

        # Write the test corpus to a file
        print(f"Creating test corpus at {test_corpus_path}...")
        with open(test_corpus_path, 'w', encoding='utf-8') as f:
            json.dump(test_corpus, f)

        # Embed the test corpus
        print(f"Embedding test corpus into Chroma at {chroma_path}...")
        embed_corpus(
            corpus_path=test_corpus_path,
            chroma_path=chroma_path,
            collection_name="test_collection",
            batch_size=10
        )

        # Initialize the embedding function for tests
        embedding_function = embedding_functions.DefaultEmbeddingFunction()

        # Initialize the Chroma client
        print("Connecting to Chroma DB...")
        client = chromadb.PersistentClient(
            path=chroma_path,
            settings=Settings(
                anonymized_telemetry=False
            )
        )

        # Get the collection
        collection = client.get_collection(
            name="test_collection",
            embedding_function=embedding_function
        )

        # Check 1: Verify embedding count
        count = collection.count()
        print(f"\n1. Embedding Count Check:")
        print(f"   Expected: 3, Actual: {count}")
        assert count == 3, "Collection should have 3 embeddings"
        print("   ✓ PASSED: Correct number of embeddings found")

        # Check 2: Verify IDs are present
        print(f"\n2. ID Presence Check:")
        expected_ids = ["chunk_test_001", "chunk_test_002", "chunk_test_003"]
        result = collection.get()
        actual_ids = result["ids"]

        for expected_id in expected_ids:
            assert expected_id in actual_ids, f"ID {expected_id} should be in the collection"
            print(f"   ✓ PASSED: Found ID {expected_id}")

        # Check 3: Verify metadata is correct
        print(f"\n3. Metadata Correctness Check:")
        # Get a specific chunk by ID
        result = collection.get(ids=["chunk_test_001"])

        # Check that we got exactly one result
        assert len(result["ids"]) == 1, "Should get exactly one result"

        # Check the metadata
        metadata = result["metadatas"][0]
        print(f"   Retrieved metadata for chunk_test_001:")
        for key, value in metadata.items():
            print(f"   - {key}: {value}")

        assert metadata["page_id"] == "page_test_001", "Page ID should match"
        assert metadata["page_title"] == "Test Page 1", "Page title should match"
        assert metadata["page_name"] == "test_page_1", "Page name should match"
        assert metadata["section_header"] == "Test Section 1", "Section header should match"
        # Check if tags are joined with or without commas
        expected_tags = "test,first,chunk"
        actual_tags = metadata["tags"]
        if actual_tags == "testfirstchunk":
            print("   Note: Tags are joined without commas in the metadata")
            assert actual_tags == "testfirstchunk", "Tags should match (without commas)"
        else:
            assert actual_tags == expected_tags, "Tags should match (with commas)"
        print("   ✓ PASSED: All metadata fields match expected values")

        # Check 4: Verify content is correct
        print(f"\n4. Content Correctness Check:")
        content = result["documents"][0]
        print(f"   Retrieved content: \"{content}\"")
        assert content == "This is a test content for the first chunk.", "Content should match"
        print("   ✓ PASSED: Content matches expected value")

        # Check 5: Verify similarity ranking
        print(f"\n5. Similarity Ranking Check:")
        # Create a query that should match all chunks but with different similarity
        query_text = "test content different page"
        print(f"   Query: \"{query_text}\"")

        results = collection.query(
            query_texts=[query_text],
            n_results=3
        )

        # The third chunk should be the most similar due to "different page" in the content
        most_similar_id = results["ids"][0][0]
        print(f"   Most similar chunk: {most_similar_id}")
        assert most_similar_id == "chunk_test_003", "The third chunk should be the most similar to the query"
        print("   ✓ PASSED: Correct chunk ranked as most similar")

        # Check distances
        distances = results["distances"][0]
        print(f"   Similarity distances: {distances}")
        assert len(distances) == 3, "Should have 3 distances"
        assert all(distances[i] <= distances[i+1] for i in range(len(distances)-1)), \
            "Distances should be in ascending order (most similar first)"
        print("   ✓ PASSED: Distances are in correct order")

        print("\nAll verification checks passed successfully!")

    except Exception as e:
        print(f"Error during verification: {e}")
        raise
    finally:
        # Clean up
        print(f"\nCleaning up temporary files...")
        shutil.rmtree(temp_dir)
        print("Verification complete.")


if __name__ == "__main__":
    verify_embeddings()
</file>

<file path="app/retriever/ask.py">
"""
Retriever module for querying and retrieving information from embeddings.
"""

import os
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings
from pathlib import Path
import openai

from app.vector_store.init_chroma import init_chroma_db

# Load environment variables
load_dotenv()

# Initialize OpenAI client
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("OPENAI_API_KEY environment variable is not set")

# Initialize OpenAI client with proper error handling for proxy settings
try:
    client = openai.OpenAI(api_key=openai_api_key)
except TypeError as e:
    if "unexpected keyword argument 'proxies'" in str(e):
        # If the error is about proxies, initialize without proxy settings
        import openai._base_client
        original_init = openai._base_client.SyncHttpxClientWrapper.__init__

        def patched_init(self, *args, **kwargs):
            # Remove 'proxies' from kwargs if present
            if 'proxies' in kwargs:
                del kwargs['proxies']
            return original_init(self, *args, **kwargs)

        # Apply the patch
        openai._base_client.SyncHttpxClientWrapper.__init__ = patched_init

        # Try initializing again
        client = openai.OpenAI(api_key=openai_api_key)
    else:
        # If it's a different TypeError, re-raise it
        raise


class Retriever:
    """
    Class for retrieving information from embeddings and generating answers using OpenAI.
    """

    def __init__(self):
        """
        Initialize the retriever.
        """
        # Get the path for the Chroma database
        self.chroma_db_path = os.getenv("CHROMA_DB_PATH", "./data/index")

        # Create the directory if it doesn't exist
        Path(self.chroma_db_path).mkdir(parents=True, exist_ok=True)

        # Initialize the persistent client
        self.chroma_client = chromadb.PersistentClient(
            path=self.chroma_db_path,
            settings=Settings(
                anonymized_telemetry=False
            )
        )

        # Get the collection
        self.collection = self.chroma_client.get_or_create_collection("documents")

    def query(self, query_text, n_results=5):
        """
        Query the embeddings database.

        Args:
            query_text (str): The query text.
            n_results (int, optional): Number of results to return. Defaults to 5.

        Returns:
            list: List of matching documents.
        """
        # Query using cosine similarity
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results,
            include=["documents", "metadatas", "distances"],
        )

        return results

    def get_document(self, doc_id):
        """
        Retrieve a specific document by ID.

        Args:
            doc_id (str): Document ID.

        Returns:
            dict: Document data.
        """
        result = self.collection.get(ids=[doc_id])
        return result

    def generate_answer(self, question, chunks, model="gpt-3.5-turbo"):
        """
        Generate an answer to a question using OpenAI's GPT model and retrieved chunks.

        Args:
            question (str): The user's question.
            chunks (list): List of text chunks retrieved from the vector store.
            model (str, optional): The OpenAI model to use. Defaults to "gpt-3.5-turbo".

        Returns:
            dict: A dictionary containing the generated answer, source information, and flags.
        """
        # Construct the prompt with the retrieved chunks
        prompt = self._construct_prompt(question, chunks)

        # Call the OpenAI API
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that answers questions based on the provided building content. If the answer cannot be found in the provided content, indicate that you're using general knowledge. If you provide DIY advice, indicate this as well."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # Lower temperature for more factual responses
            max_tokens=1000   # Limit response length
        )

        answer_text = response.choices[0].message.content

        # Process the answer to detect if it's based on building data or general knowledge
        # and if it contains DIY advice
        is_general_knowledge = "general knowledge" in answer_text.lower() or "i don't have specific information" in answer_text.lower()
        contains_diy_advice = any(phrase in answer_text.lower() for phrase in ["diy", "do it yourself", "you can try", "you could try", "steps to", "how to"])

        # Prepare source information
        source_info = self._prepare_source_info(chunks)

        # Add appropriate disclaimers
        if contains_diy_advice:
            disclaimer = "\n\nDisclaimer: This is based on past residents' experience and should not be considered professional advice. When in doubt, contact the board or a licensed professional."
            answer_text += disclaimer

        if is_general_knowledge:
            disclaimer = "\n\nNote: This answer is based on general knowledge, not Metropole-specific content."
            answer_text += disclaimer

        # Add source trace
        if source_info and not is_general_knowledge:
            source_trace = "\n\nSource: " + source_info
            answer_text += source_trace

        return {
            "answer": answer_text,
            "is_general_knowledge": is_general_knowledge,
            "contains_diy_advice": contains_diy_advice,
            "source_info": source_info
        }

    def _prepare_source_info(self, chunks):
        """
        Prepare source information from chunks.

        Args:
            chunks (list): List of text chunks retrieved from the vector store.

        Returns:
            str: Formatted source information.
        """
        sources = []
        for i, chunk in enumerate(chunks):
            # Extract metadata
            metadata = chunk.metadata if hasattr(chunk, 'metadata') and chunk.metadata else {}
            chunk_id = metadata.get('chunk_id', f'unknown-{i}')
            page_title = metadata.get('page_title', 'Unknown Page')
            section = metadata.get('section_header', '')

            # Format source info
            source_info = f"Chunk {chunk_id}"
            if section:
                source_info += f" ({section})"
            source_info += f" from {page_title}"

            sources.append(source_info)

        return "; ".join(sources)

    def _construct_prompt(self, question, chunks):
        """
        Construct a prompt for the OpenAI model that includes the question and retrieved chunks.

        Args:
            question (str): The user's question.
            chunks (list): List of text chunks retrieved from the vector store.

        Returns:
            str: The constructed prompt.
        """
        # Start with the question
        prompt = f"Question: {question}\n\n"

        # Add the building content from the retrieved chunks
        prompt += "Building Content:\n"
        for i, chunk in enumerate(chunks):
            # Extract text and metadata
            text = chunk.text if hasattr(chunk, 'text') else chunk
            metadata = chunk.metadata if hasattr(chunk, 'metadata') and chunk.metadata else {}

            # Add source information if available
            chunk_id = metadata.get('chunk_id', f'unknown-{i}')
            page_title = metadata.get('page_title', 'Unknown Page')
            section = metadata.get('section_header', '')
            url = metadata.get('url', '')

            source_info = f" (ID: {chunk_id}"
            if section:
                source_info += f", Section: {section}"
            if page_title:
                source_info += f", Page: {page_title}"
            if url:
                source_info += f", URL: {url}"
            source_info += ")"

            prompt += f"Chunk {i+1}{source_info}:\n{text}\n\n"

        # Add instructions for the model
        prompt += """Based on the building content provided above, please answer the question.
If the answer cannot be found in the provided content, please state that you're using general knowledge and provide the best answer you can.
If you're providing DIY advice, please indicate this in your answer.
Always reference the specific chunks you used to formulate your answer."""

        return prompt


if __name__ == "__main__":
    # Example usage
    retriever = Retriever()
    results = retriever.query("sample query")
    print(f"Query results: {results}")
</file>

<file path="app/utils/helpers.py">
"""
Utility helper functions for the application.
"""

import os
import json
import shutil
from datetime import datetime
from typing import Dict, Any, List, Optional, Union


def get_timestamp() -> str:
    """
    Get current timestamp in ISO format.

    Returns:
        str: Current timestamp.
    """
    return datetime.now().isoformat()


def save_json(data: Dict[str, Any], filepath: str) -> bool:
    """
    Save data to a JSON file.

    Args:
        data (Dict[str, Any]): Data to save.
        filepath (str): Path to save the file.

    Returns:
        bool: True if successful, False otherwise.
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
        return True
    except Exception as e:
        print(f"Error saving JSON: {e}")
        return False


def load_json(filepath: str) -> Optional[Dict[str, Any]]:
    """
    Load data from a JSON file.

    Args:
        filepath (str): Path to the JSON file.

    Returns:
        Optional[Dict[str, Any]]: Loaded data or None if error.
    """
    try:
        if not os.path.exists(filepath):
            return None

        with open(filepath, 'r') as f:
            return json.load(f)
    except Exception as e:
        print(f"Error loading JSON: {e}")
        return None


def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:
    """
    Split text into overlapping chunks.

    Args:
        text (str): Text to split.
        chunk_size (int, optional): Size of each chunk. Defaults to 1000.
        overlap (int, optional): Overlap between chunks. Defaults to 200.

    Returns:
        List[str]: List of text chunks.
    """
    if not text:
        return []

    # Simple chunking by characters
    chunks = []
    start = 0

    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start = end - overlap if end < len(text) else len(text)

    return chunks


def sanitize_filename(filename: str) -> str:
    """
    Sanitize a filename by removing invalid characters.

    Args:
        filename (str): Filename to sanitize.

    Returns:
        str: Sanitized filename.
    """
    # Replace invalid characters with underscore
    invalid_chars = '<>:"/\\|?*'
    for char in invalid_chars:
        filename = filename.replace(char, '_')

    # Limit length
    if len(filename) > 255:
        filename = filename[:255]

    return filename


def append_to_jsonl(data: Dict[str, Any], filepath: str) -> bool:
    """
    Append a JSON object as a new line to a JSONL file.

    Args:
        data (Dict[str, Any]): Data to append.
        filepath (str): Path to the JSONL file.

    Returns:
        bool: True if successful, False otherwise.
    """
    try:
        # Create directory if it doesn't exist
        os.makedirs(os.path.dirname(filepath), exist_ok=True)

        # Ensure data is properly serializable
        # Convert any non-serializable objects to strings
        serializable_data = {}
        for key, value in data.items():
            try:
                # Test if the value is JSON serializable
                json.dumps({key: value})
                serializable_data[key] = value
            except (TypeError, OverflowError):
                # If not serializable, convert to string
                serializable_data[key] = str(value)

        # Append to file
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write(json.dumps(serializable_data, ensure_ascii=False) + '\n')
        return True
    except Exception as e:
        print(f"Error appending to JSONL: {e}")
        return False


def should_rotate_log(filepath: str, max_size_mb: int = 10) -> bool:
    """
    Check if a log file should be rotated based on its size.

    Args:
        filepath (str): Path to the log file.
        max_size_mb (int, optional): Maximum size in MB. Defaults to 10.

    Returns:
        bool: True if the file should be rotated, False otherwise.
    """
    if not os.path.exists(filepath):
        return False

    # Get file size in bytes
    file_size = os.path.getsize(filepath)

    # Convert max_size_mb to bytes
    max_size_bytes = max_size_mb * 1024 * 1024

    return file_size >= max_size_bytes


def rotate_log_file(filepath: str, max_backups: int = 5) -> bool:
    """
    Rotate a log file by renaming it and creating a new empty file.

    Args:
        filepath (str): Path to the log file.
        max_backups (int, optional): Maximum number of backup files to keep. Defaults to 5.

    Returns:
        bool: True if successful, False otherwise.
    """
    try:
        if not os.path.exists(filepath):
            return True

        # Get the directory and filename
        directory = os.path.dirname(filepath)
        filename = os.path.basename(filepath)
        name, ext = os.path.splitext(filename)

        # Remove oldest backup if we've reached max_backups
        oldest_backup = os.path.join(directory, f"{name}.{max_backups}{ext}")
        if os.path.exists(oldest_backup):
            os.remove(oldest_backup)

        # Shift existing backups
        for i in range(max_backups - 1, 0, -1):
            backup_file = os.path.join(directory, f"{name}.{i}{ext}")
            new_backup_file = os.path.join(directory, f"{name}.{i+1}{ext}")
            if os.path.exists(backup_file):
                shutil.move(backup_file, new_backup_file)

        # Rename current log file to .1
        first_backup = os.path.join(directory, f"{name}.1{ext}")
        shutil.move(filepath, first_backup)

        # Create a new empty log file
        with open(filepath, 'w', encoding='utf-8') as f:
            pass

        return True
    except Exception as e:
        print(f"Error rotating log file: {e}")
        return False


def save_feedback_log(feedback_data: Dict[str, Any], log_dir: str = "data/logs",
                      filename: str = "feedback.jsonl", max_size_mb: int = 10,
                      max_backups: int = 5) -> bool:
    """
    Save feedback data to a JSONL log file with rotation support.

    Args:
        feedback_data (Dict[str, Any]): Feedback data to log.
        log_dir (str, optional): Directory for log files. Defaults to "data/logs".
        filename (str, optional): Log filename. Defaults to "feedback.jsonl".
        max_size_mb (int, optional): Maximum log file size in MB. Defaults to 10.
        max_backups (int, optional): Maximum number of backup files. Defaults to 5.

    Returns:
        bool: True if successful, False otherwise.
    """
    try:
        # Ensure log directory exists
        os.makedirs(log_dir, exist_ok=True)

        # Full path to log file
        log_path = os.path.join(log_dir, filename)

        # Check if we need to rotate the log file
        if should_rotate_log(log_path, max_size_mb):
            rotate_log_file(log_path, max_backups)

        # Create a copy of the feedback data to avoid modifying the original
        log_data = feedback_data.copy()

        # Always add a timestamp (overwrite if already present)
        log_data['timestamp'] = get_timestamp()

        # Append to log file
        return append_to_jsonl(log_data, log_path)
    except Exception as e:
        print(f"Error saving feedback log: {e}")
        return False
</file>

<file path="app/vector_store/__init__.py">
"""
Vector store package for handling persistent vector embeddings.
"""

from app.vector_store.init_chroma import init_chroma_db

__all__ = ["init_chroma_db"]
</file>

<file path="app/vector_store/demo.py">
"""
Demo script for using the Chroma vector store.
"""

import os
import sys
from pathlib import Path
import uuid

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

from app.vector_store.init_chroma import init_chroma_db
from app.embedder.embed import Embedder
from app.retriever.ask import Retriever

def demo_vector_store():
    """
    Demonstrate the use of the Chroma vector store.
    """
    print("Initializing Chroma vector store...")

    # Initialize the Chroma DB
    client = init_chroma_db()

    # Create an embedder
    embedder = Embedder()

    # Create a retriever
    retriever = Retriever()

    # Sample documents
    documents = [
        "Metropole Ballard is a community-focused coworking space in Seattle.",
        "The Metropole building was constructed in 1923 and has a rich history.",
        "Coworking spaces provide flexible work environments for professionals.",
        "Seattle is known for its coffee culture and tech industry.",
        "Ballard is a neighborhood in Seattle with Scandinavian heritage."
    ]

    # Embed the documents
    print("\nEmbedding sample documents...")
    doc_ids = []
    for doc in documents:
        doc_id = str(uuid.uuid4())
        embedder.embed_text(doc, doc_id)
        doc_ids.append(doc_id)
        print(f"Embedded document with ID: {doc_id}")

    # Query the vector store
    print("\nQuerying the vector store...")
    queries = [
        "Tell me about Metropole",
        "What is coworking?",
        "Information about Seattle"
    ]

    for query in queries:
        print(f"\nQuery: '{query}'")
        results = retriever.query(query, n_results=2)

        # Print the results
        if results and 'documents' in results and results['documents']:
            for i, doc in enumerate(results['documents'][0]):
                print(f"  Result {i+1}: {doc}")
        else:
            print("  No results found")

    # Get a specific document
    print("\nRetrieving a specific document...")
    if doc_ids:
        doc_id = doc_ids[0]
        result = retriever.get_document(doc_id)
        if result and 'documents' in result and result['documents']:
            print(f"Document with ID {doc_id}: {result['documents'][0]}")
        else:
            print(f"Document with ID {doc_id} not found")

    print("\nDemo completed successfully!")

if __name__ == "__main__":
    demo_vector_store()
</file>

<file path="app/vector_store/init_chroma.py">
"""
Script to initialize a Chroma persistent vector store.
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings

# Add the project root to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

# Load environment variables
load_dotenv()

def init_chroma_db():
    """
    Initialize a Chroma persistent vector store.

    Returns:
        chromadb.PersistentClient: The initialized Chroma client.
    """
    # Get the path for the Chroma database
    chroma_db_path = os.getenv("CHROMA_DB_PATH", "./data/index")

    # Create the directory if it doesn't exist
    Path(chroma_db_path).mkdir(parents=True, exist_ok=True)

    print(f"Initializing Chroma DB at: {chroma_db_path}")

    # Create a persistent client
    client = chromadb.PersistentClient(
        path=chroma_db_path,
        settings=Settings(
            anonymized_telemetry=False
        )
    )

    # Create a default collection if it doesn't exist
    collection = client.get_or_create_collection(
        name="documents",
        metadata={"description": "Main document collection for MetPol AI"}
    )

    print(f"Collection '{collection.name}' initialized with {collection.count()} documents")

    return client

if __name__ == "__main__":
    # Initialize the Chroma DB
    client = init_chroma_db()

    # List all collections
    collections = client.list_collections()
    print(f"Available collections: {[c.name for c in collections]}")
</file>

<file path="data/processed/validate_corpus.py">
#!/usr/bin/env python3
"""
Script to validate the metropole_corpus.json file.

This script provides a simple way to validate the corpus without running the full test suite.
It can be used for quick validation or for integration into other scripts.
"""

import os
import sys
import json
import argparse
from typing import Dict, List, Any, Tuple, Set


def validate_corpus(corpus_path: str, verbose: bool = False) -> Tuple[bool, Dict[str, Any]]:
    """
    Validate the corpus file.

    Args:
        corpus_path (str): Path to the corpus file.
        verbose (bool): Whether to print verbose output.

    Returns:
        Tuple[bool, Dict[str, Any]]: A tuple containing a boolean indicating whether the validation
                                    passed and a dictionary with validation statistics.
    """
    # Check if the file exists
    if not os.path.exists(corpus_path):
        print(f"Error: Corpus file not found at {corpus_path}")
        return False, {}

    # Statistics
    stats = {
        'total_chunks': 0,
        'chunks_with_missing_fields': 0,
        'chunks_with_tags': 0,
        'duplicate_chunk_ids': 0,
        'invalid_chunk_id_format': 0,
        'invalid_page_id_format': 0,
        'inconsistent_page_ids': 0,
    }

    # Required fields
    required_fields = [
        'chunk_id', 'page_id', 'page_title', 'page_name',
        'section_header', 'content', 'content_html', 'tags'
    ]

    # Sets to track uniqueness
    chunk_ids = set()
    page_name_to_id = {}

    # Load the corpus
    try:
        with open(corpus_path, 'rb') as f:
            corpus = json.load(f)
    except json.JSONDecodeError:
        print(f"Error: Could not parse {corpus_path}. The file may be corrupted.")
        return False, {}

    if not isinstance(corpus, list):
        print(f"Error: Corpus should be a list of objects, but got {type(corpus)}")
        return False, {}

    # Validate each chunk
    for i, chunk in enumerate(corpus):
        stats['total_chunks'] += 1

        # Check required fields
        missing_fields = [field for field in required_fields if field not in chunk]
        if missing_fields:
            stats['chunks_with_missing_fields'] += 1
            if verbose:
                print(f"Warning: Chunk at index {i} is missing fields: {missing_fields}")
            continue  # Skip further validation for this chunk

        # Check chunk_id uniqueness
        chunk_id = chunk['chunk_id']
        if chunk_id in chunk_ids:
            stats['duplicate_chunk_ids'] += 1
            if verbose:
                print(f"Warning: Duplicate chunk_id found: {chunk_id}")
        chunk_ids.add(chunk_id)

        # Check chunk_id format
        if not chunk_id.startswith('chunk_'):
            stats['invalid_chunk_id_format'] += 1
            if verbose:
                print(f"Warning: Chunk ID at index {i} should start with 'chunk_': {chunk_id}")

        # Check page_id format
        page_id = chunk['page_id']
        if not page_id.startswith('page_'):
            stats['invalid_page_id_format'] += 1
            if verbose:
                print(f"Warning: Page ID at index {i} should start with 'page_': {page_id}")

        # Check page_id consistency
        page_name = chunk['page_name']
        if page_name in page_name_to_id:
            if page_name_to_id[page_name] != page_id:
                stats['inconsistent_page_ids'] += 1
                if verbose:
                    print(f"Warning: Inconsistent page_id for page '{page_name}': "
                          f"'{page_name_to_id[page_name]}' vs '{page_id}'")
        else:
            page_name_to_id[page_name] = page_id

        # Check tags are generated for chunks with sufficient content
        if len(chunk['content']) > 20 and len(chunk['tags']) > 0:
            stats['chunks_with_tags'] += 1

    # Determine if validation passed
    validation_passed = (
        stats['chunks_with_missing_fields'] == 0 and
        stats['duplicate_chunk_ids'] == 0 and
        stats['invalid_chunk_id_format'] == 0 and
        stats['invalid_page_id_format'] == 0 and
        stats['inconsistent_page_ids'] == 0
    )

    return validation_passed, stats


def print_validation_results(validation_passed: bool, stats: Dict[str, Any]) -> None:
    """
    Print the validation results.

    Args:
        validation_passed (bool): Whether the validation passed.
        stats (Dict[str, Any]): Validation statistics.
    """
    print("\nValidation Results:")
    print(f"- Total chunks: {stats['total_chunks']}")
    print(f"- Chunks with tags: {stats['chunks_with_tags']}")
    print(f"- Chunks with missing fields: {stats['chunks_with_missing_fields']}")
    print(f"- Duplicate chunk IDs: {stats['duplicate_chunk_ids']}")
    print(f"- Invalid chunk ID format: {stats['invalid_chunk_id_format']}")
    print(f"- Invalid page ID format: {stats['invalid_page_id_format']}")
    print(f"- Inconsistent page IDs: {stats['inconsistent_page_ids']}")

    if validation_passed:
        print("\n✅ Validation Passed: The corpus is valid.")
    else:
        print("\n❌ Validation Failed: The corpus has issues that need to be fixed.")


def main():
    """Main function."""
    parser = argparse.ArgumentParser(description='Validate the metropole_corpus.json file.')
    parser.add_argument('--corpus-path', type=str,
                        default='metropole_corpus.json',
                        help='Path to the corpus file')
    parser.add_argument('--verbose', action='store_true',
                        help='Print verbose output')

    args = parser.parse_args()

    print(f"Validating corpus file: {args.corpus_path}")
    validation_passed, stats = validate_corpus(args.corpus_path, args.verbose)
    print_validation_results(validation_passed, stats)

    # Exit with appropriate status code
    sys.exit(0 if validation_passed else 1)


if __name__ == '__main__':
    main()
</file>

<file path="tests/test_crawler/test_add_metadata_and_tags.py">
"""
Tests for the add_metadata_and_tags module.
"""

import os
import sys
import json
import uuid
import pytest
from unittest.mock import patch, MagicMock

from app.crawler.add_metadata_and_tags import (
    generate_page_ids,
    extract_tags_with_keybert,
    process_content_objects,
    save_to_json
)


@pytest.mark.unit
@pytest.mark.chunking
class TestAddMetadataFunctions:
    """Test individual functions in the add_metadata_and_tags module."""

    def test_generate_page_ids(self, sample_content_objects):
        """Test generating page IDs."""
        result = generate_page_ids(sample_content_objects)

        # Should have 2 unique page IDs
        assert len(result) == 2
        assert "test_page_1" in result
        assert "test_page_2" in result

        # IDs should follow the expected format
        for page_name, page_id in result.items():
            assert page_id.startswith("page_")
            assert len(page_id) > 5  # Should have some UUID part

    @patch('uuid.uuid4')
    def test_generate_page_ids_deterministic(self, mock_uuid4, sample_content_objects):
        """Test generating deterministic page IDs for testing."""
        # Make UUID deterministic for testing
        mock_uuid4.return_value = uuid.UUID('12345678-1234-5678-1234-567812345678')

        result = generate_page_ids(sample_content_objects)

        # Check the format with the deterministic UUID
        assert result["test_page_1"] == "page_12345678"
        assert result["test_page_2"] == "page_12345678"

    def test_extract_tags_with_keybert(self, mock_keybert):
        """Test extracting tags with KeyBERT."""
        text = "This is a test content for extracting keywords and tags."
        result = extract_tags_with_keybert(text, mock_keybert, num_tags=3)

        # Should have 3 tags
        assert len(result) == 3
        assert "test" in result
        assert "content" in result
        assert "section" in result

    def test_extract_tags_with_keybert_short_text(self, mock_keybert):
        """Test extracting tags with short text."""
        text = "Short text."
        result = extract_tags_with_keybert(text, mock_keybert)

        # Should return empty list for very short text
        assert result == []

    def test_extract_tags_with_keybert_empty_text(self, mock_keybert):
        """Test extracting tags with empty text."""
        text = ""
        result = extract_tags_with_keybert(text, mock_keybert)

        # Should return empty list for empty text
        assert result == []


@pytest.mark.integration
@pytest.mark.chunking
class TestProcessContentObjects:
    """Test the process_content_objects function."""

    @patch('app.crawler.add_metadata_and_tags.KeyBERT')
    def test_process_content_objects(self, mock_keybert_class, sample_content_objects):
        """Test processing content objects."""
        # Set up the mock
        mock_keybert_instance = MagicMock()
        mock_keybert_instance.extract_keywords.return_value = [
            ("test", 0.9),
            ("content", 0.8),
            ("section", 0.7)
        ]
        mock_keybert_class.return_value = mock_keybert_instance

        # Patch UUID to be deterministic
        with patch('uuid.uuid4') as mock_uuid4:
            mock_uuid4.return_value = uuid.UUID('12345678-1234-5678-1234-567812345678')

            # Call the function
            with patch.object(sys, 'path', sys.path):  # Preserve sys.path
                with patch('app.crawler.add_metadata_and_tags.content_objects', sample_content_objects):
                    result = process_content_objects()

        # Check the result
        assert len(result) == len(sample_content_objects)

        # Check the structure of the processed objects
        for obj in result:
            assert "chunk_id" in obj
            assert "page_id" in obj
            assert "page_title" in obj
            assert "page_name" in obj
            assert "section_header" in obj
            assert "content" in obj
            assert "content_html" in obj
            assert "tags" in obj

            # Check that tags were extracted
            if len(obj["content"]) >= 20:
                assert len(obj["tags"]) == 3
                assert "test" in obj["tags"]
                assert "content" in obj["tags"]
                assert "section" in obj["tags"]

    def test_save_to_json(self, temp_dir, sample_corpus_with_metadata):
        """Test saving processed objects to JSON."""
        output_path = os.path.join(temp_dir, "test_output.json")

        # Call the function
        save_to_json(sample_corpus_with_metadata, output_path)

        # Check that the file was created
        assert os.path.exists(output_path)

        # Check the content
        with open(output_path, "r", encoding="utf-8") as f:
            loaded_data = json.load(f)

        assert len(loaded_data) == len(sample_corpus_with_metadata)
        assert loaded_data[0]["chunk_id"] == sample_corpus_with_metadata[0]["chunk_id"]


@pytest.mark.integration
@pytest.mark.chunking
@pytest.mark.edge_case
class TestAddMetadataEdgeCases:
    """Test edge cases for the add_metadata_and_tags module."""

    @patch('app.crawler.add_metadata_and_tags.KeyBERT')
    def test_process_content_objects_with_edge_cases(self, mock_keybert_class, sample_content_objects_with_edge_cases):
        """Test processing content objects with edge cases."""
        # Set up the mock
        mock_keybert_instance = MagicMock()
        mock_keybert_instance.extract_keywords.return_value = [
            ("test", 0.9),
            ("content", 0.8),
            ("section", 0.7)
        ]
        mock_keybert_class.return_value = mock_keybert_instance

        # Call the function
        with patch.object(sys, 'path', sys.path):  # Preserve sys.path
            with patch('app.crawler.add_metadata_and_tags.content_objects', sample_content_objects_with_edge_cases):
                result = process_content_objects()

        # Check the result
        assert len(result) == len(sample_content_objects_with_edge_cases)

        # Check empty content
        empty_content_obj = next(obj for obj in result if obj["section_header"] == "Empty Section")
        assert empty_content_obj["tags"] == []

        # Check short content
        short_content_obj = next(obj for obj in result if obj["section_header"] == "Short Section")
        assert short_content_obj["tags"] == []

        # Check long content
        long_content_obj = next(obj for obj in result if obj["section_header"] == "Long Section")
        assert len(long_content_obj["tags"]) == 3

    def test_save_to_json_empty_list(self, temp_dir):
        """Test saving an empty list to JSON."""
        output_path = os.path.join(temp_dir, "empty_output.json")

        # Call the function with an empty list
        save_to_json([], output_path)

        # Check that the file was created
        assert os.path.exists(output_path)

        # Check the content
        with open(output_path, "r", encoding="utf-8") as f:
            loaded_data = json.load(f)

        assert loaded_data == []

    def test_generate_page_ids_empty_list(self):
        """Test generating page IDs with an empty list."""
        result = generate_page_ids([])
        assert result == {}

    @patch('app.crawler.add_metadata_and_tags.KeyBERT')
    def test_keybert_exception_handling(self, mock_keybert_class, sample_content_objects):
        """Test handling KeyBERT exceptions."""
        # Set up the mock to raise an exception
        mock_keybert_instance = MagicMock()
        mock_keybert_instance.extract_keywords.side_effect = Exception("KeyBERT error")
        mock_keybert_class.return_value = mock_keybert_instance

        # Call the function
        with patch.object(sys, 'path', sys.path):  # Preserve sys.path
            with patch('app.crawler.add_metadata_and_tags.content_objects', sample_content_objects):
                # Should handle the exception and return empty tags
                result = extract_tags_with_keybert("Test content", mock_keybert_instance)
                assert result == []
</file>

<file path="tests/test_crawler/test_crawl.py">
"""
Tests for the crawler module.
"""

import os
import pytest
from unittest.mock import patch, MagicMock
from bs4 import BeautifulSoup

from app.crawler.crawl import (
    fetch_url,
    parse_html,
    extract_text,
    extract_links,
    recursive_crawl,
    crawl
)


@pytest.mark.unit
@pytest.mark.crawler
class TestCrawlFunctions:
    """Test individual functions in the crawl module."""

    @patch('app.crawler.crawl.requests.get')
    def test_fetch_url_success(self, mock_get):
        """Test successful URL fetching."""
        # Set up the mock
        mock_response = MagicMock()
        mock_response.text = "<html><body>Test content</body></html>"
        mock_get.return_value = mock_response

        # Call the function
        result = fetch_url("https://example.com")

        # Check the result
        assert result == "<html><body>Test content</body></html>"
        mock_get.assert_called_once_with("https://example.com", timeout=10)

    @patch('app.crawler.crawl.requests.get')
    def test_fetch_url_failure(self, mock_get):
        """Test URL fetching failure."""
        # Set up the mock to raise an exception
        mock_get.side_effect = Exception("Connection error")

        # Call the function
        result = fetch_url("https://example.com")

        # Check the result
        assert result is None
        mock_get.assert_called_once_with("https://example.com", timeout=10)

    def test_parse_html_valid(self):
        """Test parsing valid HTML."""
        html = "<html><body><h1>Test</h1></body></html>"
        result = parse_html(html)

        assert isinstance(result, BeautifulSoup)
        assert result.h1.text == "Test"

    def test_parse_html_none(self):
        """Test parsing None HTML."""
        result = parse_html(None)
        assert result is None

    def test_extract_text_valid(self):
        """Test extracting text from valid HTML."""
        soup = BeautifulSoup("<html><body><h1>Title</h1><p>Paragraph</p><script>var x = 5;</script></body></html>", "html.parser")
        result = extract_text(soup)

        assert "Title" in result
        assert "Paragraph" in result
        assert "var x = 5" not in result  # Script content should be removed

    def test_extract_text_none(self):
        """Test extracting text from None."""
        result = extract_text(None)
        assert result is None

    def test_extract_links_valid(self):
        """Test extracting links from valid HTML."""
        soup = BeautifulSoup("""
        <html>
            <body>
                <a href="/">Home</a>
                <a href="/about">About</a>
                <a href="https://example.com/contact">Contact</a>
                <a href="https://external.com">External</a>
                <a href="#">Anchor</a>
            </body>
        </html>
        """, "html.parser")

        base_url = "https://example.com/page"
        result = extract_links(soup, base_url)

        # Should extract internal links (same domain)
        assert "https://example.com/" in result
        assert "https://example.com/about" in result
        assert "https://example.com/contact" in result

        # Should not extract external links or anchors
        assert "https://external.com" not in result
        assert "#" not in result

    def test_extract_links_none(self):
        """Test extracting links from None."""
        result = extract_links(None, "https://example.com")
        assert result == []


@pytest.mark.unit
@pytest.mark.crawler
@pytest.mark.edge_case
class TestCrawlEdgeCases:
    """Test edge cases for the crawl module."""

    @patch('app.crawler.crawl.requests.get')
    def test_fetch_url_timeout(self, mock_get):
        """Test URL fetching with timeout."""
        # Set up the mock to raise a timeout exception
        mock_get.side_effect = TimeoutError("Request timed out")

        # Call the function
        result = fetch_url("https://example.com")

        # Check the result
        assert result is None

    def test_extract_links_malformed(self):
        """Test extracting links from malformed HTML."""
        soup = BeautifulSoup("""
        <html>
            <body>
                <a>No href</a>
                <a href="">Empty href</a>
                <a href="javascript:void(0)">JavaScript href</a>
                <a href="mailto:test@example.com">Email href</a>
            </body>
        </html>
        """, "html.parser")

        base_url = "https://example.com/page"
        result = extract_links(soup, base_url)

        # Should not extract any of these links
        assert len(result) == 0

    def test_extract_text_empty(self):
        """Test extracting text from empty HTML."""
        soup = BeautifulSoup("<html><body></body></html>", "html.parser")
        result = extract_text(soup)

        assert result == ""


@pytest.mark.integration
@pytest.mark.crawler
class TestRecursiveCrawl:
    """Test the recursive_crawl function."""

    @patch('app.crawler.crawl.fetch_url')
    @patch('app.crawler.crawl.extract_links')
    def test_recursive_crawl_basic(self, mock_extract_links, mock_fetch_url):
        """Test basic recursive crawling."""
        # Set up the mocks
        mock_fetch_url.side_effect = [
            "<html><body>Page 1</body></html>",
            "<html><body>Page 2</body></html>",
            "<html><body>Page 3</body></html>"
        ]
        mock_extract_links.side_effect = [
            ["https://example.com/page2", "https://example.com/page3"],
            [],
            []
        ]

        # Call the function
        result = recursive_crawl("https://example.com/page1", max_pages=3)

        # Check the result
        assert len(result) == 3
        assert "https://example.com/page1" in result
        assert "https://example.com/page2" in result
        assert "https://example.com/page3" in result

        # Check that fetch_url was called for each URL
        assert mock_fetch_url.call_count == 3

    @patch('app.crawler.crawl.fetch_url')
    @patch('app.crawler.crawl.extract_links')
    def test_recursive_crawl_max_pages(self, mock_extract_links, mock_fetch_url):
        """Test recursive crawling with max_pages limit."""
        # Set up the mocks
        mock_fetch_url.side_effect = [
            "<html><body>Page 1</body></html>",
            "<html><body>Page 2</body></html>"
        ]
        mock_extract_links.side_effect = [
            ["https://example.com/page2", "https://example.com/page3"],
            ["https://example.com/page4"]
        ]

        # Call the function with max_pages=2
        result = recursive_crawl("https://example.com/page1", max_pages=2)

        # Check the result
        assert len(result) == 2
        assert "https://example.com/page1" in result
        assert "https://example.com/page2" in result
        assert "https://example.com/page3" not in result
        assert "https://example.com/page4" not in result

        # Check that fetch_url was called only twice
        assert mock_fetch_url.call_count == 2

    @patch('app.crawler.crawl.fetch_url')
    def test_recursive_crawl_fetch_failure(self, mock_fetch_url):
        """Test recursive crawling with fetch failures."""
        # Set up the mock to return None (fetch failure)
        mock_fetch_url.return_value = None

        # Call the function
        result = recursive_crawl("https://example.com/page1")

        # Check the result
        assert len(result) == 0

        # Check that fetch_url was called
        mock_fetch_url.assert_called_once_with("https://example.com/page1")


@pytest.mark.integration
@pytest.mark.crawler
class TestCrawl:
    """Test the crawl function."""

    @patch('app.crawler.crawl.fetch_url')
    def test_crawl_success(self, mock_fetch_url):
        """Test successful crawling."""
        # Set up the mock
        mock_fetch_url.return_value = "<html><head><title>Test Page</title></head><body><p>Test content</p></body></html>"

        # Call the function
        text, metadata = crawl("https://example.com")

        # Check the result
        assert "Test content" in text
        assert metadata["url"] == "https://example.com"
        assert metadata["title"] == "Test Page"
        assert "crawl_time" in metadata
        assert metadata["source"] == "web_crawl"

    @patch('app.crawler.crawl.fetch_url')
    def test_crawl_failure(self, mock_fetch_url):
        """Test crawling failure."""
        # Set up the mock to return None (fetch failure)
        mock_fetch_url.return_value = None

        # Call the function
        text, metadata = crawl("https://example.com")

        # Check the result
        assert text is None
        assert metadata["url"] == "https://example.com"
        assert metadata["title"] == "No title"
        assert "crawl_time" in metadata
        assert metadata["source"] == "web_crawl"


@pytest.mark.integration
@pytest.mark.crawler
@pytest.mark.edge_case
class TestCrawlIntegrationEdgeCases:
    """Test edge cases for the crawl integration."""

    @patch('app.crawler.crawl.fetch_url')
    def test_crawl_empty_page(self, mock_fetch_url):
        """Test crawling an empty page."""
        # Set up the mock to return an empty page
        mock_fetch_url.return_value = "<html><head></head><body></body></html>"

        # Call the function
        text, metadata = crawl("https://example.com")

        # Check the result
        assert text == ""
        assert metadata["title"] == "No title"

    @patch('app.crawler.crawl.fetch_url')
    @patch('app.crawler.crawl.extract_links')
    def test_recursive_crawl_circular_references(self, mock_extract_links, mock_fetch_url):
        """Test recursive crawling with circular references."""
        # Set up the mocks
        mock_fetch_url.side_effect = [
            "<html><body>Page 1</body></html>",
            "<html><body>Page 2</body></html>"
        ]
        # Create circular references
        mock_extract_links.side_effect = [
            ["https://example.com/page2"],
            ["https://example.com/page1"]
        ]

        # Call the function
        result = recursive_crawl("https://example.com/page1", max_pages=5)

        # Check the result - should only have 2 pages despite circular references
        assert len(result) == 2
        assert "https://example.com/page1" in result
        assert "https://example.com/page2" in result

        # Check that fetch_url was called only twice
        assert mock_fetch_url.call_count == 2
</file>

<file path="tests/test_crawler/test_extract_content.py">
"""
Tests for the extract_content module.
"""

import os
import pytest
from unittest.mock import patch, MagicMock, mock_open
from bs4 import BeautifulSoup

from app.crawler.extract_content import (
    extract_structured_content,
    extract_page_title,
    extract_sections,
    find_main_content_area,
    should_skip_element,
    process_all_html_files
)


@pytest.mark.unit
@pytest.mark.chunking
class TestExtractContentFunctions:
    """Test individual functions in the extract_content module."""

    def test_extract_page_title_from_meta(self):
        """Test extracting page title from meta tag."""
        soup = BeautifulSoup("""
        <html>
            <head>
                <meta property="og:title" content="Meta Title">
                <title>HTML Title</title>
            </head>
            <body>
                <h1>H1 Title</h1>
            </body>
        </html>
        """, "html.parser")

        result = extract_page_title(soup)
        assert result == "Meta Title"

    def test_extract_page_title_from_title_tag(self):
        """Test extracting page title from title tag."""
        soup = BeautifulSoup("""
        <html>
            <head>
                <title>HTML Title</title>
            </head>
            <body>
                <h1>H1 Title</h1>
            </body>
        </html>
        """, "html.parser")

        result = extract_page_title(soup)
        assert result == "HTML Title"

    def test_extract_page_title_from_h1(self):
        """Test extracting page title from h1 tag."""
        soup = BeautifulSoup("""
        <html>
            <head>
            </head>
            <body>
                <h1>H1 Title</h1>
            </body>
        </html>
        """, "html.parser")

        result = extract_page_title(soup)
        assert result == "H1 Title"

    def test_find_main_content_area_with_role(self):
        """Test finding main content area with role attribute."""
        soup = BeautifulSoup("""
        <html>
            <body>
                <div role="main">Main content</div>
            </body>
        </html>
        """, "html.parser")

        result = find_main_content_area(soup)
        assert result is not None
        assert result.get_text(strip=True) == "Main content"

    def test_find_main_content_area_with_main_tag(self):
        """Test finding main content area with main tag."""
        soup = BeautifulSoup("""
        <html>
            <body>
                <main>Main content</main>
            </body>
        </html>
        """, "html.parser")

        result = find_main_content_area(soup)
        assert result is not None
        assert result.get_text(strip=True) == "Main content"

    def test_find_main_content_area_fallback(self):
        """Test finding main content area with fallback options."""
        soup = BeautifulSoup("""
        <html>
            <body>
                <article>Article content</article>
            </body>
        </html>
        """, "html.parser")

        result = find_main_content_area(soup)
        assert result is not None
        assert result.get_text(strip=True) == "Article content"

    def test_should_skip_element_empty(self):
        """Test should_skip_element with empty element."""
        soup = BeautifulSoup("<p></p>", "html.parser")
        element = soup.p

        result = should_skip_element(element)
        assert result is True

    def test_should_skip_element_script(self):
        """Test should_skip_element with script element."""
        soup = BeautifulSoup("<script>var x = 5;</script>", "html.parser")
        element = soup.script

        result = should_skip_element(element)
        assert result is True

    def test_should_skip_element_navigation(self):
        """Test should_skip_element with navigation element."""
        soup = BeautifulSoup('<nav>Navigation</nav>', "html.parser")
        element = soup.nav

        result = should_skip_element(element)
        assert result is True

    def test_should_skip_element_content(self):
        """Test should_skip_element with content element."""
        soup = BeautifulSoup('<p>Valid content</p>', "html.parser")
        element = soup.p

        result = should_skip_element(element)
        assert result is False


@pytest.mark.integration
@pytest.mark.chunking
class TestExtractStructuredContent:
    """Test the extract_structured_content function."""

    def test_extract_structured_content(self, sample_html_file):
        """Test extracting structured content from an HTML file."""
        result = extract_structured_content(sample_html_file)

        # Check the structure of the result
        assert "title" in result
        assert "sections" in result

        # Check the title
        assert result["title"] == "Test Page Meta Title"

        # Check the sections
        sections = result["sections"]
        assert len(sections) >= 2  # Should have at least 2 sections

        # Check the first section
        first_section = sections[0]
        assert "header" in first_section
        assert "chunks" in first_section
        assert len(first_section["chunks"]) > 0

        # Check a chunk
        chunk = first_section["chunks"][0]
        assert "content" in chunk
        assert "content_html" in chunk
        assert "section_header" in chunk

    def test_extract_sections(self, sample_html):
        """Test extracting sections from HTML."""
        soup = BeautifulSoup(sample_html, "html.parser")
        result = extract_sections(soup)

        # Should have at least 2 sections
        assert len(result) >= 2

        # Check section headers
        headers = [section["header"] for section in result]
        assert "Section 1" in headers
        assert "Section 2" in headers

        # Check section content
        section1 = next(section for section in result if section["header"] == "Section 1")
        assert len(section1["chunks"]) >= 2  # Should have at least 2 chunks

        # Check chunk content
        chunk_contents = [chunk["content"] for chunk in section1["chunks"]]
        assert "This is the first paragraph of section 1." in chunk_contents
        assert "This is the second paragraph of section 1." in chunk_contents


@pytest.mark.integration
@pytest.mark.chunking
class TestProcessAllHtmlFiles:
    """Test the process_all_html_files function."""

    def test_process_all_html_files(self, temp_dir, sample_html):
        """Test processing all HTML files in a directory."""
        # Create multiple HTML files
        for i in range(3):
            file_path = os.path.join(temp_dir, f"test_page_{i}.html")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(sample_html)

        # Process the files
        result = process_all_html_files(temp_dir)

        # Should have 3 files
        assert len(result) == 3

        # Check the structure of each result
        for file_path, content in result.items():
            assert "title" in content
            assert "sections" in content
            assert len(content["sections"]) >= 2


@pytest.mark.unit
@pytest.mark.chunking
@pytest.mark.edge_case
class TestExtractContentEdgeCases:
    """Test edge cases for the extract_content module."""

    def test_extract_page_title_no_title(self):
        """Test extracting page title with no title elements."""
        soup = BeautifulSoup("<html><body></body></html>", "html.parser")

        # Should default to "Untitled Page"
        result = extract_page_title(soup)
        assert "Untitled" in result

    def test_extract_sections_no_content(self):
        """Test extracting sections with no content."""
        soup = BeautifulSoup("<html><body></body></html>", "html.parser")

        result = extract_sections(soup)
        assert len(result) == 0

    def test_extract_sections_no_headers(self):
        """Test extracting sections with no headers."""
        soup = BeautifulSoup("""
        <html>
            <body>
                <main>
                    <p>Paragraph 1</p>
                    <p>Paragraph 2</p>
                </main>
            </body>
        </html>
        """, "html.parser")

        result = extract_sections(soup)

        # Should create a default "Introduction" section
        assert len(result) == 1
        assert result[0]["header"] == "Introduction"
        assert len(result[0]["chunks"]) == 2

    def test_find_main_content_area_none(self):
        """Test finding main content area when none exists."""
        soup = BeautifulSoup("<html><body></body></html>", "html.parser")

        result = find_main_content_area(soup)
        assert result is None

    def test_process_all_html_files_empty_dir(self, temp_dir):
        """Test processing an empty directory."""
        result = process_all_html_files(temp_dir)
        assert len(result) == 0

    def test_process_all_html_files_non_html(self, temp_dir):
        """Test processing a directory with non-HTML files."""
        # Create a non-HTML file
        file_path = os.path.join(temp_dir, "test.txt")
        with open(file_path, "w", encoding="utf-8") as f:
            f.write("This is not HTML")

        result = process_all_html_files(temp_dir)
        assert len(result) == 0


@pytest.mark.integration
@pytest.mark.chunking
@pytest.mark.edge_case
class TestExtractContentIntegrationEdgeCases:
    """Test edge cases for the extract_content integration."""

    def test_extract_structured_content_missing_tags(self, sample_html_file_with_missing_tags):
        """Test extracting structured content from HTML with missing tags."""
        result = extract_structured_content(sample_html_file_with_missing_tags)

        # Should still have a title and sections
        assert "title" in result
        assert "sections" in result

        # Title should be derived from filename
        assert "Untitled" in result["title"] or result["title"].endswith(".html")

        # Should have at least one section (Introduction)
        assert len(result["sections"]) >= 1

        # Check the content
        first_section = result["sections"][0]
        assert len(first_section["chunks"]) > 0

    def test_extract_structured_content_empty_content(self, sample_html_file_with_empty_content):
        """Test extracting structured content from HTML with empty content."""
        result = extract_structured_content(sample_html_file_with_empty_content)

        # Should have a title and sections
        assert "title" in result
        assert "sections" in result
        assert result["title"] == "Empty Page"

        # Should have at least one section
        assert len(result["sections"]) >= 1

        # The section might have no chunks or empty chunks
        first_section = result["sections"][0]
        if first_section["chunks"]:
            for chunk in first_section["chunks"]:
                assert "content" in chunk
                # Content might be empty
</file>

<file path="tests/test_embedder/test_embed_corpus.py">
"""
Tests for the embed_corpus module.
"""

import os
import json
import pytest
import logging
from unittest.mock import patch, MagicMock, mock_open

from app.embedder.embed_corpus import (
    load_corpus,
    embed_corpus
)


@pytest.mark.unit
@pytest.mark.embedding
class TestLoadCorpus:
    """Test the load_corpus function."""

    def test_load_corpus_success(self, sample_corpus_json_file):
        """Test successfully loading a corpus from a JSON file."""
        result = load_corpus(sample_corpus_json_file)

        # Check that the corpus was loaded
        assert len(result) == 3
        assert result[0]["chunk_id"] == "chunk_test_001"
        assert result[1]["chunk_id"] == "chunk_test_002"
        assert result[2]["chunk_id"] == "chunk_test_003"

    def test_load_corpus_file_not_found(self):
        """Test handling a file not found error."""
        with pytest.raises(Exception) as excinfo:
            load_corpus("nonexistent_file.json")

        # Check the exception
        assert "Error loading corpus" in str(excinfo.value)

    @patch('builtins.open', new_callable=mock_open)
    @patch('json.load')
    def test_load_corpus_json_error(self, mock_json_load, mock_file_open):
        """Test handling a JSON parsing error."""
        # Set up the mock to raise an exception
        mock_json_load.side_effect = json.JSONDecodeError("JSON error", "", 0)

        with pytest.raises(Exception) as excinfo:
            load_corpus("test_corpus.json")

        # Check the exception
        assert "Error loading corpus" in str(excinfo.value)


@pytest.mark.integration
@pytest.mark.embedding
class TestEmbedCorpus:
    """Test the embed_corpus function."""

    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    def test_embed_corpus(self, mock_embedding_function, mock_persistent_client, sample_corpus_json_file):
        """Test embedding a corpus."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Call the function
        embed_corpus(
            corpus_path=sample_corpus_json_file,
            chroma_path="test_chroma_path",
            collection_name="test_collection",
            batch_size=2
        )

        # Check that the client was initialized
        mock_persistent_client.assert_called_once_with(
            path="test_chroma_path",
            settings=pytest.approx({})  # Approximate match for Settings object
        )

        # Check that the collection was created
        mock_client.get_or_create_collection.assert_called_once_with(
            name="test_collection",
            embedding_function=mock_embedding_function.return_value,
            metadata={"description": "Metropole corpus embeddings"}
        )

        # Check that the collection's add method was called twice (batch_size=2, 3 documents)
        assert mock_collection.add.call_count == 2

        # Check the first batch
        first_batch_args, first_batch_kwargs = mock_collection.add.call_args_list[0]
        assert len(first_batch_kwargs["ids"]) == 2
        assert first_batch_kwargs["ids"][0] == "chunk_test_001"
        assert first_batch_kwargs["ids"][1] == "chunk_test_002"
        assert len(first_batch_kwargs["documents"]) == 2
        assert first_batch_kwargs["documents"][0] == "This is the content of section 1."
        assert first_batch_kwargs["documents"][1] == "This is the content of section 2."

        # Check the second batch
        second_batch_args, second_batch_kwargs = mock_collection.add.call_args_list[1]
        assert len(second_batch_kwargs["ids"]) == 1
        assert second_batch_kwargs["ids"][0] == "chunk_test_003"
        assert len(second_batch_kwargs["documents"]) == 1
        assert second_batch_kwargs["documents"][0] == "This is the content of section 1 on page 2."

    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    def test_embed_corpus_with_env_path(self, mock_embedding_function, mock_persistent_client, sample_corpus_json_file):
        """Test embedding a corpus with environment variable path."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set the CHROMA_DB_PATH environment variable
        with patch.dict(os.environ, {"CHROMA_DB_PATH": "env_chroma_path"}):
            # Call the function without specifying chroma_path
            embed_corpus(
                corpus_path=sample_corpus_json_file,
                chroma_path=None,
                collection_name="test_collection",
                batch_size=10
            )

        # Check that the client was initialized with the environment variable path
        mock_persistent_client.assert_called_once_with(
            path="env_chroma_path",
            settings=pytest.approx({})  # Approximate match for Settings object
        )


@pytest.mark.integration
@pytest.mark.embedding
@pytest.mark.edge_case
class TestEmbedCorpusEdgeCases:
    """Test edge cases for the embed_corpus function."""

    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    def test_embed_corpus_empty(self, mock_embedding_function, mock_persistent_client, temp_dir):
        """Test embedding an empty corpus."""
        # Create an empty corpus file
        empty_corpus_path = os.path.join(temp_dir, "empty_corpus.json")
        with open(empty_corpus_path, "w", encoding="utf-8") as f:
            json.dump([], f)

        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Call the function
        embed_corpus(
            corpus_path=empty_corpus_path,
            chroma_path="test_chroma_path",
            collection_name="test_collection",
            batch_size=10
        )

        # Check that the collection's add method was not called
        mock_collection.add.assert_not_called()

    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    def test_embed_corpus_batch_size_one(self, mock_embedding_function, mock_persistent_client, sample_corpus_json_file):
        """Test embedding a corpus with batch size of 1."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Call the function with batch_size=1
        embed_corpus(
            corpus_path=sample_corpus_json_file,
            chroma_path="test_chroma_path",
            collection_name="test_collection",
            batch_size=1
        )

        # Check that the collection's add method was called three times (once for each document)
        assert mock_collection.add.call_count == 3

    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    def test_embed_corpus_chroma_exception(self, mock_embedding_function, mock_persistent_client, sample_corpus_json_file):
        """Test handling ChromaDB exceptions when embedding a corpus."""
        # Set up the mock to raise an exception
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.add.side_effect = Exception("ChromaDB error")
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Call the function - should log the error but not raise it
        with patch('app.embedder.embed_corpus.logger') as mock_logger:
            embed_corpus(
                corpus_path=sample_corpus_json_file,
                chroma_path="test_chroma_path",
                collection_name="test_collection",
                batch_size=10
            )

            # Check that the error was logged
            mock_logger.error.assert_called()

    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    def test_embed_corpus_with_edge_cases(self, mock_embedding_function, mock_persistent_client, sample_corpus_with_edge_cases_json_file):
        """Test embedding a corpus with edge cases."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Call the function
        embed_corpus(
            corpus_path=sample_corpus_with_edge_cases_json_file,
            chroma_path="test_chroma_path",
            collection_name="test_collection",
            batch_size=10
        )

        # Check that the collection's add method was called once
        mock_collection.add.assert_called_once()

        # Check the arguments
        args, kwargs = mock_collection.add.call_args
        assert len(kwargs["ids"]) == 4
        assert kwargs["ids"][0] == "chunk_test_001"
        assert kwargs["ids"][1] == "chunk_test_002"  # Empty content
        assert kwargs["ids"][2] == "chunk_test_003"  # Missing tags
        assert kwargs["ids"][3] == "chunk_test_004"  # Very long content

        # Check the documents
        assert kwargs["documents"][0] == "This is the content of section 1."
        assert kwargs["documents"][1] == ""  # Empty content
        assert kwargs["documents"][2] == "This content has no tags."
        assert "This is a very long content" in kwargs["documents"][3]

        # Check the metadata
        assert kwargs["metadatas"][0]["tags"] == "test,content,section1"
        assert kwargs["metadatas"][1]["tags"] == ""  # Empty tags
        assert kwargs["metadatas"][2]["tags"] == ""  # Empty tags
        assert kwargs["metadatas"][3]["tags"] == "test,long,content"

    @patch('app.embedder.embed_corpus.load_corpus')
    def test_embed_corpus_load_failure(self, mock_load_corpus, temp_dir):
        """Test handling corpus loading failures."""
        # Set up the mock to raise an exception
        mock_load_corpus.side_effect = Exception("Failed to load corpus")

        # Call the function - should raise the exception
        with pytest.raises(Exception) as excinfo:
            embed_corpus(
                corpus_path="test_corpus.json",
                chroma_path=temp_dir,
                collection_name="test_collection",
                batch_size=10
            )

        # Check the exception
        assert "Failed to load corpus" in str(excinfo.value)
</file>

<file path="tests/test_embedder/test_embed.py">
"""
Tests for the embed module.
"""

import os
import pytest
from unittest.mock import patch, MagicMock
from datetime import datetime

from app.embedder.embed import Embedder


@pytest.mark.unit
@pytest.mark.embedding
class TestEmbedderClass:
    """Test the Embedder class."""

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embedder_init(self, mock_persistent_client):
        """Test initializing the Embedder."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Check that the client was initialized
        mock_persistent_client.assert_called_once()

        # Check that the collection was created
        mock_client.get_or_create_collection.assert_called_once_with("documents")

        # Check the embedder attributes
        assert embedder.model_name == "default"
        assert embedder.collection == mock_collection

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_text(self, mock_persistent_client):
        """Test embedding text."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Embed text
        doc_id = embedder.embed_text("This is a test document.")

        # Check that the collection's add method was called
        mock_collection.add.assert_called_once()

        # Check the arguments
        args, kwargs = mock_collection.add.call_args
        assert kwargs["documents"] == ["This is a test document."]
        assert len(kwargs["ids"]) == 1
        assert kwargs["ids"][0] == doc_id
        assert len(kwargs["metadatas"]) == 1
        assert kwargs["metadatas"][0]["source"] == "direct_input"
        assert "timestamp" in kwargs["metadatas"][0]

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_text_with_metadata(self, mock_persistent_client):
        """Test embedding text with custom metadata."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Embed text with custom metadata
        metadata = {"source": "test", "category": "example"}
        doc_id = embedder.embed_text("This is a test document.", doc_id="test_id", metadata=metadata)

        # Check that the collection's add method was called
        mock_collection.add.assert_called_once()

        # Check the arguments
        args, kwargs = mock_collection.add.call_args
        assert kwargs["documents"] == ["This is a test document."]
        assert kwargs["ids"] == ["test_id"]
        assert kwargs["metadatas"] == [metadata]

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_documents(self, mock_persistent_client):
        """Test embedding multiple documents."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Prepare documents
        documents = [
            ("doc1", "This is document 1."),
            ("doc2", "This is document 2.", {"source": "test"}),
            ("doc3", "This is document 3.")
        ]

        # Embed documents
        doc_ids = embedder.embed_documents(documents)

        # Check that the collection's add method was called for each document
        assert mock_collection.add.call_count == 3

        # Check the returned document IDs
        assert doc_ids == ["doc1", "doc2", "doc3"]


@pytest.mark.integration
@pytest.mark.embedding
class TestEmbedderIntegration:
    """Test the Embedder integration."""

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_workflow(self, mock_persistent_client, temp_dir):
        """Test the complete embedding workflow."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set the CHROMA_DB_PATH environment variable
        with patch.dict(os.environ, {"CHROMA_DB_PATH": temp_dir}):
            # Initialize the embedder
            embedder = Embedder()

            # Embed a document
            doc_id = embedder.embed_text("This is a test document.")

            # Check that the collection's add method was called
            mock_collection.add.assert_called_once()

            # Check the arguments
            args, kwargs = mock_collection.add.call_args
            assert kwargs["documents"] == ["This is a test document."]
            assert len(kwargs["ids"]) == 1
            assert kwargs["ids"][0] == doc_id


@pytest.mark.unit
@pytest.mark.embedding
@pytest.mark.edge_case
class TestEmbedderEdgeCases:
    """Test edge cases for the Embedder."""

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_empty_text(self, mock_persistent_client):
        """Test embedding empty text."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Embed empty text
        doc_id = embedder.embed_text("")

        # Check that the collection's add method was called
        mock_collection.add.assert_called_once()

        # Check the arguments
        args, kwargs = mock_collection.add.call_args
        assert kwargs["documents"] == [""]

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_documents_empty_list(self, mock_persistent_client):
        """Test embedding an empty list of documents."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Embed empty list
        doc_ids = embedder.embed_documents([])

        # Check that the collection's add method was not called
        mock_collection.add.assert_not_called()

        # Check the returned document IDs
        assert doc_ids == []

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_text_with_none_metadata(self, mock_persistent_client):
        """Test embedding text with None metadata."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Embed text with None metadata
        doc_id = embedder.embed_text("This is a test document.", metadata=None)

        # Check that the collection's add method was called
        mock_collection.add.assert_called_once()

        # Check the arguments
        args, kwargs = mock_collection.add.call_args
        assert kwargs["documents"] == ["This is a test document."]
        assert len(kwargs["metadatas"]) == 1
        assert kwargs["metadatas"][0]["source"] == "direct_input"
        assert "timestamp" in kwargs["metadatas"][0]

    @patch('app.embedder.embed.chromadb.PersistentClient')
    @patch('app.embedder.embed.uuid.uuid4')
    def test_embed_text_uuid_generation(self, mock_uuid4, mock_persistent_client):
        """Test UUID generation when embedding text."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client
        mock_uuid4.return_value = "test-uuid"

        # Initialize the embedder
        embedder = Embedder()

        # Embed text without specifying doc_id
        doc_id = embedder.embed_text("This is a test document.")

        # Check that uuid4 was called
        mock_uuid4.assert_called_once()

        # Check that the generated UUID was used
        assert doc_id == "test-uuid"

    @patch('app.embedder.embed.chromadb.PersistentClient')
    def test_embed_text_chroma_exception(self, mock_persistent_client):
        """Test handling ChromaDB exceptions when embedding text."""
        # Set up the mock to raise an exception
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_collection.add.side_effect = Exception("ChromaDB error")
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the embedder
        embedder = Embedder()

        # Embed text - should raise the exception
        with pytest.raises(Exception) as excinfo:
            embedder.embed_text("This is a test document.")

        # Check the exception
        assert "ChromaDB error" in str(excinfo.value)
</file>

<file path="tests/test_integration/test_full_pipeline.py">
"""
Integration tests for the full pipeline from crawling to retrieval.
"""

import os
import json
import pytest
import tempfile
import shutil
from unittest.mock import patch, MagicMock
from pathlib import Path

from app.crawler.crawl import fetch_url, recursive_crawl
from app.crawler.extract_content import process_all_html_files
from app.crawler.add_metadata_and_tags import process_content_objects, save_to_json
from app.embedder.embed_corpus import embed_corpus
from app.retriever.ask import Retriever


@pytest.mark.integration
class TestFullPipeline:
    """Test the full pipeline from crawling to retrieval."""

    @patch('app.crawler.crawl.fetch_url')
    @patch('app.crawler.crawl.extract_links')
    @patch('app.crawler.add_metadata_and_tags.KeyBERT')
    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_full_pipeline(
        self,
        mock_openai,
        mock_retriever_client,
        mock_embedding_function,
        mock_embedder_client,
        mock_keybert,
        mock_extract_links,
        mock_fetch_url,
        temp_dir
    ):
        """Test the full pipeline from crawling to retrieval."""
        # Set up the temp directories
        html_dir = os.path.join(temp_dir, "html")
        processed_dir = os.path.join(temp_dir, "processed")
        index_dir = os.path.join(temp_dir, "index")

        os.makedirs(html_dir, exist_ok=True)
        os.makedirs(processed_dir, exist_ok=True)
        os.makedirs(index_dir, exist_ok=True)

        # Set up the mocks for crawling
        mock_fetch_url.side_effect = [
            """
            <!DOCTYPE html>
            <html>
            <head>
                <title>Test Page 1</title>
            </head>
            <body>
                <main>
                    <h1>Main Heading</h1>
                    <section>
                        <h2>Section 1</h2>
                        <p>This is the first paragraph of section 1.</p>
                        <p>This is the second paragraph of section 1.</p>
                    </section>
                    <section>
                        <h2>Section 2</h2>
                        <p>This is the first paragraph of section 2.</p>
                        <ul>
                            <li>List item 1</li>
                            <li>List item 2</li>
                        </ul>
                    </section>
                </main>
            </body>
            </html>
            """,
            """
            <!DOCTYPE html>
            <html>
            <head>
                <title>Test Page 2</title>
            </head>
            <body>
                <main>
                    <h1>Page 2 Heading</h1>
                    <section>
                        <h2>Section 1</h2>
                        <p>This is the content of page 2, section 1.</p>
                    </section>
                </main>
            </body>
            </html>
            """
        ]

        mock_extract_links.side_effect = [
            ["https://example.com/page2"],
            []
        ]

        # Set up the mock for KeyBERT
        mock_keybert_instance = MagicMock()
        mock_keybert_instance.extract_keywords.return_value = [
            ("test", 0.9),
            ("content", 0.8),
            ("section", 0.7)
        ]
        mock_keybert.return_value = mock_keybert_instance

        # Set up the mocks for ChromaDB
        mock_embedder_client_instance = MagicMock()
        mock_embedder_collection = MagicMock()
        mock_embedder_client_instance.get_or_create_collection.return_value = mock_embedder_collection
        mock_embedder_client.return_value = mock_embedder_client_instance

        mock_retriever_client_instance = MagicMock()
        mock_retriever_collection = MagicMock()
        mock_retriever_client_instance.get_or_create_collection.return_value = mock_retriever_collection
        mock_retriever_client.return_value = mock_retriever_client_instance

        # Set up the mock for OpenAI
        mock_openai_client = MagicMock()
        mock_completion = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "This is a sample response from the LLM."
        mock_choice.message = mock_message
        mock_completion.choices = [mock_choice]
        mock_openai_client.chat.completions.create.return_value = mock_completion
        mock_openai.return_value = mock_openai_client

        # Set up the retriever collection's query method to return sample results
        mock_retriever_collection.query.return_value = {
            "ids": [["chunk_test_001", "chunk_test_002"]],
            "documents": [["This is the first paragraph of section 1.", "This is the first paragraph of section 2."]],
            "metadatas": [[
                {"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 1", "tags": "test,content,section"},
                {"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 2", "tags": "test,content,section"}
            ]],
            "distances": [[0.1, 0.2]]
        }

        # Step 1: Crawl the website
        with patch.dict(os.environ, {"PYTHONPATH": os.getcwd()}):
            content_dict = recursive_crawl("https://example.com/page1", max_pages=2, save_to_files=True)

            # Check that the crawl was successful
            assert len(content_dict) == 2
            assert "https://example.com/page1" in content_dict
            assert "https://example.com/page2" in content_dict

            # Write the HTML files to the temp directory
            for i, (url, html) in enumerate(content_dict.items()):
                file_path = os.path.join(html_dir, f"test_page_{i+1}.html")
                with open(file_path, "w", encoding="utf-8") as f:
                    f.write(html)

            # Step 2: Process the HTML files
            results = process_all_html_files(html_dir)

            # Check that the processing was successful
            assert len(results) == 2

            # Create content objects
            content_objects = []
            for file_path, content in results.items():
                page_title = content['title']
                page_name = os.path.basename(file_path).replace('.html', '')

                for section in content['sections']:
                    section_header = section['header']

                    for chunk in section['chunks']:
                        chunk_object = {
                            'page_title': page_title,
                            'page_name': page_name,
                            'section_header': section_header,
                            'content': chunk['content'],
                            'content_html': chunk['content_html']
                        }
                        content_objects.append(chunk_object)

            # Step 3: Add metadata and tags
            with patch('app.crawler.add_metadata_and_tags.content_objects', content_objects):
                processed_objects = process_content_objects()

                # Check that the metadata and tags were added
                assert len(processed_objects) == len(content_objects)
                for obj in processed_objects:
                    assert "chunk_id" in obj
                    assert "page_id" in obj
                    assert "tags" in obj

                # Save to JSON
                corpus_path = os.path.join(processed_dir, "test_corpus.json")
                save_to_json(processed_objects, corpus_path)

                # Check that the file was created
                assert os.path.exists(corpus_path)

            # Step 4: Embed the corpus
            embed_corpus(
                corpus_path=corpus_path,
                chroma_path=index_dir,
                collection_name="test_collection",
                batch_size=10
            )

            # Check that the embeddings were created
            mock_embedder_collection.add.assert_called_once()

            # Step 5: Query the vector store
            with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
                retriever = Retriever()

                # Query the vector store
                query_results = retriever.query("test query", n_results=2)

                # Check that the query was successful
                assert "ids" in query_results
                assert "documents" in query_results
                assert "metadatas" in query_results

                # Generate an answer
                chunks = []
                for i, doc_id in enumerate(query_results["ids"][0]):
                    chunk = MagicMock()
                    chunk.text = query_results["documents"][0][i]
                    chunk.metadata = query_results["metadatas"][0][i]
                    chunks.append(chunk)

                answer_result = retriever.generate_answer("What is the answer?", chunks)

                # Check that the answer was generated
                assert "answer" in answer_result
                assert answer_result["answer"] == "This is a sample response from the LLM."


@pytest.mark.integration
@pytest.mark.edge_case
class TestFullPipelineEdgeCases:
    """Test edge cases for the full pipeline."""

    @patch('app.crawler.crawl.fetch_url')
    @patch('app.crawler.crawl.extract_links')
    @patch('app.crawler.add_metadata_and_tags.KeyBERT')
    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_pipeline_with_empty_content(
        self,
        mock_openai,
        mock_retriever_client,
        mock_embedding_function,
        mock_embedder_client,
        mock_keybert,
        mock_extract_links,
        mock_fetch_url,
        temp_dir
    ):
        """Test the pipeline with empty content."""
        # Set up the temp directories
        html_dir = os.path.join(temp_dir, "html")
        processed_dir = os.path.join(temp_dir, "processed")
        index_dir = os.path.join(temp_dir, "index")

        os.makedirs(html_dir, exist_ok=True)
        os.makedirs(processed_dir, exist_ok=True)
        os.makedirs(index_dir, exist_ok=True)

        # Set up the mocks for crawling with empty content
        mock_fetch_url.side_effect = [
            """
            <!DOCTYPE html>
            <html>
            <head>
                <title>Empty Page</title>
            </head>
            <body>
                <main>
                    <h1>Empty Heading</h1>
                    <section>
                        <h2>Empty Section</h2>
                        <!-- No content here -->
                    </section>
                </main>
            </body>
            </html>
            """
        ]

        mock_extract_links.return_value = []

        # Set up the mock for KeyBERT
        mock_keybert_instance = MagicMock()
        mock_keybert_instance.extract_keywords.return_value = []  # No keywords for empty content
        mock_keybert.return_value = mock_keybert_instance

        # Set up the mocks for ChromaDB
        mock_embedder_client_instance = MagicMock()
        mock_embedder_collection = MagicMock()
        mock_embedder_client_instance.get_or_create_collection.return_value = mock_embedder_collection
        mock_embedder_client.return_value = mock_embedder_client_instance

        mock_retriever_client_instance = MagicMock()
        mock_retriever_collection = MagicMock()
        mock_retriever_client_instance.get_or_create_collection.return_value = mock_retriever_collection
        mock_retriever_client.return_value = mock_retriever_client_instance

        # Set up the mock for OpenAI
        mock_openai_client = MagicMock()
        mock_completion = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "I don't have specific information about that."
        mock_choice.message = mock_message
        mock_completion.choices = [mock_choice]
        mock_openai_client.chat.completions.create.return_value = mock_completion
        mock_openai.return_value = mock_openai_client

        # Set up the retriever collection's query method to return empty results
        mock_retriever_collection.query.return_value = {
            "ids": [[]],
            "documents": [[]],
            "metadatas": [[]],
            "distances": [[]]
        }

        # Step 1: Crawl the website
        with patch.dict(os.environ, {"PYTHONPATH": os.getcwd()}):
            content_dict = recursive_crawl("https://example.com/empty", max_pages=1, save_to_files=True)

            # Check that the crawl was successful
            assert len(content_dict) == 1

            # Write the HTML file to the temp directory
            file_path = os.path.join(html_dir, "empty_page.html")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content_dict["https://example.com/empty"])

            # Step 2: Process the HTML files
            results = process_all_html_files(html_dir)

            # Check that the processing was successful
            assert len(results) == 1

            # Create content objects
            content_objects = []
            for file_path, content in results.items():
                page_title = content['title']
                page_name = os.path.basename(file_path).replace('.html', '')

                for section in content['sections']:
                    section_header = section['header']

                    for chunk in section['chunks']:
                        chunk_object = {
                            'page_title': page_title,
                            'page_name': page_name,
                            'section_header': section_header,
                            'content': chunk['content'],
                            'content_html': chunk['content_html']
                        }
                        content_objects.append(chunk_object)

            # Step 3: Add metadata and tags
            with patch('app.crawler.add_metadata_and_tags.content_objects', content_objects):
                processed_objects = process_content_objects()

                # Check that the metadata and tags were added
                assert len(processed_objects) == len(content_objects)

                # Save to JSON
                corpus_path = os.path.join(processed_dir, "empty_corpus.json")
                save_to_json(processed_objects, corpus_path)

            # Step 4: Embed the corpus
            embed_corpus(
                corpus_path=corpus_path,
                chroma_path=index_dir,
                collection_name="empty_collection",
                batch_size=10
            )

            # Step 5: Query the vector store
            with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
                retriever = Retriever()

                # Query the vector store
                query_results = retriever.query("test query", n_results=2)

                # Check that the query returned empty results
                assert query_results["ids"][0] == []

                # Generate an answer with empty chunks
                answer_result = retriever.generate_answer("What is the answer?", [])

                # Check that the answer was generated
                assert "answer" in answer_result
                assert result["is_general_knowledge"] is True

    @patch('app.crawler.crawl.fetch_url')
    @patch('app.crawler.crawl.extract_links')
    @patch('app.crawler.add_metadata_and_tags.KeyBERT')
    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_pipeline_with_missing_tags(
        self,
        mock_openai,
        mock_retriever_client,
        mock_embedding_function,
        mock_embedder_client,
        mock_keybert,
        mock_extract_links,
        mock_fetch_url,
        temp_dir
    ):
        """Test the pipeline with content missing tags."""
        # Set up the temp directories
        html_dir = os.path.join(temp_dir, "html")
        processed_dir = os.path.join(temp_dir, "processed")
        index_dir = os.path.join(temp_dir, "index")

        os.makedirs(html_dir, exist_ok=True)
        os.makedirs(processed_dir, exist_ok=True)
        os.makedirs(index_dir, exist_ok=True)

        # Set up the mocks for crawling
        mock_fetch_url.return_value = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Test Page</title>
        </head>
        <body>
            <main>
                <h1>Main Heading</h1>
                <section>
                    <h2>Section 1</h2>
                    <p>This is a paragraph.</p>
                </section>
            </main>
        </body>
        </html>
        """

        mock_extract_links.return_value = []

        # Set up the mock for KeyBERT to return empty tags
        mock_keybert_instance = MagicMock()
        mock_keybert_instance.extract_keywords.return_value = []
        mock_keybert.return_value = mock_keybert_instance

        # Set up the mocks for ChromaDB
        mock_embedder_client_instance = MagicMock()
        mock_embedder_collection = MagicMock()
        mock_embedder_client_instance.get_or_create_collection.return_value = mock_embedder_collection
        mock_embedder_client.return_value = mock_embedder_client_instance

        mock_retriever_client_instance = MagicMock()
        mock_retriever_collection = MagicMock()
        mock_retriever_client_instance.get_or_create_collection.return_value = mock_retriever_collection
        mock_retriever_client.return_value = mock_retriever_client_instance

        # Set up the mock for OpenAI
        mock_openai_client = MagicMock()
        mock_completion = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "This is a sample response from the LLM."
        mock_choice.message = mock_message
        mock_completion.choices = [mock_choice]
        mock_openai_client.chat.completions.create.return_value = mock_completion
        mock_openai.return_value = mock_openai_client

        # Set up the retriever collection's query method to return results with missing tags
        mock_retriever_collection.query.return_value = {
            "ids": [["chunk_test_001"]],
            "documents": [["This is a paragraph."]],
            "metadatas": [[
                {"page_id": "page_test_001", "page_title": "Test Page", "section_header": "Section 1", "tags": ""}
            ]],
            "distances": [[0.1]]
        }

        # Step 1: Crawl the website
        with patch.dict(os.environ, {"PYTHONPATH": os.getcwd()}):
            content_dict = recursive_crawl("https://example.com/page", max_pages=1, save_to_files=True)

            # Write the HTML file to the temp directory
            file_path = os.path.join(html_dir, "test_page.html")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content_dict["https://example.com/page"])

            # Step 2: Process the HTML files
            results = process_all_html_files(html_dir)

            # Create content objects
            content_objects = []
            for file_path, content in results.items():
                page_title = content['title']
                page_name = os.path.basename(file_path).replace('.html', '')

                for section in content['sections']:
                    section_header = section['header']

                    for chunk in section['chunks']:
                        chunk_object = {
                            'page_title': page_title,
                            'page_name': page_name,
                            'section_header': section_header,
                            'content': chunk['content'],
                            'content_html': chunk['content_html']
                        }
                        content_objects.append(chunk_object)

            # Step 3: Add metadata and tags
            with patch('app.crawler.add_metadata_and_tags.content_objects', content_objects):
                processed_objects = process_content_objects()

                # Check that the metadata was added but tags are empty
                assert len(processed_objects) == len(content_objects)
                for obj in processed_objects:
                    assert "chunk_id" in obj
                    assert "page_id" in obj
                    assert "tags" in obj
                    assert obj["tags"] == []  # Tags should be empty

                # Save to JSON
                corpus_path = os.path.join(processed_dir, "no_tags_corpus.json")
                save_to_json(processed_objects, corpus_path)

            # Step 4: Embed the corpus
            embed_corpus(
                corpus_path=corpus_path,
                chroma_path=index_dir,
                collection_name="no_tags_collection",
                batch_size=10
            )

            # Step 5: Query the vector store
            with patch.dict(os.environ, {"OPENAI_API_KEY": "test_key"}):
                retriever = Retriever()

                # Query the vector store
                query_results = retriever.query("test query", n_results=2)

                # Generate an answer
                chunks = []
                for i, doc_id in enumerate(query_results["ids"][0]):
                    chunk = MagicMock()
                    chunk.text = query_results["documents"][0][i]
                    chunk.metadata = query_results["metadatas"][0][i]
                    chunks.append(chunk)

                answer_result = retriever.generate_answer("What is the answer?", chunks)

                # Check that the answer was generated
                assert "answer" in answer_result
                assert answer_result["answer"] == "This is a sample response from the LLM."

    @patch('app.crawler.crawl.fetch_url')
    @patch('app.crawler.crawl.extract_links')
    @patch('app.crawler.add_metadata_and_tags.KeyBERT')
    @patch('app.embedder.embed_corpus.chromadb.PersistentClient')
    @patch('app.embedder.embed_corpus.embedding_functions.DefaultEmbeddingFunction')
    @patch('app.retriever.ask.chromadb.PersistentClient')
    def test_pipeline_with_missing_api_key(
        self,
        mock_retriever_client,
        mock_embedding_function,
        mock_embedder_client,
        mock_keybert,
        mock_extract_links,
        mock_fetch_url,
        temp_dir
    ):
        """Test the pipeline with missing OpenAI API key."""
        # Set up the temp directories
        html_dir = os.path.join(temp_dir, "html")
        processed_dir = os.path.join(temp_dir, "processed")
        index_dir = os.path.join(temp_dir, "index")

        os.makedirs(html_dir, exist_ok=True)
        os.makedirs(processed_dir, exist_ok=True)
        os.makedirs(index_dir, exist_ok=True)

        # Set up the mocks for crawling
        mock_fetch_url.return_value = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Test Page</title>
        </head>
        <body>
            <main>
                <h1>Main Heading</h1>
                <section>
                    <h2>Section 1</h2>
                    <p>This is a paragraph.</p>
                </section>
            </main>
        </body>
        </html>
        """

        mock_extract_links.return_value = []

        # Set up the mock for KeyBERT
        mock_keybert_instance = MagicMock()
        mock_keybert_instance.extract_keywords.return_value = [
            ("test", 0.9),
            ("content", 0.8)
        ]
        mock_keybert.return_value = mock_keybert_instance

        # Set up the mocks for ChromaDB
        mock_embedder_client_instance = MagicMock()
        mock_embedder_collection = MagicMock()
        mock_embedder_client_instance.get_or_create_collection.return_value = mock_embedder_collection
        mock_embedder_client.return_value = mock_embedder_client_instance

        mock_retriever_client_instance = MagicMock()
        mock_retriever_collection = MagicMock()
        mock_retriever_client_instance.get_or_create_collection.return_value = mock_retriever_collection
        mock_retriever_client.return_value = mock_retriever_client_instance

        # Step 1: Crawl the website
        with patch.dict(os.environ, {"PYTHONPATH": os.getcwd()}):
            content_dict = recursive_crawl("https://example.com/page", max_pages=1, save_to_files=True)

            # Write the HTML file to the temp directory
            file_path = os.path.join(html_dir, "test_page.html")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content_dict["https://example.com/page"])

            # Step 2: Process the HTML files
            results = process_all_html_files(html_dir)

            # Create content objects
            content_objects = []
            for file_path, content in results.items():
                page_title = content['title']
                page_name = os.path.basename(file_path).replace('.html', '')

                for section in content['sections']:
                    section_header = section['header']

                    for chunk in section['chunks']:
                        chunk_object = {
                            'page_title': page_title,
                            'page_name': page_name,
                            'section_header': section_header,
                            'content': chunk['content'],
                            'content_html': chunk['content_html']
                        }
                        content_objects.append(chunk_object)

            # Step 3: Add metadata and tags
            with patch('app.crawler.add_metadata_and_tags.content_objects', content_objects):
                processed_objects = process_content_objects()

                # Save to JSON
                corpus_path = os.path.join(processed_dir, "api_key_corpus.json")
                save_to_json(processed_objects, corpus_path)

            # Step 4: Embed the corpus
            embed_corpus(
                corpus_path=corpus_path,
                chroma_path=index_dir,
                collection_name="api_key_collection",
                batch_size=10
            )

            # Step 5: Try to initialize the retriever with missing API key
            with patch.dict(os.environ, {"OPENAI_API_KEY": ""}):
                # Should raise ValueError
                with pytest.raises(ValueError) as excinfo:
                    retriever = Retriever()

                # Check the exception
                assert "OPENAI_API_KEY" in str(excinfo.value)
</file>

<file path="tests/test_retriever/test_ask.py">
"""
Tests for the ask module.
"""

import os
import pytest
from unittest.mock import patch, MagicMock
from pathlib import Path

from app.retriever.ask import Retriever


@pytest.mark.unit
@pytest.mark.retrieval
class TestRetrieverClass:
    """Test the Retriever class."""

    @patch('app.retriever.ask.chromadb.PersistentClient')
    def test_retriever_init(self, mock_persistent_client):
        """Test initializing the Retriever."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the retriever
        retriever = Retriever()

        # Check that the client was initialized
        mock_persistent_client.assert_called_once()

        # Check that the collection was created
        mock_client.get_or_create_collection.assert_called_once_with("documents")

        # Check the retriever attributes
        assert retriever.collection == mock_collection

    @patch('app.retriever.ask.chromadb.PersistentClient')
    def test_query(self, mock_persistent_client):
        """Test querying the vector store."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the collection's query method to return sample results
        mock_collection.query.return_value = {
            "ids": [["chunk_test_001", "chunk_test_002", "chunk_test_003"]],
            "documents": [["This is the content of section 1.", "This is the content of section 2.", "This is the content of section 1 on page 2."]],
            "metadatas": [[
                {"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 1", "tags": "test,content,section1"},
                {"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 2", "tags": "test,content,section2"},
                {"page_id": "page_test_002", "page_title": "Test Page 2", "section_header": "Section 1", "tags": "test,content,page2,section1"}
            ]],
            "distances": [[0.1, 0.2, 0.3]]
        }

        # Initialize the retriever
        retriever = Retriever()

        # Query the vector store
        result = retriever.query("test query", n_results=3)

        # Check that the collection's query method was called
        mock_collection.query.assert_called_once_with(
            query_texts=["test query"],
            n_results=3,
            include=["documents", "metadatas", "distances"],
        )

        # Check the result
        assert result == mock_collection.query.return_value

    @patch('app.retriever.ask.chromadb.PersistentClient')
    def test_get_document(self, mock_persistent_client):
        """Test retrieving a specific document by ID."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the collection's get method to return sample results
        mock_collection.get.return_value = {
            "ids": ["chunk_test_001"],
            "documents": ["This is the content of section 1."],
            "metadatas": [{"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 1", "tags": "test,content,section1"}]
        }

        # Initialize the retriever
        retriever = Retriever()

        # Get a document
        result = retriever.get_document("chunk_test_001")

        # Check that the collection's get method was called
        mock_collection.get.assert_called_once_with(ids=["chunk_test_001"])

        # Check the result
        assert result == mock_collection.get.return_value


@pytest.mark.integration
@pytest.mark.retrieval
class TestRetrieverIntegration:
    """Test the Retriever integration."""

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_generate_answer(self, mock_openai, mock_persistent_client):
        """Test generating an answer using OpenAI."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the OpenAI mock
        mock_openai_client = MagicMock()
        mock_openai.return_value = mock_openai_client

        # Set up the chat completions mock
        mock_completion = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "This is a sample response from the LLM."
        mock_choice.message = mock_message
        mock_completion.choices = [mock_choice]
        mock_openai_client.chat.completions.create.return_value = mock_completion

        # Initialize the retriever
        retriever = Retriever()

        # Generate an answer
        chunks = [
            MagicMock(text="Chunk 1 text", metadata={"chunk_id": "chunk1", "page_title": "Page 1", "section_header": "Section 1"}),
            MagicMock(text="Chunk 2 text", metadata={"chunk_id": "chunk2", "page_title": "Page 1", "section_header": "Section 2"})
        ]

        result = retriever.generate_answer("What is the answer?", chunks)

        # Check that the OpenAI API was called
        mock_openai_client.chat.completions.create.assert_called_once()

        # Check the result
        assert "answer" in result
        assert result["answer"] == "This is a sample response from the LLM."
        assert "is_general_knowledge" in result
        assert "contains_diy_advice" in result
        assert "source_info" in result

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_generate_answer_with_general_knowledge(self, mock_openai, mock_persistent_client):
        """Test generating an answer with general knowledge flag."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the OpenAI mock
        mock_openai_client = MagicMock()
        mock_openai.return_value = mock_openai_client

        # Set up the chat completions mock with general knowledge response
        mock_completion = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "Based on general knowledge, the answer is..."
        mock_choice.message = mock_message
        mock_completion.choices = [mock_choice]
        mock_openai_client.chat.completions.create.return_value = mock_completion

        # Initialize the retriever
        retriever = Retriever()

        # Generate an answer
        chunks = [
            MagicMock(text="Chunk 1 text", metadata={"chunk_id": "chunk1", "page_title": "Page 1", "section_header": "Section 1"})
        ]

        result = retriever.generate_answer("What is the answer?", chunks)

        # Check the result
        assert result["is_general_knowledge"] is True
        assert "general knowledge" in result["answer"]

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_generate_answer_with_diy_advice(self, mock_openai, mock_persistent_client):
        """Test generating an answer with DIY advice flag."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the OpenAI mock
        mock_openai_client = MagicMock()
        mock_openai.return_value = mock_openai_client

        # Set up the chat completions mock with DIY advice response
        mock_completion = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "You can try these steps to fix the issue..."
        mock_choice.message = mock_message
        mock_completion.choices = [mock_choice]
        mock_openai_client.chat.completions.create.return_value = mock_completion

        # Initialize the retriever
        retriever = Retriever()

        # Generate an answer
        chunks = [
            MagicMock(text="Chunk 1 text", metadata={"chunk_id": "chunk1", "page_title": "Page 1", "section_header": "Section 1"})
        ]

        result = retriever.generate_answer("How do I fix this?", chunks)

        # Check the result
        assert result["contains_diy_advice"] is True
        assert "Disclaimer" in result["answer"]


@pytest.mark.unit
@pytest.mark.retrieval
@pytest.mark.edge_case
class TestRetrieverEdgeCases:
    """Test edge cases for the Retriever."""

    @patch('app.retriever.ask.chromadb.PersistentClient')
    def test_query_empty_results(self, mock_persistent_client):
        """Test querying with empty results."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the collection's query method to return empty results
        mock_collection.query.return_value = {
            "ids": [[]],
            "documents": [[]],
            "metadatas": [[]],
            "distances": [[]]
        }

        # Initialize the retriever
        retriever = Retriever()

        # Query the vector store
        result = retriever.query("test query")

        # Check the result
        assert result["ids"][0] == []
        assert result["documents"][0] == []
        assert result["metadatas"][0] == []
        assert result["distances"][0] == []

    @patch('app.retriever.ask.chromadb.PersistentClient')
    def test_get_document_not_found(self, mock_persistent_client):
        """Test retrieving a document that doesn't exist."""
        # Set up the mock
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the collection's get method to return empty results
        mock_collection.get.return_value = {
            "ids": [],
            "documents": [],
            "metadatas": []
        }

        # Initialize the retriever
        retriever = Retriever()

        # Get a document
        result = retriever.get_document("nonexistent_id")

        # Check the result
        assert result["ids"] == []
        assert result["documents"] == []
        assert result["metadatas"] == []

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_generate_answer_empty_chunks(self, mock_openai, mock_persistent_client):
        """Test generating an answer with empty chunks."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the OpenAI mock
        mock_openai_client = MagicMock()
        mock_openai.return_value = mock_openai_client

        # Set up the chat completions mock
        mock_completion = MagicMock()
        mock_choice = MagicMock()
        mock_message = MagicMock()
        mock_message.content = "I don't have specific information about that."
        mock_choice.message = mock_message
        mock_completion.choices = [mock_choice]
        mock_openai_client.chat.completions.create.return_value = mock_completion

        # Initialize the retriever
        retriever = Retriever()

        # Generate an answer with empty chunks
        result = retriever.generate_answer("What is the answer?", [])

        # Check the result
        assert "answer" in result
        assert result["is_general_knowledge"] is True
        assert "source_info" in result
        assert result["source_info"] == ""

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_generate_answer_openai_error(self, mock_openai, mock_persistent_client):
        """Test handling OpenAI API errors."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the OpenAI mock to raise an exception
        mock_openai_client = MagicMock()
        mock_openai.return_value = mock_openai_client
        mock_openai_client.chat.completions.create.side_effect = Exception("OpenAI API error")

        # Initialize the retriever
        retriever = Retriever()

        # Generate an answer - should raise the exception
        with pytest.raises(Exception) as excinfo:
            retriever.generate_answer("What is the answer?", [
                MagicMock(text="Chunk 1 text", metadata={"chunk_id": "chunk1", "page_title": "Page 1", "section_header": "Section 1"})
            ])

        # Check the exception
        assert "OpenAI API error" in str(excinfo.value)

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    @patch.dict(os.environ, {"OPENAI_API_KEY": ""})
    def test_missing_api_key(self, mock_openai, mock_persistent_client):
        """Test handling missing OpenAI API key."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Initialize the retriever - should raise ValueError
        with pytest.raises(ValueError) as excinfo:
            retriever = Retriever()

        # Check the exception
        assert "OPENAI_API_KEY" in str(excinfo.value)

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_prepare_source_info_empty(self, mock_openai, mock_persistent_client):
        """Test preparing source info with empty chunks."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the OpenAI mock
        mock_openai_client = MagicMock()
        mock_openai.return_value = mock_openai_client

        # Initialize the retriever
        retriever = Retriever()

        # Call the private method
        result = retriever._prepare_source_info([])

        # Check the result
        assert result == ""

    @patch('app.retriever.ask.chromadb.PersistentClient')
    @patch('app.retriever.ask.openai.OpenAI')
    def test_prepare_source_info_missing_metadata(self, mock_openai, mock_persistent_client):
        """Test preparing source info with chunks missing metadata."""
        # Set up the mocks
        mock_client = MagicMock()
        mock_collection = MagicMock()
        mock_client.get_or_create_collection.return_value = mock_collection
        mock_persistent_client.return_value = mock_client

        # Set up the OpenAI mock
        mock_openai_client = MagicMock()
        mock_openai.return_value = mock_openai_client

        # Initialize the retriever
        retriever = Retriever()

        # Create chunks without metadata
        chunks = [
            MagicMock(text="Chunk 1 text", metadata=None),
            MagicMock(text="Chunk 2 text")  # No metadata attribute
        ]

        # Call the private method
        result = retriever._prepare_source_info(chunks)

        # Check the result - should handle missing metadata gracefully
        assert "unknown-0" in result
        assert "unknown-1" in result
</file>

<file path="tests/conftest.py">
"""
Pytest configuration file with common fixtures for all tests.
"""

import os
import sys
import json
import pytest
import tempfile
import shutil
from pathlib import Path
from unittest.mock import patch, MagicMock

# Add the project root to the Python path to allow importing from app
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import chromadb
from chromadb.config import Settings
from chromadb.utils import embedding_functions


@pytest.fixture
def temp_dir():
    """Create a temporary directory for test files."""
    temp_dir = tempfile.mkdtemp()
    yield temp_dir
    shutil.rmtree(temp_dir)


@pytest.fixture
def sample_html():
    """Return a sample HTML content for testing."""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Test Page</title>
        <meta property="og:title" content="Test Page Meta Title">
    </head>
    <body>
        <header>
            <nav>
                <a href="/">Home</a>
                <a href="/about">About</a>
            </nav>
        </header>
        <main>
            <h1>Main Heading</h1>
            <section>
                <h2>Section 1</h2>
                <p>This is the first paragraph of section 1.</p>
                <p>This is the second paragraph of section 1.</p>
            </section>
            <section>
                <h2>Section 2</h2>
                <p>This is the first paragraph of section 2.</p>
                <ul>
                    <li>List item 1</li>
                    <li>List item 2</li>
                </ul>
            </section>
        </main>
        <footer>
            <p>Footer content</p>
        </footer>
    </body>
    </html>
    """


@pytest.fixture
def sample_html_file(temp_dir, sample_html):
    """Create a sample HTML file for testing."""
    file_path = os.path.join(temp_dir, "test_page.html")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(sample_html)
    return file_path


@pytest.fixture
def sample_html_with_missing_tags():
    """Return a sample HTML content with missing tags for testing edge cases."""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <!-- Missing title tag -->
    </head>
    <body>
        <!-- Missing main content -->
        <p>Some content without proper structure</p>
        <div>Another div without headers</div>
    </body>
    </html>
    """


@pytest.fixture
def sample_html_with_empty_content():
    """Return a sample HTML content with empty sections for testing edge cases."""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>Empty Page</title>
    </head>
    <body>
        <main>
            <h1>Empty Heading</h1>
            <section>
                <h2>Empty Section</h2>
                <!-- No content here -->
            </section>
        </main>
    </body>
    </html>
    """


@pytest.fixture
def sample_html_file_with_missing_tags(temp_dir, sample_html_with_missing_tags):
    """Create a sample HTML file with missing tags for testing edge cases."""
    file_path = os.path.join(temp_dir, "test_page_missing_tags.html")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(sample_html_with_missing_tags)
    return file_path


@pytest.fixture
def sample_html_file_with_empty_content(temp_dir, sample_html_with_empty_content):
    """Create a sample HTML file with empty content for testing edge cases."""
    file_path = os.path.join(temp_dir, "test_page_empty_content.html")
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(sample_html_with_empty_content)
    return file_path


@pytest.fixture
def sample_content_objects():
    """Return sample content objects for testing."""
    return [
        {
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Section 1",
            "content": "This is the content of section 1.",
            "content_html": "<p>This is the content of section 1.</p>"
        },
        {
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Section 2",
            "content": "This is the content of section 2.",
            "content_html": "<p>This is the content of section 2.</p>"
        },
        {
            "page_title": "Test Page 2",
            "page_name": "test_page_2",
            "section_header": "Section 1",
            "content": "This is the content of section 1 on page 2.",
            "content_html": "<p>This is the content of section 1 on page 2.</p>"
        }
    ]


@pytest.fixture
def sample_content_objects_with_edge_cases():
    """Return sample content objects with edge cases for testing."""
    return [
        # Normal content
        {
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Section 1",
            "content": "This is the content of section 1.",
            "content_html": "<p>This is the content of section 1.</p>"
        },
        # Empty content
        {
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Empty Section",
            "content": "",
            "content_html": "<p></p>"
        },
        # Very short content (less than 20 chars)
        {
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Short Section",
            "content": "Short text.",
            "content_html": "<p>Short text.</p>"
        },
        # Very long content
        {
            "page_title": "Test Page 2",
            "page_name": "test_page_2",
            "section_header": "Long Section",
            "content": "This is a very long content that exceeds the typical length of a chunk. " * 20,
            "content_html": f"<p>{'This is a very long content that exceeds the typical length of a chunk. ' * 20}</p>"
        }
    ]


@pytest.fixture
def sample_corpus_with_metadata():
    """Return a sample corpus with metadata and tags for testing."""
    return [
        {
            "chunk_id": "chunk_test_001",
            "page_id": "page_test_001",
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Section 1",
            "content": "This is the content of section 1.",
            "content_html": "<p>This is the content of section 1.</p>",
            "tags": ["test", "content", "section1"]
        },
        {
            "chunk_id": "chunk_test_002",
            "page_id": "page_test_001",
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Section 2",
            "content": "This is the content of section 2.",
            "content_html": "<p>This is the content of section 2.</p>",
            "tags": ["test", "content", "section2"]
        },
        {
            "chunk_id": "chunk_test_003",
            "page_id": "page_test_002",
            "page_title": "Test Page 2",
            "page_name": "test_page_2",
            "section_header": "Section 1",
            "content": "This is the content of section 1 on page 2.",
            "content_html": "<p>This is the content of section 1 on page 2.</p>",
            "tags": ["test", "content", "page2", "section1"]
        }
    ]


@pytest.fixture
def sample_corpus_with_edge_cases():
    """Return a sample corpus with edge cases for testing."""
    return [
        # Normal chunk
        {
            "chunk_id": "chunk_test_001",
            "page_id": "page_test_001",
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Section 1",
            "content": "This is the content of section 1.",
            "content_html": "<p>This is the content of section 1.</p>",
            "tags": ["test", "content", "section1"]
        },
        # Empty content
        {
            "chunk_id": "chunk_test_002",
            "page_id": "page_test_001",
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Empty Section",
            "content": "",
            "content_html": "<p></p>",
            "tags": []
        },
        # Missing tags
        {
            "chunk_id": "chunk_test_003",
            "page_id": "page_test_001",
            "page_title": "Test Page 1",
            "page_name": "test_page_1",
            "section_header": "Section with Missing Tags",
            "content": "This content has no tags.",
            "content_html": "<p>This content has no tags.</p>",
            "tags": []
        },
        # Very long content
        {
            "chunk_id": "chunk_test_004",
            "page_id": "page_test_002",
            "page_title": "Test Page 2",
            "page_name": "test_page_2",
            "section_header": "Long Section",
            "content": "This is a very long content that exceeds the typical length of a chunk. " * 20,
            "content_html": f"<p>{'This is a very long content that exceeds the typical length of a chunk. ' * 20}</p>",
            "tags": ["test", "long", "content"]
        }
    ]


@pytest.fixture
def sample_corpus_json_file(temp_dir, sample_corpus_with_metadata):
    """Create a sample corpus JSON file for testing."""
    file_path = os.path.join(temp_dir, "test_corpus.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(sample_corpus_with_metadata, f, indent=2)
    return file_path


@pytest.fixture
def sample_corpus_with_edge_cases_json_file(temp_dir, sample_corpus_with_edge_cases):
    """Create a sample corpus JSON file with edge cases for testing."""
    file_path = os.path.join(temp_dir, "test_corpus_edge_cases.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(sample_corpus_with_edge_cases, f, indent=2)
    return file_path


@pytest.fixture
def mock_chroma_client():
    """Create a mock ChromaDB client for testing."""
    mock_client = MagicMock()
    mock_collection = MagicMock()
    mock_client.get_or_create_collection.return_value = mock_collection
    mock_client.get_collection.return_value = mock_collection

    # Set up the collection's query method to return sample results
    mock_collection.query.return_value = {
        "ids": [["chunk_test_001", "chunk_test_002", "chunk_test_003"]],
        "documents": [["This is the content of section 1.", "This is the content of section 2.", "This is the content of section 1 on page 2."]],
        "metadatas": [[
            {"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 1", "tags": "test,content,section1"},
            {"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 2", "tags": "test,content,section2"},
            {"page_id": "page_test_002", "page_title": "Test Page 2", "section_header": "Section 1", "tags": "test,content,page2,section1"}
        ]],
        "distances": [[0.1, 0.2, 0.3]]
    }

    # Set up the collection's get method to return sample results
    mock_collection.get.return_value = {
        "ids": ["chunk_test_001"],
        "documents": ["This is the content of section 1."],
        "metadatas": [{"page_id": "page_test_001", "page_title": "Test Page 1", "section_header": "Section 1", "tags": "test,content,section1"}]
    }

    return mock_client


@pytest.fixture
def mock_openai_client():
    """Create a mock OpenAI client for testing."""
    mock_client = MagicMock()
    mock_completion = MagicMock()
    mock_choice = MagicMock()
    mock_message = MagicMock()

    # Set up the message content
    mock_message.content = "This is a sample response from the LLM."

    # Set up the choice with the message
    mock_choice.message = mock_message

    # Set up the completion with the choice
    mock_completion.choices = [mock_choice]

    # Set up the client's chat.completions.create method to return the mock completion
    mock_client.chat.completions.create.return_value = mock_completion

    return mock_client


@pytest.fixture
def mock_keybert():
    """Create a mock KeyBERT model for testing."""
    mock_model = MagicMock()
    mock_model.extract_keywords.return_value = [
        ("test", 0.9),
        ("content", 0.8),
        ("section", 0.7)
    ]
    return mock_model
</file>

<file path="main.py">
from dotenv import load_dotenv
import os
from fastapi import FastAPI
import uvicorn
from pathlib import Path
from fastapi.middleware.cors import CORSMiddleware
from app.api.routes import router as api_router
from app.vector_store.init_chroma import init_chroma_db

# Load environment variables
load_dotenv()

# Set default Chroma DB path if not in environment
if not os.getenv("CHROMA_DB_PATH"):
    os.environ["CHROMA_DB_PATH"] = "./data/index"

# Create FastAPI app
app = FastAPI(
    title="MetPol AI",
    description="A FastAPI application for crawling, embedding, and retrieving information",
    version="0.1.0"
)

# Configure CORS
origins = [
    "http://localhost:5173",  # Local development frontend
    "https://metpol-ai-frontend.onrender.com",  # Render frontend
    "https://metpol-ai-frontend.vercel.app",  # Optional Vercel frontend
    "*"  # Allow all origins during development (remove in production)
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include API routes
app.include_router(api_router, prefix="/api")

@app.get("/")
def read_root():
    return {"message": "Hello World"}

@app.on_event("startup")
async def startup_event():
    """Initialize services on startup."""
    # Initialize the Chroma DB
    chroma_db_path = os.getenv("CHROMA_DB_PATH")
    print(f"Initializing Chroma DB at: {chroma_db_path}")

    # Ensure the directory exists
    Path(chroma_db_path).mkdir(parents=True, exist_ok=True)

    # Initialize the Chroma DB
    init_chroma_db()

if __name__ == "__main__":
    # Print some environment info
    print(f"CHROMA_DB_PATH: {os.getenv('CHROMA_DB_PATH')}")

    # Run the FastAPI server
    print("Run the FastAPI server at http://127.0.0.1:8000 with: uvicorn main:app --reload")
    uvicorn.run("main:app", host="127.0.0.1", port=8000, reload=True)
</file>

<file path="run_tests.py">
#!/usr/bin/env python3
"""
Test runner script for Metropole AI test suite.
"""

import os
import sys
import argparse
import subprocess


def run_tests(args):
    """Run the tests with the specified options."""
    # Base command
    cmd = ["pytest"]

    # Add verbosity
    if args.verbose:
        cmd.append("-v")

    # Add markers
    if args.marker:
        cmd.extend(["-m", args.marker])

    # Add coverage
    if args.coverage:
        cmd.extend(["--cov=app"])

        # Add coverage report format
        if args.html_report:
            cmd.extend(["--cov-report=html"])
        elif args.xml_report:
            cmd.extend(["--cov-report=xml"])
        else:
            cmd.extend(["--cov-report=term"])

    # Add specific test file or directory
    if args.test_path:
        cmd.append(args.test_path)
    else:
        cmd.append("tests/")

    # Print the command
    print(f"Running: {' '.join(cmd)}")

    # Run the command
    result = subprocess.run(cmd)
    return result.returncode


def main():
    """Parse arguments and run tests."""
    parser = argparse.ArgumentParser(description="Run Metropole AI tests")

    # Add arguments
    parser.add_argument("-v", "--verbose", action="store_true", help="Increase verbosity")
    parser.add_argument("-m", "--marker", help="Run tests with specific marker (e.g., unit, integration, crawler)")
    parser.add_argument("-c", "--coverage", action="store_true", help="Run with coverage")
    parser.add_argument("--html-report", action="store_true", help="Generate HTML coverage report")
    parser.add_argument("--xml-report", action="store_true", help="Generate XML coverage report")
    parser.add_argument("test_path", nargs="?", help="Specific test file or directory to run")

    # Parse arguments
    args = parser.parse_args()

    # Run tests
    return run_tests(args)


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="test_deployment.py">
#!/usr/bin/env python3
"""
Test script for verifying the deployment of MetPol AI.
This script tests the end-to-end flow including:
- Backend API connectivity
- Crawling functionality
- Querying and asking questions
- Source display
- Feedback logging
"""

import argparse
import requests
import json
import sys
import time
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress

console = Console()

def test_backend_connectivity(base_url):
    """Test basic connectivity to the backend API."""
    console.print("\n[bold blue]Testing Backend Connectivity[/bold blue]")

    try:
        response = requests.get(f"{base_url}/")
        if response.status_code == 200:
            console.print("[green]✓ Backend is reachable[/green]")
            return True
        else:
            console.print(f"[red]✗ Backend returned status code {response.status_code}[/red]")
            return False
    except requests.exceptions.RequestException as e:
        console.print(f"[red]✗ Failed to connect to backend: {str(e)}[/red]")
        return False

def test_api_endpoints(base_url):
    """Test the API endpoints."""
    console.print("\n[bold blue]Testing API Endpoints[/bold blue]")

    endpoints = [
        {"path": "/api/ask", "method": "POST", "data": {"question": "What is Metropole?", "top_k": 3}},
        {"path": "/api/query", "method": "POST", "data": {"query": "building rules", "n_results": 3}}
    ]

    results = []

    for endpoint in endpoints:
        try:
            if endpoint["method"] == "GET":
                response = requests.get(f"{base_url}{endpoint['path']}")
            else:
                response = requests.post(f"{base_url}{endpoint['path']}", json=endpoint["data"])

            if response.status_code in [200, 201]:
                console.print(f"[green]✓ {endpoint['path']} ({endpoint['method']}) - Success[/green]")
                results.append({"endpoint": endpoint, "success": True, "response": response.json()})
            else:
                console.print(f"[red]✗ {endpoint['path']} ({endpoint['method']}) - Failed with status {response.status_code}[/red]")
                results.append({"endpoint": endpoint, "success": False, "status_code": response.status_code})
        except requests.exceptions.RequestException as e:
            console.print(f"[red]✗ {endpoint['path']} ({endpoint['method']}) - Error: {str(e)}[/red]")
            results.append({"endpoint": endpoint, "success": False, "error": str(e)})

    return results

def test_crawl_functionality(base_url):
    """Test the crawling functionality."""
    console.print("\n[bold blue]Testing Crawl Functionality[/bold blue]")

    try:
        with Progress() as progress:
            task = progress.add_task("[cyan]Crawling...", total=100)

            # Start the crawl
            response = requests.post(f"{base_url}/api/crawl", json={"url": "https://www.metropoleballard.com/home"})

            if response.status_code in [200, 201]:
                progress.update(task, completed=50)
                data = response.json()

                if data.get("success"):
                    progress.update(task, completed=100)
                    console.print(f"[green]✓ Crawl successful[/green]")
                    console.print(Panel(f"URL: {data.get('url')}\nDoc ID: {data.get('doc_id')}", title="Crawl Result"))
                    return True
                else:
                    progress.update(task, completed=100)
                    console.print(f"[red]✗ Crawl failed: {data.get('message')}[/red]")
                    return False
            else:
                progress.update(task, completed=100)
                console.print(f"[red]✗ Crawl request failed with status {response.status_code}[/red]")
                return False
    except requests.exceptions.RequestException as e:
        console.print(f"[red]✗ Crawl request error: {str(e)}[/red]")
        return False

def test_ask_question(base_url):
    """Test asking a question and getting a response with sources."""
    console.print("\n[bold blue]Testing Question Answering[/bold blue]")

    question = "What are the building rules?"

    try:
        console.print(f"Question: [cyan]{question}[/cyan]")

        response = requests.post(f"{base_url}/api/ask", json={"question": question, "top_k": 5})

        if response.status_code in [200, 201]:
            data = response.json()

            if data.get("success"):
                console.print("[green]✓ Question answered successfully[/green]")

                # Display the answer
                console.print(Panel(data.get("answer", "No answer provided"), title="Answer"))

                # Display source information
                if data.get("chunks"):
                    table = Table(title="Source Information")
                    table.add_column("Page", style="cyan")
                    table.add_column("Section", style="green")
                    table.add_column("Content Preview", style="yellow")

                    for chunk in data.get("chunks"):
                        metadata = chunk.get("metadata", {})
                        page = metadata.get("page_title", "Unknown")
                        section = metadata.get("section_header", "Unknown")
                        content = chunk.get("text", "")
                        content_preview = content[:100] + "..." if len(content) > 100 else content

                        table.add_row(page, section, content_preview)

                    console.print(table)

                return data
            else:
                console.print(f"[red]✗ Question answering failed: {data.get('message')}[/red]")
                return None
        else:
            console.print(f"[red]✗ Question request failed with status {response.status_code}[/red]")
            return None
    except requests.exceptions.RequestException as e:
        console.print(f"[red]✗ Question request error: {str(e)}[/red]")
        return None

def test_feedback_logging(base_url, question_data):
    """Test the feedback logging functionality."""
    console.print("\n[bold blue]Testing Feedback Logging[/bold blue]")

    if not question_data:
        console.print("[yellow]! Skipping feedback test as question data is not available[/yellow]")
        return False

    try:
        feedback_data = {
            "question": question_data.get("question"),
            "response": question_data.get("answer", ""),
            "chunk_ids": [chunk.get("metadata", {}).get("chunk_id") for chunk in question_data.get("chunks", []) if chunk.get("metadata", {}).get("chunk_id")],
            "rating": 5,
            "comments": "This is a test feedback from the deployment test script.",
            "user_id": "test_user",
            "session_id": f"test_session_{int(time.time())}"
        }

        response = requests.post(f"{base_url}/api/feedback", json=feedback_data)

        if response.status_code in [200, 201]:
            data = response.json()

            if data.get("success"):
                console.print("[green]✓ Feedback logged successfully[/green]")
                return True
            else:
                console.print(f"[red]✗ Feedback logging failed: {data.get('message')}[/red]")
                return False
        else:
            console.print(f"[red]✗ Feedback request failed with status {response.status_code}[/red]")
            return False
    except requests.exceptions.RequestException as e:
        console.print(f"[red]✗ Feedback request error: {str(e)}[/red]")
        return False

def main():
    parser = argparse.ArgumentParser(description="Test the deployment of MetPol AI")
    parser.add_argument("--backend-url", default="http://localhost:8000", help="Backend API URL")
    args = parser.parse_args()

    console.print(Panel.fit(
        "[bold green]MetPol AI Deployment Test[/bold green]\n\n"
        f"Backend URL: [cyan]{args.backend_url}[/cyan]",
        title="Test Configuration"
    ))

    # Test backend connectivity
    if not test_backend_connectivity(args.backend_url):
        console.print("[bold red]Backend connectivity test failed. Aborting further tests.[/bold red]")
        sys.exit(1)

    # Test API endpoints
    api_results = test_api_endpoints(args.backend_url)

    # Test crawl functionality
    crawl_success = test_crawl_functionality(args.backend_url)

    # Test asking a question
    question_data = test_ask_question(args.backend_url)

    # Test feedback logging
    if question_data:
        feedback_success = test_feedback_logging(args.backend_url, question_data)
    else:
        feedback_success = False

    # Summary
    console.print("\n[bold blue]Test Summary[/bold blue]")

    table = Table(title="Test Results")
    table.add_column("Test", style="cyan")
    table.add_column("Result", style="green")

    table.add_row("Backend Connectivity", "[green]✓ Passed[/green]")
    table.add_row("API Endpoints", f"[{'green' if all(r['success'] for r in api_results) else 'red'}]{'✓ All Passed' if all(r['success'] for r in api_results) else '✗ Some Failed'}[/{'green' if all(r['success'] for r in api_results) else 'red'}]")
    table.add_row("Crawl Functionality", f"[{'green' if crawl_success else 'red'}]{'✓ Passed' if crawl_success else '✗ Failed'}[/{'green' if crawl_success else 'red'}]")
    table.add_row("Question Answering", f"[{'green' if question_data else 'red'}]{'✓ Passed' if question_data else '✗ Failed'}[/{'green' if question_data else 'red'}]")
    table.add_row("Feedback Logging", f"[{'green' if feedback_success else 'red'}]{'✓ Passed' if feedback_success else '✗ Failed'}[/{'green' if feedback_success else 'red'}]")

    console.print(table)

    # Overall result
    all_passed = all([
        True,  # Backend connectivity (we already exited if this failed)
        all(r["success"] for r in api_results),
        crawl_success,
        question_data is not None,
        feedback_success
    ])

    if all_passed:
        console.print(Panel("[bold green]All tests passed! The deployment is working correctly.[/bold green]", title="Success"))
    else:
        console.print(Panel("[bold yellow]Some tests failed. Please check the results above for details.[/bold yellow]", title="Warning"))

if __name__ == "__main__":
    main()
</file>

</files>
